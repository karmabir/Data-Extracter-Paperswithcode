{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6590ed9c-4df7-4fb5-a4cc-ba26eb54aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (4.12.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (2.9.0.post0)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Downloading numpy-2.2.4-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from python-dateutil) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from webdriver-manager) (2.32.3)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from trio~=0.17->selenium) (24.3.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/envs/Study/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading numpy-2.2.4-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, pytz, wsproto, tzdata, python-dotenv, outcome, numpy, webdriver-manager, trio, pandas, trio-websocket, selenium\n",
      "Successfully installed numpy-2.2.4 outcome-1.3.0.post0 pandas-2.2.3 python-dotenv-1.1.0 pytz-2025.2 selenium-4.30.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.2 tzdata-2025.2 webdriver-manager-4.0.2 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium beautifulsoup4 pandas python-dateutil webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "908efa4b-87b0-4019-95e3-870ad653ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pose Estimation time series data collection...\n",
      "Identifying top pose estimation datasets...\n",
      "Found 5 datasets to process: ['object-detection-on-coco', 'object-detection-on-coco', 'object-detection-on-coco', 'object-detection-on-coco', 'object-detection-on-coco-minival']\n",
      "Scraping SOTA timeline for https://paperswithcode.com/sota/object-detection-on-coco...\n",
      "No timeline link found for https://paperswithcode.com/sota/object-detection-on-coco, using main leaderboard\n",
      "Scraped 262 models for object-detection-on-coco\n",
      "Scraping SOTA timeline for https://paperswithcode.com/sota/object-detection-on-coco...\n",
      "No timeline link found for https://paperswithcode.com/sota/object-detection-on-coco, using main leaderboard\n",
      "Scraped 262 models for object-detection-on-coco\n",
      "Scraping SOTA timeline for https://paperswithcode.com/sota/object-detection-on-coco...\n",
      "No timeline link found for https://paperswithcode.com/sota/object-detection-on-coco, using main leaderboard\n",
      "Scraped 262 models for object-detection-on-coco\n",
      "Scraping SOTA timeline for https://paperswithcode.com/sota/object-detection-on-coco...\n",
      "No timeline link found for https://paperswithcode.com/sota/object-detection-on-coco, using main leaderboard\n",
      "Scraped 262 models for object-detection-on-coco\n",
      "Scraping SOTA timeline for https://paperswithcode.com/sota/object-detection-on-coco-minival...\n",
      "No timeline link found for https://paperswithcode.com/sota/object-detection-on-coco-minival, using main leaderboard\n",
      "Scraped 219 models for object-detection-on-coco-minival\n",
      "Saved 1267 models to tech_progress_data/pose_estimation/pose_estimation_all_models.csv\n",
      "Saved yearly summary to tech_progress_data/pose_estimation/pose_estimation_yearly_count.csv\n",
      "Saved 20 monthly SOTA models to tech_progress_data/pose_estimation/pose_estimation_monthly_sota.csv\n",
      "Data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data\n",
    "data_dir = 'tech_progress_data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'pose_estimation'), exist_ok=True)\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:Ω\n",
    "            year = int(year_match.group())\n",
    "            return datetime.date(year, 1, 1)  # Default to January 1st of the year\n",
    "    return None\n",
    "\n",
    "def extract_parameters(text):\n",
    "    \"\"\"Extract model parameter count in millions from text\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    text = text.lower()\n",
    "    param_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*([kmbtgp]?)\\s*(?:parameters|params|param)', text)\n",
    "    \n",
    "    if param_match:\n",
    "        param_value = float(param_match.group(1))\n",
    "        param_unit = param_match.group(2).lower() if param_match.group(2) else ''\n",
    "        \n",
    "        # Convert to millions\n",
    "        if param_unit == 'k':\n",
    "            return param_value / 1000  # K to M\n",
    "        elif param_unit == 'm':\n",
    "            return param_value  # Already in M\n",
    "        elif param_unit == 'b' or param_unit == 'g':\n",
    "            return param_value * 1000  # B to M\n",
    "        elif param_unit == 't':\n",
    "            return param_value * 1000000  # T to M\n",
    "        elif param_unit == 'p':\n",
    "            return param_value * 1000000000  # P to M\n",
    "        else:\n",
    "            return param_value  # Assume already in M if no unit\n",
    "            \n",
    "    return None\n",
    "\n",
    "def get_top_object_detection_datasets(max_datasets=5):\n",
    "    \"\"\"Get the most important pose estimaiton datasets/benchmarks\"\"\"\n",
    "    print(\"Identifying top pose estimation datasets...\")\n",
    "    \n",
    "    # Prioritize these well-known datasets first\n",
    "    priority_datasets = ['coco', 'pascal-voc', 'objects365', 'open-images']\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    dataset_links = []\n",
    "    \n",
    "    try:\n",
    "        # Navigate to pose estimation task page\n",
    "        driver.get(\"https://paperswithcode.com/task/object-detection\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all dataset links\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href:\n",
    "                # Extract dataset name from URL\n",
    "                dataset_name = href.split(\"/\")[-1]\n",
    "                if dataset_name:\n",
    "                    dataset_links.append({\n",
    "                        \"name\": dataset_name,\n",
    "                        \"url\": href\n",
    "                    })\n",
    "        \n",
    "        # Sort by priority\n",
    "        def get_priority(dataset):\n",
    "            for i, priority in enumerate(priority_datasets):\n",
    "                if priority in dataset[\"name\"].lower():\n",
    "                    return i\n",
    "            return len(priority_datasets)\n",
    "        \n",
    "        dataset_links.sort(key=get_priority)\n",
    "        \n",
    "        # Limit to max_datasets\n",
    "        return dataset_links[:max_datasets]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting datasets: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def scrape_sota_timeline(dataset_url):\n",
    "    \"\"\"Scrape the SOTA over time for a specific dataset\"\"\"\n",
    "    print(f\"Scraping SOTA timeline for {dataset_url}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # First load the dataset page\n",
    "        driver.get(dataset_url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to find SOTA over time link\n",
    "        timeline_url = None\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"sota-over-time\" in href:\n",
    "                timeline_url = href\n",
    "                break\n",
    "        \n",
    "        # If no timeline link found, use the main leaderboard\n",
    "        if not timeline_url:\n",
    "            timeline_url = dataset_url\n",
    "            print(f\"No timeline link found for {dataset_url}, using main leaderboard\")\n",
    "        \n",
    "        # Navigate to the timeline page\n",
    "        driver.get(timeline_url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Extract dataset/benchmark name\n",
    "        dataset_name = dataset_url.split(\"/\")[-1].replace(\"-\", \" \").title()\n",
    "        \n",
    "        # Try to extract data from JSON first (best source of truth)\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Get model parameters and compute if available\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                parameters = extract_parameters(model_desc)\n",
    "                                \n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': dataset_name,\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'parameters_millions': parameters,\n",
    "                                        'description': model_desc,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': dataset_name,\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        return models_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping SOTA timeline: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def consolidate_to_monthly_sota(models_df):\n",
    "    \"\"\"\n",
    "    From a full model dataframe, create a consolidated monthly SOTA dataset\n",
    "    with one best model per month per dataset\n",
    "    \"\"\"\n",
    "    if models_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create year-month column if not exists\n",
    "    if 'year_month' not in models_df.columns:\n",
    "        models_df['year_month'] = pd.to_datetime(models_df['date']).dt.to_period('M')\n",
    "    \n",
    "    # Identify primary metric columns (typically mAP, accuracy, etc.)\n",
    "    # This is a heuristic approach - you may need to adjust for specific datasets\n",
    "    metric_cols = []\n",
    "    for col in models_df.columns:\n",
    "        if col.lower() in ['map', 'ap', 'accuracy', 'f1', 'precision', 'recall']:\n",
    "            metric_cols.append(col)\n",
    "    \n",
    "    if not metric_cols:\n",
    "        # If no clear metrics, try to find numeric columns\n",
    "        for col in models_df.columns:\n",
    "            if models_df[col].dtype in [float, int] and col not in ['year', 'month', 'parameters_millions']:\n",
    "                metric_cols.append(col)\n",
    "    \n",
    "    # If still no metrics, just return the original data\n",
    "    if not metric_cols:\n",
    "        return models_df\n",
    "    \n",
    "    # Use first metric as primary sort key\n",
    "    primary_metric = metric_cols[0]\n",
    "    \n",
    "    # Group by dataset and month, then take the best model\n",
    "    # (assuming higher value is better for the metric)\n",
    "    datasets = models_df['dataset'].unique()\n",
    "    monthly_sota = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_df = models_df[models_df['dataset'] == dataset]\n",
    "        \n",
    "        # Get monthly best models\n",
    "        try:\n",
    "            # Sort by year_month and metric (descending for metric)\n",
    "            monthly_best = dataset_df.sort_values(['year_month', primary_metric], \n",
    "                                              ascending=[True, False])\\\n",
    "                                   .groupby('year_month')\\\n",
    "                                   .first()\\\n",
    "                                   .reset_index()\n",
    "            \n",
    "            monthly_sota.append(monthly_best)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset}: {e}\")\n",
    "    \n",
    "    # Combine all datasets\n",
    "    if monthly_sota:\n",
    "        combined = pd.concat(monthly_sota, ignore_index=True)\n",
    "        # Convert period to date string\n",
    "        combined['year_month'] = combined['year_month'].astype(str)\n",
    "        return combined\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    print(\"Starting Pose Estimation time series data collection...\")\n",
    "    \n",
    "    # Get top pose estimation datasets\n",
    "    datasets = get_top_object_detection_datasets(max_datasets=5)\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"No datasets found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(datasets)} datasets to process: {[d['name'] for d in datasets]}\")\n",
    "    \n",
    "    # DataFrame to store all models\n",
    "    all_models = []\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            dataset_models = scrape_sota_timeline(dataset[\"url\"])\n",
    "            \n",
    "            if dataset_models:\n",
    "                print(f\"Scraped {len(dataset_models)} models for {dataset['name']}\")\n",
    "                all_models.extend(dataset_models)\n",
    "            else:\n",
    "                print(f\"No models found for {dataset['name']}\")\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset['name']}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if all_models:\n",
    "        df = pd.DataFrame(all_models)\n",
    "        \n",
    "        # Save full dataset\n",
    "        full_output_path = os.path.join(data_dir, 'pose_estimation', 'pose_estimation_all_models.csv')\n",
    "        df.to_csv(full_output_path, index=False)\n",
    "        print(f\"Saved {len(df)} models to {full_output_path}\")\n",
    "        \n",
    "        # Create year-month column for aggregation\n",
    "        df['year_month'] = pd.to_datetime(df['date']).dt.to_period('M')\n",
    "        \n",
    "        # Save yearly summary\n",
    "        yearly_summary = df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "        yearly_summary_path = os.path.join(data_dir, 'pose_estimation', 'pose_estimation_yearly_count.csv')\n",
    "        yearly_summary.to_csv(yearly_summary_path, index=False)\n",
    "        print(f\"Saved yearly summary to {yearly_summary_path}\")\n",
    "        \n",
    "        # Get monthly SOTA models\n",
    "        monthly_sota = consolidate_to_monthly_sota(df)\n",
    "        if not monthly_sota.empty:\n",
    "            monthly_path = os.path.join(data_dir, 'pose_estimation', 'pose_estimation_monthly_sota.csv')\n",
    "            monthly_sota.to_csv(monthly_path, index=False)\n",
    "            print(f\"Saved {len(monthly_sota)} monthly SOTA models to {monthly_path}\")\n",
    "        \n",
    "        # Create a time series summary (year, month, count of models, avg performance)\n",
    "        try:\n",
    "            # Get the most common metric column\n",
    "            metric_cols = []\n",
    "            for col in df.columns:\n",
    "                if col.lower() in ['map', 'ap', 'accuracy', 'f1']:\n",
    "                    metric_cols.append(col)\n",
    "            \n",
    "            if metric_cols:\n",
    "                primary_metric = metric_cols[0]\n",
    "                time_series = df.groupby(['year', 'month']).agg({\n",
    "                    'model_name': 'count',\n",
    "                    primary_metric: ['mean', 'max'],\n",
    "                    'parameters_millions': ['mean', 'max', 'count']\n",
    "                }).reset_index()\n",
    "                \n",
    "                # Flatten multi-level columns\n",
    "                time_series.columns = ['_'.join(col).strip('_') for col in time_series.columns.values]\n",
    "                \n",
    "                time_series_path = os.path.join(data_dir, 'pose_estimation', 'pose_estimation_time_series.csv')\n",
    "                time_series.to_csv(time_series_path, index=False)\n",
    "                print(f\"Saved time series summary to {time_series_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating time series summary: {e}\")\n",
    "        \n",
    "        print(\"Data collection complete!\")\n",
    "    else:\n",
    "        print(\"No models collected.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18acad3-b999-4fc0-994e-6f72e2a11b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting pose estimation data collection (20+ datapoints)...\n",
      "Finding pose estimation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 34 potential pose estimation benchmarks\n",
      "Checking pose-estimation-on-coco...\n",
      "pose-estimation-on-coco: 10 datapoints\n",
      "Checking pose-estimation-on-mpii...\n",
      "pose-estimation-on-mpii: 1 datapoints\n",
      "Checking 3d-human-pose-estimation-on-human36m...\n",
      "3d-human-pose-estimation-on-human36m: 357 datapoints\n",
      "Added 3d-human-pose-estimation-on-human36m with 357 datapoints\n",
      "Checking pose-estimation-on-ochuman...\n",
      "pose-estimation-on-ochuman: 18 datapoints\n",
      "Checking 3d-human-pose-estimation-on-mpi-inf-3dhp...\n",
      "3d-human-pose-estimation-on-mpi-inf-3dhp: 116 datapoints\n",
      "Added 3d-human-pose-estimation-on-mpi-inf-3dhp with 116 datapoints\n",
      "Checking multi-person-pose-estimation-on-coco...\n",
      "multi-person-pose-estimation-on-coco: 15 datapoints\n",
      "Checking pose-estimation-on-mpii-human-pose...\n",
      "pose-estimation-on-mpii-human-pose: 46 datapoints\n",
      "Added pose-estimation-on-mpii-human-pose with 46 datapoints\n",
      "Checking pose-estimation-on-coco-test-dev...\n",
      "pose-estimation-on-coco-test-dev: 46 datapoints\n",
      "Added pose-estimation-on-coco-test-dev with 46 datapoints\n",
      "Checking pose-estimation-on-leeds-sports-poses...\n",
      "pose-estimation-on-leeds-sports-poses: 18 datapoints\n",
      "Checking pose-estimation-on-crowdpose...\n",
      "pose-estimation-on-crowdpose: 12 datapoints\n",
      "Checking pose-estimation-on-coco-val2017...\n",
      "pose-estimation-on-coco-val2017: 11 datapoints\n",
      "Checking pose-estimation-on-aic...\n",
      "pose-estimation-on-aic: 10 datapoints\n",
      "Checking pose-estimation-on-itop-front-view...\n",
      "pose-estimation-on-itop-front-view: 7 datapoints\n",
      "Checking pose-estimation-on-inloc...\n",
      "pose-estimation-on-inloc: 6 datapoints\n",
      "Checking pose-estimation-on-itop-top-view...\n",
      "pose-estimation-on-itop-top-view: 5 datapoints\n",
      "Checking pose-estimation-on-upenn-action...\n",
      "pose-estimation-on-upenn-action: 5 datapoints\n",
      "Checking pose-estimation-on-j-hmdb...\n",
      "pose-estimation-on-j-hmdb: 5 datapoints\n",
      "Checking pose-estimation-on-mpii-single-person...\n",
      "pose-estimation-on-mpii-single-person: 5 datapoints\n",
      "Checking pose-estimation-on-salsa...\n",
      "pose-estimation-on-salsa: 4 datapoints\n",
      "Checking pose-estimation-on-300w-full...\n",
      "pose-estimation-on-300w-full: 3 datapoints\n",
      "Checking pose-estimation-on-densepose-coco...\n",
      "pose-estimation-on-densepose-coco: 2 datapoints\n",
      "Checking pose-estimation-on-flic-elbows...\n",
      "pose-estimation-on-flic-elbows: 2 datapoints\n",
      "Checking pose-estimation-on-flic-wrists...\n",
      "pose-estimation-on-flic-wrists: 2 datapoints\n",
      "Checking pose-estimation-on-uav-human...\n",
      "pose-estimation-on-uav-human: 2 datapoints\n",
      "Checking pose-estimation-on-brace...\n",
      "pose-estimation-on-brace: 2 datapoints\n",
      "Checking pose-estimation-on-coco-minival...\n",
      "pose-estimation-on-coco-minival: 1 datapoints\n",
      "Checking pose-estimation-on-3dpw...\n",
      "pose-estimation-on-3dpw: 1 datapoints\n",
      "Checking pose-estimation-on-apollocar3d...\n",
      "pose-estimation-on-apollocar3d: 1 datapoints\n",
      "Checking pose-estimation-on-pix3d...\n",
      "pose-estimation-on-pix3d: 1 datapoints\n",
      "Checking pose-estimation-on-kitti-2015...\n",
      "pose-estimation-on-kitti-2015: 1 datapoints\n",
      "Checking pose-estimation-on-merl-rav...\n",
      "pose-estimation-on-merl-rav: 1 datapoints\n",
      "Checking pose-estimation-on-ms-coco...\n",
      "pose-estimation-on-ms-coco: 1 datapoints\n",
      "Checking pose-estimation-on-2...\n",
      "pose-estimation-on-2: 1 datapoints\n",
      "Checking pose-estimation-on-coco-2017-val...\n",
      "pose-estimation-on-coco-2017-val: 2 datapoints\n",
      "Saved 4 benchmarks with 20+ datapoints\n",
      "Found 4 pose estimation benchmarks with 20+ datapoints\n",
      "Scraping data for 3d-human-pose-estimation-on-human36m...\n",
      "Saved 357 models for 3d-human-pose-estimation-on-human36m\n",
      "Scraping data for 3d-human-pose-estimation-on-mpi-inf-3dhp...\n",
      "Saved 116 models for 3d-human-pose-estimation-on-mpi-inf-3dhp\n",
      "Scraping data for pose-estimation-on-mpii-human-pose...\n",
      "Saved 46 models for pose-estimation-on-mpii-human-pose\n",
      "Scraping data for pose-estimation-on-coco-test-dev...\n",
      "Saved 45 models for pose-estimation-on-coco-test-dev\n",
      "Collected a total of 564 models across all benchmarks\n",
      "Combined 564 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Pose estimation data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory (karmabirchakraborty)\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'pose_estimation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def get_pose_benchmarks_with_20plus():\n",
    "    \"\"\"Get pose estimation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding pose estimation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/pose-estimation-on-coco\",\n",
    "        \"https://paperswithcode.com/sota/pose-estimation-on-mpii\",\n",
    "        \"https://paperswithcode.com/sota/3d-human-pose-estimation-on-human36m\",\n",
    "        \"https://paperswithcode.com/sota/pose-estimation-on-ochuman\",\n",
    "        \"https://paperswithcode.com/sota/3d-human-pose-estimation-on-mpi-inf-3dhp\",\n",
    "        \"https://paperswithcode.com/sota/multi-person-pose-estimation-on-coco\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main pose estimation page\n",
    "        driver.get(\"https://paperswithcode.com/task/pose-estimation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"pose\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential pose estimation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'pose_estimation', 'pose_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'pose_estimation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'pose_estimation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all pose estimation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'pose_estimation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'pose_estimation', 'all_pose_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'pose_estimation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting pose estimation data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_pose_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No pose estimation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} pose estimation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Pose estimation data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df888a2-e37a-418a-80f9-2d06c52e8704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting object detection data collection (20+ datapoints)...\n",
      "Finding object detection benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 126 potential object detection benchmarks\n",
      "Checking object-detection-on-coco...\n",
      "object-detection-on-coco: 262 datapoints\n",
      "Added object-detection-on-coco with 262 datapoints\n",
      "Checking object-detection-on-pascal-voc-2007...\n",
      "object-detection-on-pascal-voc-2007: 30 datapoints\n",
      "Added object-detection-on-pascal-voc-2007 with 30 datapoints\n",
      "Checking object-detection-on-pascal-voc-2012...\n",
      "object-detection-on-pascal-voc-2012: 7 datapoints\n",
      "Checking object-detection-on-lvis...\n",
      "object-detection-on-lvis: 0 datapoints\n",
      "Checking object-detection-on-open-images...\n",
      "object-detection-on-open-images: 0 datapoints\n",
      "Checking real-time-object-detection-on-coco...\n",
      "real-time-object-detection-on-coco: 82 datapoints\n",
      "Added real-time-object-detection-on-coco with 82 datapoints\n",
      "Checking object-detection-on-coco-minival...\n",
      "object-detection-on-coco-minival: 355 datapoints\n",
      "Added object-detection-on-coco-minival with 355 datapoints\n",
      "Checking object-detection-on-coco-o...\n",
      "object-detection-on-coco-o: 45 datapoints\n",
      "Added object-detection-on-coco-o with 45 datapoints\n",
      "Checking object-detection-on-coco-2017-val...\n",
      "object-detection-on-coco-2017-val: 33 datapoints\n",
      "Added object-detection-on-coco-2017-val with 33 datapoints\n",
      "Checking object-detection-on-coco-2017...\n",
      "object-detection-on-coco-2017: 24 datapoints\n",
      "Added object-detection-on-coco-2017 with 24 datapoints\n",
      "Checking object-detection-on-crowdhuman-full-body...\n",
      "object-detection-on-crowdhuman-full-body: 19 datapoints\n",
      "Checking object-detection-on-cppe-5...\n",
      "object-detection-on-cppe-5: 16 datapoints\n",
      "Checking object-detection-on-waymo-2d-detection-all-ns-1...\n",
      "object-detection-on-waymo-2d-detection-all-ns-1: 15 datapoints\n",
      "Checking object-detection-on-manga109-s-15test...\n",
      "object-detection-on-manga109-s-15test: 15 datapoints\n",
      "Checking object-detection-on-pku-ddd17-car...\n",
      "object-detection-on-pku-ddd17-car: 14 datapoints\n",
      "Checking object-detection-on-usb-standard-usb-1-0...\n",
      "object-detection-on-usb-standard-usb-1-0: 13 datapoints\n",
      "Checking object-detection-on-lvis-v1-0-val...\n",
      "object-detection-on-lvis-v1-0-val: 13 datapoints\n",
      "Checking object-detection-on-dsec...\n",
      "object-detection-on-dsec: 12 datapoints\n",
      "Checking object-detection-on-sfchd...\n",
      "object-detection-on-sfchd: 12 datapoints\n",
      "Checking object-detection-on-gen1-detection...\n",
      "object-detection-on-gen1-detection: 11 datapoints\n",
      "Checking object-detection-on-seadronessee...\n",
      "object-detection-on-seadronessee: 10 datapoints\n",
      "Checking object-detection-on-ua-detrac...\n",
      "object-detection-on-ua-detrac: 9 datapoints\n",
      "Checking object-detection-on-uavdt...\n",
      "object-detection-on-uavdt: 8 datapoints\n",
      "Checking object-detection-on-odinw-full-shot-13-tasks...\n",
      "object-detection-on-odinw-full-shot-13-tasks: 8 datapoints\n",
      "Checking object-detection-on-ai-tod...\n",
      "object-detection-on-ai-tod: 7 datapoints\n",
      "Checking object-detection-on-nao...\n",
      "object-detection-on-nao: 7 datapoints\n",
      "Checking object-detection-on-tbbr...\n",
      "object-detection-on-tbbr: 7 datapoints\n",
      "Checking object-detection-on-peopleart...\n",
      "object-detection-on-peopleart: 6 datapoints\n",
      "Checking object-detection-on-bigdetection-val...\n",
      "object-detection-on-bigdetection-val: 6 datapoints\n",
      "Checking object-detection-on-lvis-v1-0-minival...\n",
      "object-detection-on-lvis-v1-0-minival: 6 datapoints\n",
      "Checking object-detection-on-grazpedwri-dx...\n",
      "object-detection-on-grazpedwri-dx: 6 datapoints\n",
      "Checking object-detection-on-inoutdoor...\n",
      "object-detection-on-inoutdoor: 6 datapoints\n",
      "Checking object-detection-on-eventped...\n",
      "object-detection-on-eventped: 6 datapoints\n",
      "Checking object-detection-on-stcrowd...\n",
      "object-detection-on-stcrowd: 6 datapoints\n",
      "Checking object-detection-on-kitti-cars-easy...\n",
      "object-detection-on-kitti-cars-easy: 5 datapoints\n",
      "Checking object-detection-on-kitti-cars-hard...\n",
      "object-detection-on-kitti-cars-hard: 5 datapoints\n",
      "Checking object-detection-on-isaid...\n",
      "object-detection-on-isaid: 5 datapoints\n",
      "Checking object-detection-on-waymo-open-dataset...\n",
      "object-detection-on-waymo-open-dataset: 5 datapoints\n",
      "Checking object-detection-on-visdrone-det2019-1...\n",
      "object-detection-on-visdrone-det2019-1: 5 datapoints\n",
      "Checking object-detection-on-visual-genome...\n",
      "object-detection-on-visual-genome: 4 datapoints\n",
      "Checking object-detection-on-india-driving-dataset...\n",
      "object-detection-on-india-driving-dataset: 4 datapoints\n",
      "Checking object-detection-on-kitti-cars-moderate...\n",
      "object-detection-on-kitti-cars-moderate: 4 datapoints\n",
      "Checking object-detection-on-widerperson...\n",
      "object-detection-on-widerperson: 4 datapoints\n",
      "Checking object-detection-on-waterscenes...\n",
      "object-detection-on-waterscenes: 4 datapoints\n",
      "Checking object-detection-on-coco-1...\n",
      "object-detection-on-coco-1: 3 datapoints\n",
      "Checking object-detection-on-flickrlogos-32...\n",
      "object-detection-on-flickrlogos-32: 3 datapoints\n",
      "Checking object-detection-on-vedai...\n",
      "object-detection-on-vedai: 3 datapoints\n",
      "Checking object-detection-on-oodis...\n",
      "object-detection-on-oodis: 3 datapoints\n",
      "Checking object-detection-on-sixray...\n",
      "object-detection-on-sixray: 2 datapoints\n",
      "Checking object-detection-on-nuscenes...\n",
      "object-detection-on-nuscenes: 2 datapoints\n",
      "Checking object-detection-on-pascal-voc-10...\n",
      "object-detection-on-pascal-voc-10: 2 datapoints\n",
      "Checking object-detection-on-mscoco-6...\n",
      "object-detection-on-mscoco-6: 7 datapoints\n",
      "Checking object-detection-on-drone-vs-bird...\n",
      "object-detection-on-drone-vs-bird: 2 datapoints\n",
      "Checking object-detection-on-spacenet-2...\n",
      "object-detection-on-spacenet-2: 2 datapoints\n",
      "Checking object-detection-on-odinw-full-shot-35-tasks...\n",
      "object-detection-on-odinw-full-shot-35-tasks: 2 datapoints\n",
      "Checking object-detection-on-manga109...\n",
      "object-detection-on-manga109: 2 datapoints\n",
      "Checking object-detection-on-openimages-v6...\n",
      "object-detection-on-openimages-v6: 2 datapoints\n",
      "Checking object-detection-on-pascal-voc-to-clipart1k...\n",
      "object-detection-on-pascal-voc-to-clipart1k: 2 datapoints\n",
      "Checking object-detection-on-industreal...\n",
      "object-detection-on-industreal: 2 datapoints\n",
      "Checking object-detection-on-sa-det-100k...\n",
      "object-detection-on-sa-det-100k: 2 datapoints\n",
      "Checking object-detection-on-bdd100k...\n",
      "object-detection-on-bdd100k: 1 datapoints\n",
      "Checking object-detection-on-pascal-part-2010-animals...\n",
      "object-detection-on-pascal-part-2010-animals: 1 datapoints\n",
      "Checking object-detection-on-sun-rgbd-val...\n",
      "object-detection-on-sun-rgbd-val: 1 datapoints\n",
      "Checking object-detection-on-kitti-pedestrians-easy...\n",
      "object-detection-on-kitti-pedestrians-easy: 1 datapoints\n",
      "Checking object-detection-on-kitti-pedestrians...\n",
      "object-detection-on-kitti-pedestrians: 1 datapoints\n",
      "Checking object-detection-on-kitti-pedestrians-hard...\n",
      "object-detection-on-kitti-pedestrians-hard: 1 datapoints\n",
      "Checking object-detection-on-kitti-cyclists-easy...\n",
      "object-detection-on-kitti-cyclists-easy: 1 datapoints\n",
      "Checking object-detection-on-kitti-cyclists-moderate...\n",
      "object-detection-on-kitti-cyclists-moderate: 1 datapoints\n",
      "Checking object-detection-on-kitti-cyclists-hard...\n",
      "object-detection-on-kitti-cyclists-hard: 1 datapoints\n",
      "Checking object-detection-on-extragalactic-planetary...\n",
      "object-detection-on-extragalactic-planetary: 1 datapoints\n",
      "Checking object-detection-on-coco-5...\n",
      "object-detection-on-coco-5: 1 datapoints\n",
      "Checking object-detection-on-waymo-2d-detection-all-ns...\n",
      "object-detection-on-waymo-2d-detection-all-ns: 1 datapoints\n",
      "Checking object-detection-on-deeptrash...\n",
      "object-detection-on-deeptrash: 1 datapoints\n",
      "Checking object-detection-on-mju-waste...\n",
      "object-detection-on-mju-waste: 1 datapoints\n",
      "Checking object-detection-on-uavvaste...\n",
      "object-detection-on-uavvaste: 1 datapoints\n",
      "Checking object-detection-on-extended-taco-1...\n",
      "object-detection-on-extended-taco-1: 1 datapoints\n",
      "Checking object-detection-on-drinking-waste...\n",
      "object-detection-on-drinking-waste: 1 datapoints\n",
      "Checking object-detection-on-extended-taco-7...\n",
      "object-detection-on-extended-taco-7: 1 datapoints\n",
      "Checking object-detection-on-a2d...\n",
      "object-detection-on-a2d: 1 datapoints\n",
      "Checking object-detection-on-plad...\n",
      "object-detection-on-plad: 1 datapoints\n",
      "Checking object-detection-on-a-dataset-of...\n",
      "object-detection-on-a-dataset-of: 1 datapoints\n",
      "Checking object-detection-on-spacenet-1...\n",
      "object-detection-on-spacenet-1: 1 datapoints\n",
      "Checking object-detection-on-aquatrash...\n",
      "object-detection-on-aquatrash: 1 datapoints\n",
      "Checking object-detection-on-elevater...\n",
      "object-detection-on-elevater: 1 datapoints\n",
      "Checking object-detection-on-cityscapes-to-foggy...\n",
      "object-detection-on-cityscapes-to-foggy: 1 datapoints\n",
      "Checking object-detection-on-crowdhuman...\n",
      "object-detection-on-crowdhuman: 1 datapoints\n",
      "Checking object-detection-on-citypersons...\n",
      "object-detection-on-citypersons: 1 datapoints\n",
      "Checking object-detection-on-bdd100k-val...\n",
      "object-detection-on-bdd100k-val: 1 datapoints\n",
      "Checking object-detection-on-coco-val2017...\n",
      "object-detection-on-coco-val2017: 1 datapoints\n",
      "Checking object-detection-on-shel5k-1...\n",
      "object-detection-on-shel5k-1: 1 datapoints\n",
      "Checking object-detection-on-lvis-v1-0-1...\n",
      "object-detection-on-lvis-v1-0-1: 1 datapoints\n",
      "Checking object-detection-on-objects365...\n",
      "object-detection-on-objects365: 1 datapoints\n",
      "Checking object-detection-on-visdrone-1-labeled-data...\n",
      "object-detection-on-visdrone-1-labeled-data: 1 datapoints\n",
      "Checking object-detection-on-visdrone-5-labeled-data...\n",
      "object-detection-on-visdrone-5-labeled-data: 1 datapoints\n",
      "Checking object-detection-on-visdrone-10-labeled-data...\n",
      "object-detection-on-visdrone-10-labeled-data: 1 datapoints\n",
      "Checking object-detection-on-multispectral-dataset...\n",
      "object-detection-on-multispectral-dataset: 1 datapoints\n",
      "Checking object-detection-on-pascal-voc...\n",
      "object-detection-on-pascal-voc: 1 datapoints\n",
      "Checking object-detection-on-mscoco-7...\n",
      "object-detection-on-mscoco-7: 1 datapoints\n",
      "Checking object-detection-on-ldd...\n",
      "object-detection-on-ldd: 1 datapoints\n",
      "Checking object-detection-on-watercolor2k...\n",
      "object-detection-on-watercolor2k: 1 datapoints\n",
      "Checking object-detection-on-comic2k...\n",
      "object-detection-on-comic2k: 1 datapoints\n",
      "Checking object-detection-on-clipart1k...\n",
      "object-detection-on-clipart1k: 1 datapoints\n",
      "Checking object-detection-on-pascal-voc-to...\n",
      "object-detection-on-pascal-voc-to: 1 datapoints\n",
      "Checking object-detection-on-pascal-voc-to-comic2k...\n",
      "object-detection-on-pascal-voc-to-comic2k: 1 datapoints\n",
      "Checking object-detection-on-evd4uav...\n",
      "object-detection-on-evd4uav: 1 datapoints\n",
      "Checking object-detection-on-4...\n",
      "object-detection-on-4: 1 datapoints\n",
      "Checking object-detection-on-pku-ddd17-car-1...\n",
      "object-detection-on-pku-ddd17-car-1: 1 datapoints\n",
      "Checking object-detection-on-pascal-voc-2007-15-5...\n",
      "object-detection-on-pascal-voc-2007-15-5: 1 datapoints\n",
      "Checking object-detection-on-llvip...\n",
      "object-detection-on-llvip: 1 datapoints\n",
      "Checking object-detection-on-flir...\n",
      "object-detection-on-flir: 1 datapoints\n",
      "Checking object-detection-on-gmot-40...\n",
      "object-detection-on-gmot-40: 1 datapoints\n",
      "Checking object-detection-on-pascal-voc-2012-test...\n",
      "object-detection-on-pascal-voc-2012-test: 1 datapoints\n",
      "Checking object-detection-on-nii-cu-mapd...\n",
      "object-detection-on-nii-cu-mapd: 1 datapoints\n",
      "Checking object-detection-on-c2a-human-detection-in...\n",
      "object-detection-on-c2a-human-detection-in: 1 datapoints\n",
      "Checking object-detection-on-muses-multi-sensor...\n",
      "object-detection-on-muses-multi-sensor: 1 datapoints\n",
      "Checking object-detection-on-leukemiaattri...\n",
      "object-detection-on-leukemiaattri: 1 datapoints\n",
      "Checking object-detection-on-sar-aircraft-1-0...\n",
      "object-detection-on-sar-aircraft-1-0: 1 datapoints\n",
      "Checking object-detection-on-aodraw...\n",
      "object-detection-on-aodraw: 1 datapoints\n",
      "Checking object-detection-on-coco-11...\n",
      "object-detection-on-coco-11: 1 datapoints\n",
      "Checking object-detection-on-texbig-2023-test...\n",
      "object-detection-on-texbig-2023-test: 1 datapoints\n",
      "Checking object-detection-on-texbig-2022-test...\n",
      "object-detection-on-texbig-2022-test: 1 datapoints\n",
      "Checking object-detection-on-cisol-track-a-td-tsr...\n",
      "object-detection-on-cisol-track-a-td-tsr: 1 datapoints\n",
      "Checking object-detection-on-cisol-track-b-tsr-only...\n",
      "object-detection-on-cisol-track-b-tsr-only: 1 datapoints\n",
      "Checking object-detection-on-gqa...\n",
      "object-detection-on-gqa: 1 datapoints\n",
      "Checking object-detection-on-10000-people-human-pose...\n",
      "object-detection-on-10000-people-human-pose: 1 datapoints\n",
      "Checking object-detection-on-songdo-vision...\n",
      "object-detection-on-songdo-vision: 1 datapoints\n",
      "Saved 7 benchmarks with 20+ datapoints\n",
      "Found 7 object detection benchmarks with 20+ datapoints\n",
      "Scraping data for object-detection-on-coco...\n",
      "Saved 262 models for object-detection-on-coco\n",
      "Scraping data for object-detection-on-pascal-voc-2007...\n",
      "Saved 30 models for object-detection-on-pascal-voc-2007\n",
      "Scraping data for real-time-object-detection-on-coco...\n",
      "Saved 82 models for real-time-object-detection-on-coco\n",
      "Scraping data for object-detection-on-coco-minival...\n",
      "Saved 219 models for object-detection-on-coco-minival\n",
      "Scraping data for object-detection-on-coco-o...\n",
      "Saved 45 models for object-detection-on-coco-o\n",
      "Scraping data for object-detection-on-coco-2017-val...\n",
      "Saved 33 models for object-detection-on-coco-2017-val\n",
      "Scraping data for object-detection-on-coco-2017...\n",
      "Saved 24 models for object-detection-on-coco-2017\n",
      "Collected a total of 695 models across all benchmarks\n",
      "Combined 695 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Analyzing performance trends...\n",
      "Error combining models: agg function failed [how->mean,dtype->object]\n",
      "Object detection data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'object_detection'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_object_detection_benchmarks_with_20plus():\n",
    "    \"\"\"Get object detection benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding object detection benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/object-detection-on-coco\",\n",
    "        \"https://paperswithcode.com/sota/object-detection-on-pascal-voc-2007\",\n",
    "        \"https://paperswithcode.com/sota/object-detection-on-pascal-voc-2012\",\n",
    "        \"https://paperswithcode.com/sota/object-detection-on-lvis\",\n",
    "        \"https://paperswithcode.com/sota/object-detection-on-open-images\",\n",
    "        \"https://paperswithcode.com/sota/real-time-object-detection-on-coco\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main object detection page\n",
    "        driver.get(\"https://paperswithcode.com/task/object-detection\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"detection\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential object detection benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'object_detection', 'object_detection_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'object_detection', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'object_detection', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def extract_inference_speed(model_name, description):\n",
    "    \"\"\"Extract inference speed (FPS) from model name or description\"\"\"\n",
    "    if not model_name and not description:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for FPS\n",
    "    fps_patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*fps',\n",
    "        r'(\\d+\\.?\\d*)\\s*FPS',\n",
    "        r'(\\d+\\.?\\d*)\\s*frames\\s*per\\s*second',\n",
    "        r'runs\\s*at\\s*(\\d+\\.?\\d*)\\s*fps',\n",
    "        r'speed\\s*[\\-:]\\s*(\\d+\\.?\\d*)\\s*fps'\n",
    "    ]\n",
    "    \n",
    "    text = f\"{model_name} {description}\"\n",
    "    \n",
    "    for pattern in fps_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all object detection models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'object_detection')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        \n",
    "                        # Try to extract FPS if available in model name or description\n",
    "                        if 'description' in df.columns and 'model_name' in df.columns:\n",
    "                            fps_values = []\n",
    "                            for _, row in df.iterrows():\n",
    "                                fps = extract_inference_speed(row.get('model_name', ''), row.get('description', ''))\n",
    "                                fps_values.append(fps)\n",
    "                            df['inference_fps'] = fps_values\n",
    "                        \n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'object_detection', 'all_object_detection_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'object_detection', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "            \n",
    "            # Analyze trends in performance metrics\n",
    "            if len(combined_df) > 0:\n",
    "                print(\"Analyzing performance trends...\")\n",
    "                # For COCO dataset specifically (if present)\n",
    "                coco_data = combined_df[combined_df['dataset'].str.contains('coco', case=False, na=False)]\n",
    "                if len(coco_data) > 0:\n",
    "                    # Check common metrics in COCO\n",
    "                    metric_cols = [col for col in coco_data.columns if any(m in col.lower() for m in ['map', 'ap', 'accuracy', 'f1'])]\n",
    "                    if metric_cols:\n",
    "                        # Create yearly metrics summary\n",
    "                        yearly_metrics = coco_data.groupby('year')[metric_cols].mean().reset_index()\n",
    "                        yearly_metrics.to_csv(os.path.join(data_dir, 'object_detection', 'coco_yearly_metrics.csv'), index=False)\n",
    "                        print(f\"Created yearly metrics summary for COCO data\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting object detection data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_object_detection_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No object detection benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} object detection benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Object detection data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af0f651-6b19-4dee-be0f-35c89506320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting representation learning data collection (20+ datapoints)...\n",
      "Finding representation learning benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 12 potential representation learning benchmarks\n",
      "Checking self-supervised-image-classification-on-1...\n",
      "self-supervised-image-classification-on-1: 65 datapoints\n",
      "Added self-supervised-image-classification-on-1 with 65 datapoints\n",
      "Checking self-supervised-image-classification-on...\n",
      "self-supervised-image-classification-on: 142 datapoints\n",
      "Added self-supervised-image-classification-on with 142 datapoints\n",
      "Checking representation-learning-on-imagenet...\n",
      "representation-learning-on-imagenet: 0 datapoints\n",
      "Checking representation-learning-on-cifar-10...\n",
      "representation-learning-on-cifar-10: 0 datapoints\n",
      "Checking representation-learning-on-cifar-100...\n",
      "representation-learning-on-cifar-100: 0 datapoints\n",
      "Checking contrastive-learning-on-imagenet...\n",
      "contrastive-learning-on-imagenet: 0 datapoints\n",
      "Checking unsupervised-image-classification-on-imagenet...\n",
      "unsupervised-image-classification-on-imagenet: 8 datapoints\n",
      "Checking representation-learning-on-scidocs...\n",
      "representation-learning-on-scidocs: 7 datapoints\n",
      "Checking representation-learning-on-circle-data...\n",
      "representation-learning-on-circle-data: 1 datapoints\n",
      "Checking representation-learning-on-sports10...\n",
      "representation-learning-on-sports10: 1 datapoints\n",
      "Checking representation-learning-on-cifar10...\n",
      "representation-learning-on-cifar10: 1 datapoints\n",
      "Checking representation-learning-on-animals-10...\n",
      "representation-learning-on-animals-10: 1 datapoints\n",
      "Saved 2 benchmarks with 20+ datapoints\n",
      "Found 2 representation learning benchmarks with 20+ datapoints\n",
      "Scraping data for self-supervised-image-classification-on-1...\n",
      "Saved 65 models for self-supervised-image-classification-on-1\n",
      "Scraping data for self-supervised-image-classification-on...\n",
      "Saved 142 models for self-supervised-image-classification-on\n",
      "Collected a total of 207 models across all benchmarks\n",
      "Combined 207 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Created yearly metrics analysis\n",
      "Representation learning data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'representation_learning'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_training_data_size(text):\n",
    "    \"\"\"Extract training dataset size from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for dataset sizes\n",
    "    patterns = [\n",
    "        r'trained on\\s*(\\d+\\.?\\d*)\\s*[Mm]illion\\s*images',\n",
    "        r'trained on\\s*(\\d+\\.?\\d*)\\s*[Bb]illion\\s*images',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*training samples',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*training samples',\n",
    "        r'dataset of\\s*(\\d+\\.?\\d*)\\s*[Mm]illion',\n",
    "        r'dataset of\\s*(\\d+\\.?\\d*)\\s*[Bb]illion',\n",
    "        r'dataset with\\s*(\\d+\\.?\\d*)\\s*[Mm]illion',\n",
    "        r'dataset with\\s*(\\d+\\.?\\d*)\\s*[Bb]illion',\n",
    "        r'trained with\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'trained with\\s*(\\d+\\.?\\d*)\\s*[Bb]'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            data_size = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b training' in text.lower():\n",
    "                data_size *= 1000  # Convert billions to millions\n",
    "            return data_size\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_representation_learning_benchmarks_with_20plus():\n",
    "    \"\"\"Get representation learning benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding representation learning benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/self-supervised-image-classification-on-1\",\n",
    "        \"https://paperswithcode.com/sota/self-supervised-image-classification-on\",\n",
    "        \"https://paperswithcode.com/sota/representation-learning-on-imagenet\",\n",
    "        \"https://paperswithcode.com/sota/representation-learning-on-cifar-10\",\n",
    "        \"https://paperswithcode.com/sota/representation-learning-on-cifar-100\",\n",
    "        \"https://paperswithcode.com/sota/contrastive-learning-on-imagenet\",\n",
    "        \"https://paperswithcode.com/sota/unsupervised-image-classification-on-imagenet\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main representation learning page\n",
    "        driver.get(\"https://paperswithcode.com/task/representation-learning\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and any(term in href.lower() for term in [\"representation\", \"self-supervised\", \"contrastive\", \"unsupervised\", \"embedding\"]):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential representation learning benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'representation_learning', 'representation_learning_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def extract_compute_from_text(text):\n",
    "    \"\"\"Extract compute resources (FLOPs) from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Patterns for compute resources\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Pp]eta[Ff][Ll][Oo][Pp]s?',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Pp][Ff][Ll][Oo][Pp]s?',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Ee]xa[Ff][Ll][Oo][Pp]s?',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Ee][Ff][Ll][Oo][Pp]s?',\n",
    "        r'compute:\\s*(\\d+\\.?\\d*)\\s*[Pp]',\n",
    "        r'compute:\\s*(\\d+\\.?\\d*)\\s*[Ee]',\n",
    "        r'trained with\\s*(\\d+\\.?\\d*)\\s*[Pp]',\n",
    "        r'trained with\\s*(\\d+\\.?\\d*)\\s*[Ee]'\n",
    "    ]\n",
    "    \n",
    "    units = {\n",
    "        'p': 10**15,  # petaflops\n",
    "        'e': 10**18   # exaflops\n",
    "    }\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = float(match.group(1))\n",
    "            unit = text[match.end()-1:match.end()].lower()\n",
    "            if unit in units:\n",
    "                # Convert to petaflops\n",
    "                if unit == 'e':\n",
    "                    value *= 1000  # Convert exaflops to petaflops\n",
    "                return value\n",
    "    \n",
    "    return None\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'representation_learning', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count, training data size, and compute\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                dataset_size = extract_training_data_size(model_desc)\n",
    "                                compute = extract_compute_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'dataset_size_millions': dataset_size,\n",
    "                                        'compute_petaflops': compute,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'representation_learning', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all representation learning models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'representation_learning')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'representation_learning', 'all_representation_learning_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'representation_learning', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "            \n",
    "            # Analyze parameter scaling trends\n",
    "            if 'parameters_millions' in combined_df.columns:\n",
    "                param_df = combined_df[combined_df['parameters_millions'].notna()]\n",
    "                if len(param_df) > 0:\n",
    "                    yearly_params = param_df.groupby('year')['parameters_millions'].agg(['mean', 'median', 'max']).reset_index()\n",
    "                    yearly_params.to_csv(os.path.join(data_dir, 'representation_learning', 'yearly_parameter_scaling.csv'), index=False)\n",
    "                    print(f\"Created yearly parameter scaling analysis\")\n",
    "            \n",
    "            # Analyze compute trends\n",
    "            if 'compute_petaflops' in combined_df.columns:\n",
    "                compute_df = combined_df[combined_df['compute_petaflops'].notna()]\n",
    "                if len(compute_df) > 0:\n",
    "                    yearly_compute = compute_df.groupby('year')['compute_petaflops'].agg(['mean', 'median', 'max']).reset_index()\n",
    "                    yearly_compute.to_csv(os.path.join(data_dir, 'representation_learning', 'yearly_compute_scaling.csv'), index=False)\n",
    "                    print(f\"Created yearly compute scaling analysis\")\n",
    "            \n",
    "            # Analyze dataset size trends\n",
    "            if 'dataset_size_millions' in combined_df.columns:\n",
    "                dataset_df = combined_df[combined_df['dataset_size_millions'].notna()]\n",
    "                if len(dataset_df) > 0:\n",
    "                    yearly_dataset = dataset_df.groupby('year')['dataset_size_millions'].agg(['mean', 'median', 'max']).reset_index()\n",
    "                    yearly_dataset.to_csv(os.path.join(data_dir, 'representation_learning', 'yearly_dataset_scaling.csv'), index=False)\n",
    "                    print(f\"Created yearly dataset size scaling analysis\")\n",
    "            \n",
    "            # Create analysis of common metrics across datasets\n",
    "            metric_cols = [col for col in combined_df.columns if col not in ['dataset', 'model_name', 'paper_title', \n",
    "                                                                            'paper_url', 'code_url', 'description', \n",
    "                                                                            'parameters_millions', 'dataset_size_millions', \n",
    "                                                                            'compute_petaflops', 'date', 'year', 'month', 'day']]\n",
    "            \n",
    "            # Find common metrics with numeric values\n",
    "            numeric_metrics = []\n",
    "            for col in metric_cols:\n",
    "                if pd.api.types.is_numeric_dtype(combined_df[col]):\n",
    "                    numeric_metrics.append(col)\n",
    "            \n",
    "            if numeric_metrics:\n",
    "                yearly_metrics = combined_df.groupby(['dataset', 'year'])[numeric_metrics].agg(['mean', 'max']).reset_index()\n",
    "                metric_file = os.path.join(data_dir, 'representation_learning', 'yearly_metric_analysis.csv')\n",
    "                yearly_metrics.to_csv(metric_file, index=False)\n",
    "                print(f\"Created yearly metrics analysis\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting representation learning data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_representation_learning_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No representation learning benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} representation learning benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Representation learning data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d6645e-4bad-47f7-b1c3-6f1a2523ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting classification data collection (20+ datapoints)...\n",
      "Finding classification benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 65 potential classification benchmarks\n",
      "Checking image-classification-on-imagenet...\n",
      "image-classification-on-imagenet: 1058 datapoints\n",
      "Added image-classification-on-imagenet with 1058 datapoints\n",
      "Checking image-classification-on-cifar-10...\n",
      "image-classification-on-cifar-10: 264 datapoints\n",
      "Added image-classification-on-cifar-10 with 264 datapoints\n",
      "Checking image-classification-on-cifar-100...\n",
      "image-classification-on-cifar-100: 210 datapoints\n",
      "Added image-classification-on-cifar-100 with 210 datapoints\n",
      "Checking fine-grained-image-classification-on-oxford...\n",
      "fine-grained-image-classification-on-oxford: 25 datapoints\n",
      "Added fine-grained-image-classification-on-oxford with 25 datapoints\n",
      "Checking fine-grained-image-classification-on-stanford...\n",
      "fine-grained-image-classification-on-stanford: 82 datapoints\n",
      "Added fine-grained-image-classification-on-stanford with 82 datapoints\n",
      "Checking image-classification-on-tiny-imagenet-200...\n",
      "image-classification-on-tiny-imagenet-200: 0 datapoints\n",
      "Checking image-classification-on-mnist...\n",
      "image-classification-on-mnist: 80 datapoints\n",
      "Added image-classification-on-mnist with 80 datapoints\n",
      "Checking classification-on-n-imagenet...\n",
      "classification-on-n-imagenet: 9 datapoints\n",
      "Checking classification-on-indl...\n",
      "classification-on-indl: 9 datapoints\n",
      "Checking classification-on-mhist...\n",
      "classification-on-mhist: 9 datapoints\n",
      "Checking classification-on-spot-10...\n",
      "classification-on-spot-10: 9 datapoints\n",
      "Checking classification-on-full-body-parkinsons...\n",
      "classification-on-full-body-parkinsons: 7 datapoints\n",
      "Checking classification-on-n-cars...\n",
      "classification-on-n-cars: 6 datapoints\n",
      "Checking classification-on-n-imagenet-mini...\n",
      "classification-on-n-imagenet-mini: 6 datapoints\n",
      "Checking classification-on-imagenet-c-ood-class-out-of...\n",
      "classification-on-imagenet-c-ood-class-out-of: 5 datapoints\n",
      "Checking classification-on-autoimmune-dataset...\n",
      "classification-on-autoimmune-dataset: 4 datapoints\n",
      "Checking classification-on-cwru-bearing-dataset...\n",
      "classification-on-cwru-bearing-dataset: 3 datapoints\n",
      "Checking classification-on-forgerynet...\n",
      "classification-on-forgerynet: 3 datapoints\n",
      "Checking classification-on-shd-adding...\n",
      "classification-on-shd-adding: 3 datapoints\n",
      "Checking classification-on-ximagenet-12...\n",
      "classification-on-ximagenet-12: 3 datapoints\n",
      "Checking classification-on-rsscn7...\n",
      "classification-on-rsscn7: 2 datapoints\n",
      "Checking classification-on-reddit-ideology-database...\n",
      "classification-on-reddit-ideology-database: 2 datapoints\n",
      "Checking classification-on-medsecid...\n",
      "classification-on-medsecid: 2 datapoints\n",
      "Checking classification-on-bioscan-1m-insect-dataset...\n",
      "classification-on-bioscan-1m-insect-dataset: 2 datapoints\n",
      "Checking classification-on-sst-2...\n",
      "classification-on-sst-2: 2 datapoints\n",
      "Checking classification-on-cb...\n",
      "classification-on-cb: 2 datapoints\n",
      "Checking classification-on-wsc...\n",
      "classification-on-wsc: 2 datapoints\n",
      "Checking classification-on-wic...\n",
      "classification-on-wic: 2 datapoints\n",
      "Checking classification-on-rte...\n",
      "classification-on-rte: 2 datapoints\n",
      "Checking classification-on-boolq...\n",
      "classification-on-boolq: 2 datapoints\n",
      "Checking classification-on-burr-classification-images...\n",
      "classification-on-burr-classification-images: 3 datapoints\n",
      "Checking classification-on-biasbios...\n",
      "classification-on-biasbios: 1 datapoints\n",
      "Checking classification-on-brain-tumor-mri-dataset...\n",
      "classification-on-brain-tumor-mri-dataset: 1 datapoints\n",
      "Checking classification-on-isic-2019...\n",
      "classification-on-isic-2019: 1 datapoints\n",
      "Checking classification-on-sgd...\n",
      "classification-on-sgd: 1 datapoints\n",
      "Checking classification-on-chest-x-ray-images...\n",
      "classification-on-chest-x-ray-images: 1 datapoints\n",
      "Checking classification-on-bengali-ekman-s-six-basic...\n",
      "classification-on-bengali-ekman-s-six-basic: 1 datapoints\n",
      "Checking classification-on-hows...\n",
      "classification-on-hows: 1 datapoints\n",
      "Checking classification-on-hows-long...\n",
      "classification-on-hows-long: 1 datapoints\n",
      "Checking classification-on-sentiment140...\n",
      "classification-on-sentiment140: 1 datapoints\n",
      "Checking classification-on-mixedwm38...\n",
      "classification-on-mixedwm38: 1 datapoints\n",
      "Checking classification-on-corbel...\n",
      "classification-on-corbel: 1 datapoints\n",
      "Checking classification-on-cifake-real-and-ai...\n",
      "classification-on-cifake-real-and-ai: 1 datapoints\n",
      "Checking classification-on-sound-based-drone-fault...\n",
      "classification-on-sound-based-drone-fault: 1 datapoints\n",
      "Checking classification-on-irfl-image-recognition-of...\n",
      "classification-on-irfl-image-recognition-of: 1 datapoints\n",
      "Checking classification-on-mured-dataset...\n",
      "classification-on-mured-dataset: 1 datapoints\n",
      "Checking classification-on-kepler-exoplanet-search...\n",
      "classification-on-kepler-exoplanet-search: 1 datapoints\n",
      "Checking classification-on-tcga...\n",
      "classification-on-tcga: 1 datapoints\n",
      "Checking classification-on-cifar-100...\n",
      "classification-on-cifar-100: 1 datapoints\n",
      "Checking classification-on-adult...\n",
      "classification-on-adult: 1 datapoints\n",
      "Checking classification-on-hrf...\n",
      "classification-on-hrf: 1 datapoints\n",
      "Checking classification-on-rite...\n",
      "classification-on-rite: 1 datapoints\n",
      "Checking classification-on-les-av...\n",
      "classification-on-les-av: 1 datapoints\n",
      "Checking classification-on-aur-umb-dataset...\n",
      "classification-on-aur-umb-dataset: 1 datapoints\n",
      "Checking classification-on-diat-mradhar-radar-micro...\n",
      "classification-on-diat-mradhar-radar-micro: 1 datapoints\n",
      "Checking classification-on-covid-19-image-data...\n",
      "classification-on-covid-19-image-data: 1 datapoints\n",
      "Checking classification-on-tml1m...\n",
      "classification-on-tml1m: 1 datapoints\n",
      "Checking classification-on-tlf2k...\n",
      "classification-on-tlf2k: 1 datapoints\n",
      "Checking classification-on-tacm12k...\n",
      "classification-on-tacm12k: 1 datapoints\n",
      "Checking classification-on-respiratorydatabase-tr...\n",
      "classification-on-respiratorydatabase-tr: 1 datapoints\n",
      "Checking classification-on-cifar-10c...\n",
      "classification-on-cifar-10c: 1 datapoints\n",
      "Checking classification-on-insider-threat-test-dataset...\n",
      "classification-on-insider-threat-test-dataset: 1 datapoints\n",
      "Checking classification-on-coordinated-reply-attacks...\n",
      "classification-on-coordinated-reply-attacks: 1 datapoints\n",
      "Checking classification-on-liver-us...\n",
      "classification-on-liver-us: 1 datapoints\n",
      "Checking classification-on-simgas...\n",
      "classification-on-simgas: 1 datapoints\n",
      "Saved 6 benchmarks with 20+ datapoints\n",
      "Found 6 classification benchmarks with 20+ datapoints\n",
      "Scraping data for image-classification-on-imagenet...\n",
      "Saved 1043 models for image-classification-on-imagenet\n",
      "Scraping data for image-classification-on-cifar-10...\n",
      "Saved 263 models for image-classification-on-cifar-10\n",
      "Scraping data for image-classification-on-cifar-100...\n",
      "Saved 207 models for image-classification-on-cifar-100\n",
      "Scraping data for fine-grained-image-classification-on-oxford...\n",
      "Saved 25 models for fine-grained-image-classification-on-oxford\n",
      "Scraping data for fine-grained-image-classification-on-stanford...\n",
      "Saved 82 models for fine-grained-image-classification-on-stanford\n",
      "Scraping data for image-classification-on-mnist...\n",
      "Saved 80 models for image-classification-on-mnist\n",
      "Collected a total of 1700 models across all benchmarks\n",
      "Combined 1700 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Classification data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'classification'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_flops_from_text(text):\n",
    "    \"\"\"Extract FLOPs (computational complexity) from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # FLOPs patterns (Billions or Millions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Gg][Ff][Ll][Oo][Pp][Ss]',  # GFLOPs\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm][Ff][Ll][Oo][Pp][Ss]',  # MFLOPs\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*[Ff][Ll][Oo][Pp][Ss]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*[Ff][Ll][Oo][Pp][Ss]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*[Ff][Ll][Oo][Pp][Ss]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*[Ff][Ll][Oo][Pp][Ss]'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            flops = float(match.group(1))\n",
    "            # Convert everything to millions of FLOPs for consistency\n",
    "            if 'gflops' in text.lower() or 'g flops' in text.lower() or 'billion' in text.lower() or 'b flops' in text.lower():\n",
    "                flops *= 1000  # Convert GFLOPs/BFLOPs to MFLOPs\n",
    "            return flops\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_classification_benchmarks_with_20plus():\n",
    "    \"\"\"Get classification benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding classification benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/image-classification-on-imagenet\",\n",
    "        \"https://paperswithcode.com/sota/image-classification-on-cifar-10\",\n",
    "        \"https://paperswithcode.com/sota/image-classification-on-cifar-100\",\n",
    "        \"https://paperswithcode.com/sota/fine-grained-image-classification-on-oxford\",\n",
    "        \"https://paperswithcode.com/sota/fine-grained-image-classification-on-stanford\",\n",
    "        \"https://paperswithcode.com/sota/image-classification-on-tiny-imagenet-200\",\n",
    "        \"https://paperswithcode.com/sota/image-classification-on-mnist\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main classification page\n",
    "        driver.get(\"https://paperswithcode.com/task/classification-1\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"classification\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential classification benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'classification', 'classification_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'classification', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count and FLOPs\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                flops = extract_flops_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'flops_millions': flops,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    param_col = None\n",
    "                    flops_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif any(term in header for term in ['param', 'parameters']):\n",
    "                            param_col = i\n",
    "                        elif any(term in header for term in ['flop', 'flops', 'mac', 'macs']):\n",
    "                            flops_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract parameter count if available from dedicated column\n",
    "                            param_count = None\n",
    "                            if param_col is not None:\n",
    "                                param_text = cells[param_col].text.strip()\n",
    "                                param_count = extract_parameters_from_text(param_text)\n",
    "                            \n",
    "                            # Extract FLOPs if available from dedicated column\n",
    "                            flops = None\n",
    "                            if flops_col is not None:\n",
    "                                flops_text = cells[flops_col].text.strip()\n",
    "                                flops = extract_flops_from_text(flops_text)\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'parameters_millions': param_count,\n",
    "                                    'flops_millions': flops,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'classification', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all classification models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'classification')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'classification', 'all_classification_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'classification', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting classification data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_classification_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No classification benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} classification benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Classification data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67880536-2a50-4094-80d8-76d57fd4280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting object task data collection (20+ datapoints)...\n",
      "Finding object benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 6 potential object benchmarks\n",
      "Checking 3d-object-detection-on-kitti-cars-moderate...\n",
      "3d-object-detection-on-kitti-cars-moderate: 0 datapoints\n",
      "Checking 3d-object-detection-on-kitti-cars-easy...\n",
      "3d-object-detection-on-kitti-cars-easy: 26 datapoints\n",
      "Added 3d-object-detection-on-kitti-cars-easy with 26 datapoints\n",
      "Checking 3d-object-detection-on-kitti-cars-hard...\n",
      "3d-object-detection-on-kitti-cars-hard: 25 datapoints\n",
      "Added 3d-object-detection-on-kitti-cars-hard with 25 datapoints\n",
      "Checking 3d-object-detection-on-kitti-cyclists...\n",
      "3d-object-detection-on-kitti-cyclists: 13 datapoints\n",
      "Checking 3d-object-detection-on-kitti-pedestrians...\n",
      "3d-object-detection-on-kitti-pedestrians: 12 datapoints\n",
      "Checking 3d-object-detection-on-nuscenes...\n",
      "3d-object-detection-on-nuscenes: 372 datapoints\n",
      "Added 3d-object-detection-on-nuscenes with 372 datapoints\n",
      "Saved 3 benchmarks with 20+ datapoints\n",
      "Found 3 object benchmarks with 20+ datapoints\n",
      "Scraping data for 3d-object-detection-on-kitti-cars-easy...\n",
      "Saved 26 models for 3d-object-detection-on-kitti-cars-easy\n",
      "Scraping data for 3d-object-detection-on-kitti-cars-hard...\n",
      "Saved 25 models for 3d-object-detection-on-kitti-cars-hard\n",
      "Scraping data for 3d-object-detection-on-nuscenes...\n",
      "Saved 372 models for 3d-object-detection-on-nuscenes\n",
      "Collected a total of 423 models across all benchmarks\n",
      "Combined 423 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Object task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'object'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_object_benchmarks_with_20plus():\n",
    "    \"\"\"Get object benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding object benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-cars-moderate\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-cars-easy\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-cars-hard\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-cyclists\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-pedestrians\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-nuscenes\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main object page\n",
    "        driver.get(\"https://paperswithcode.com/task/object\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"object\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential object benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'object', 'object_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'object', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'object', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all object models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'object')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'object', 'all_object_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'object', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting object task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_object_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No object benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} object benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Object task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912949f4-031a-4730-a383-f3e076cca15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'object'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_object_benchmarks_with_20plus():\n",
    "    \"\"\"Get object benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding object benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-cars-moderate\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-cars-easy\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-cars-hard\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-cyclists\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-kitti-pedestrians\",\n",
    "        \"https://paperswithcode.com/sota/3d-object-detection-on-nuscenes\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main object page\n",
    "        driver.get(\"https://paperswithcode.com/task/object\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"object\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential object benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'object', 'object_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'object', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'object', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all object models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'object')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'object', 'all_object_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'object', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting object task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_object_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No object benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} object benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Object task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb7dfc-8da4-43ca-b8d9-ad466c638281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image classification data collection (20+ datapoints)...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "class ImageClassificationScraper:\n",
    "    def __init__(self):\n",
    "        # Base directory for saving data\n",
    "        self.home_dir = os.path.expanduser(\"~\")\n",
    "        self.data_dir = os.path.join(\n",
    "            self.home_dir, \n",
    "            \"Documents\", \n",
    "            \"Jupyter Notebooks\", \n",
    "            \"RA Task\", \n",
    "            \"tech_progress_data\"\n",
    "        )\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.data_dir, 'image'), exist_ok=True)\n",
    "\n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()), \n",
    "            options=chrome_options\n",
    "        )\n",
    "        driver.set_page_load_timeout(30)\n",
    "        return driver\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_date_from_text(text: str) -> Optional[datetime.date]:\n",
    "        \"\"\"Extract a date from text using various heuristics\"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Try direct parsing\n",
    "            return parser.parse(text, fuzzy=True).date()\n",
    "        except:\n",
    "            # Look for year patterns\n",
    "            year_match = re.search(r'20[0-2][0-9]', text)\n",
    "            if year_match:\n",
    "                year = int(year_match.group())\n",
    "                # Look for month patterns\n",
    "                month_match = re.search(\n",
    "                    r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', \n",
    "                    text, \n",
    "                    re.IGNORECASE\n",
    "                )\n",
    "                if month_match:\n",
    "                    month_text = month_match.group().lower()\n",
    "                    month_map = {\n",
    "                        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                        'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                    }\n",
    "                    for prefix, month_num in month_map.items():\n",
    "                        if month_text.startswith(prefix):\n",
    "                            return datetime.date(year, month_num, 1)\n",
    "                \n",
    "                # If only year was found, default to January\n",
    "                return datetime.date(year, 1, 1)\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_parameters_from_text(text: str) -> Optional[float]:\n",
    "        \"\"\"Extract parameter count from model description\"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        \n",
    "        # Common patterns for parameter counts (millions, billions)\n",
    "        patterns = [\n",
    "            r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "            r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "            r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "            r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "            r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "            r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "            r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "            r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "            r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "            r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                param_count = float(match.group(1))\n",
    "                # Convert to millions\n",
    "                if any(term in text.lower() for term in ['billion', 'b params', 'b parameters']):\n",
    "                    param_count *= 1000  # Convert billions to millions\n",
    "                return param_count\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def find_benchmarks(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find image classification benchmarks with at least 20 datapoints\"\"\"\n",
    "        # Major benchmarks to start with\n",
    "        major_benchmarks = [\n",
    "            \"https://paperswithcode.com/sota/image-classification-on-imagenet\",\n",
    "            \"https://paperswithcode.com/sota/image-classification-on-cifar-10\",\n",
    "            \"https://paperswithcode.com/sota/image-classification-on-cifar-100\",\n",
    "            \"https://paperswithcode.com/sota/image-classification-on-caltech-101\",\n",
    "            \"https://paperswithcode.com/sota/image-classification-on-stl-10\"\n",
    "        ]\n",
    "        \n",
    "        driver = self._setup_driver()\n",
    "        benchmarks_with_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigate to image classification page\n",
    "            driver.get(\"https://paperswithcode.com/task/image-classification\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Collect benchmark links\n",
    "            links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            benchmark_links = set(major_benchmarks)\n",
    "            \n",
    "            # Add additional benchmarks from page\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href and \"/sota/\" in href and \"image-classification\" in href.lower():\n",
    "                    benchmark_links.add(href)\n",
    "            \n",
    "            # Process each benchmark\n",
    "            for benchmark_url in benchmark_links:\n",
    "                try:\n",
    "                    benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                    driver.get(benchmark_url)\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "                    # Find timeline link\n",
    "                    timeline_url = None\n",
    "                    timeline_links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                    for link in timeline_links:\n",
    "                        href = link.get_attribute(\"href\")\n",
    "                        if href and \"sota-over-time\" in href:\n",
    "                            timeline_url = href\n",
    "                            break\n",
    "                    \n",
    "                    # Count datapoints\n",
    "                    max_datapoints = 0\n",
    "                    tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                    for table in tables:\n",
    "                        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                        datapoints = len(rows) - 1  # Subtract header\n",
    "                        max_datapoints = max(max_datapoints, datapoints)\n",
    "                    \n",
    "                    # Add if 20+ datapoints\n",
    "                    if max_datapoints >= 20:\n",
    "                        benchmarks_with_data.append({\n",
    "                            \"name\": benchmark_name,\n",
    "                            \"url\": benchmark_url,\n",
    "                            \"timeline_url\": timeline_url,\n",
    "                            \"datapoints\": max_datapoints\n",
    "                        })\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {benchmark_url}: {e}\")\n",
    "            \n",
    "            # Save benchmark list\n",
    "            if benchmarks_with_data:\n",
    "                df = pd.DataFrame(benchmarks_with_data)\n",
    "                df.to_csv(\n",
    "                    os.path.join(self.data_dir, 'image', 'image_benchmarks_20plus.csv'), \n",
    "                    index=False\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding benchmarks: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "        \n",
    "        return benchmarks_with_data\n",
    "\n",
    "    def scrape_benchmark_data(self, benchmark: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Scrape model data from a specific benchmark\"\"\"\n",
    "        driver = self._setup_driver()\n",
    "        models_data = []\n",
    "        \n",
    "        try:\n",
    "            # Use timeline URL if available\n",
    "            url = benchmark.get('timeline_url', benchmark['url'])\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Find and extract data\n",
    "            scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "            json_data_found = False\n",
    "            \n",
    "            for script in scripts:\n",
    "                script_text = script.get_attribute(\"innerHTML\")\n",
    "                if \"window.INITIAL_DATA\" in script_text:\n",
    "                    json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        try:\n",
    "                            data = json.loads(json_match.group(1))\n",
    "                            \n",
    "                            # Save raw JSON\n",
    "                            benchmark_dir = os.path.join(self.data_dir, 'image', benchmark['name'])\n",
    "                            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                            with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                                json.dump(data, f, indent=2)\n",
    "                            \n",
    "                            # Process SOTA data\n",
    "                            if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                                json_data_found = True\n",
    "                                for eval_data in data['sota']['evaluations']:\n",
    "                                    # Extract publication date\n",
    "                                    date_str = eval_data.get('date')\n",
    "                                    pub_date = self._extract_date_from_text(date_str) if date_str else None\n",
    "                                    \n",
    "                                    # Extract model info\n",
    "                                    model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                    model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                    \n",
    "                                    # Only add if we have date and model name\n",
    "                                    if pub_date and model_name:\n",
    "                                        model_entry = {\n",
    "                                            'dataset': benchmark['name'],\n",
    "                                            'model_name': model_name,\n",
    "                                            'paper_title': eval_data.get('paper', {}).get('title', ''),\n",
    "                                            'paper_url': eval_data.get('paper', {}).get('url', ''),\n",
    "                                            'code_url': eval_data.get('code', {}).get('url', '') if eval_data.get('code') else '',\n",
    "                                            'description': model_desc,\n",
    "                                            'parameters_millions': self._extract_parameters_from_text(model_desc),\n",
    "                                            'date': pub_date,\n",
    "                                            'year': pub_date.year,\n",
    "                                            'month': pub_date.month,\n",
    "                                            'day': pub_date.day,\n",
    "                                            **{\n",
    "                                                metric.get('name'): float(metric.get('value', '').replace('%', ''))\n",
    "                                                for metric in eval_data.get('metrics', [])\n",
    "                                                if metric.get('name') and metric.get('value')\n",
    "                                            }\n",
    "                                        }\n",
    "                                        models_data.append(model_entry)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error parsing JSON data: {e}\")\n",
    "            \n",
    "            # If no JSON data, try table extraction (fallback method)\n",
    "            if not json_data_found:\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                for table in tables:\n",
    "                    try:\n",
    "                        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                        if len(rows) <= 1:\n",
    "                            continue\n",
    "                        \n",
    "                        # Extract header information\n",
    "                        headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                        date_col = next((i for i, h in enumerate(headers) if any(term in h for term in ['date', 'published', 'year'])), None)\n",
    "                        model_col = next((i for i, h in enumerate(headers) if any(term in h for term in ['model', 'method', 'name'])), None)\n",
    "                        \n",
    "                        # Process rows if we have date and model columns\n",
    "                        if date_col is not None and model_col is not None:\n",
    "                            for row in rows[1:]:\n",
    "                                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                                if len(cells) != len(headers):\n",
    "                                    continue\n",
    "                                \n",
    "                                # Extract date and model name\n",
    "                                date_text = cells[date_col].text.strip()\n",
    "                                pub_date = self._extract_date_from_text(date_text)\n",
    "                                model_name = cells[model_col].text.strip()\n",
    "                                \n",
    "                                # Get paper link\n",
    "                                paper_url = \"\"\n",
    "                                paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                                if paper_links:\n",
    "                                    paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for i, header in enumerate(headers):\n",
    "                                    if header not in ['paper', 'code', 'link', headers[date_col], headers[model_col]]:\n",
    "                                        value = cells[i].text.strip()\n",
    "                                        try:\n",
    "                                            metrics[header] = float(value.replace('%', ''))\n",
    "                                        except:\n",
    "                                            metrics[header] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing table: {e}\")\n",
    "            \n",
    "            # Save models for this benchmark\n",
    "            if models_data:\n",
    "                benchmark_dir = os.path.join(self.data_dir, 'image', benchmark['name'])\n",
    "                os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                \n",
    "                models_df = pd.DataFrame(models_data)\n",
    "                models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping benchmark data: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "        \n",
    "        return models_data\n",
    "\n",
    "    def combine_models(self):\n",
    "        \"\"\"Combine all collected models into a single dataset\"\"\"\n",
    "        try:\n",
    "            # Collect all model CSV files\n",
    "            all_models = []\n",
    "            for root, _, files in os.walk(os.path.join(self.data_dir, 'image')):\n",
    "                for file in files:\n",
    "                    if file == \"models.csv\":\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        try:\n",
    "                            df = pd.read_csv(file_path)\n",
    "                            all_models.append(df)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reading {file_path}: {e}\")\n",
    "# Combine and process models\n",
    "            if all_models:\n",
    "                combined_df = pd.concat(all_models, ignore_index=True)\n",
    "                \n",
    "                # Ensure date is properly formatted\n",
    "                if 'date' in combined_df.columns:\n",
    "                    combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "                \n",
    "                # Sort by date\n",
    "                combined_df = combined_df.sort_values('date')\n",
    "                \n",
    "                # Save combined models\n",
    "                combined_output_path = os.path.join(self.data_dir, 'image', 'all_image_models.csv')\n",
    "                combined_df.to_csv(combined_output_path, index=False)\n",
    "                print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "                \n",
    "                # Create yearly summary\n",
    "                yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "                yearly_summary_path = os.path.join(self.data_dir, 'image', 'yearly_model_count.csv')\n",
    "                yearly_summary.to_csv(yearly_summary_path, index=False)\n",
    "                print(\"Created yearly model count summary\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution method for image classification data collection\"\"\"\n",
    "    print(\"Starting image classification data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = ImageClassificationScraper()\n",
    "    \n",
    "    # Find benchmarks with 20+ datapoints\n",
    "    benchmarks = scraper.find_benchmarks()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No image classification benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} image classification benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            # Scrape models for the benchmark\n",
    "            models = scraper.scrape_benchmark_data(benchmark)\n",
    "            \n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Pause between benchmarks to be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all collected models\n",
    "    scraper.combine_models()\n",
    "    \n",
    "    print(\"Image classification data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc21dd8-bc97-44b3-8ae5-1a353fff27a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting text classification task data collection (20+ datapoints)...\n",
      "Finding text classification benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 69 potential text classification benchmarks\n",
      "Checking text-classification-on-imdb...\n",
      "text-classification-on-imdb: 0 datapoints\n",
      "Checking text-classification-on-ag-news...\n",
      "text-classification-on-ag-news: 21 datapoints\n",
      "Added text-classification-on-ag-news with 21 datapoints\n",
      "Checking text-classification-on-trec-fine-grained...\n",
      "text-classification-on-trec-fine-grained: 0 datapoints\n",
      "Checking text-classification-on-dbpedia...\n",
      "text-classification-on-dbpedia: 21 datapoints\n",
      "Added text-classification-on-dbpedia with 21 datapoints\n",
      "Checking sentiment-analysis-on-sst-2-binary...\n",
      "sentiment-analysis-on-sst-2-binary: 88 datapoints\n",
      "Added sentiment-analysis-on-sst-2-binary with 88 datapoints\n",
      "Checking text-classification-on-sst-5...\n",
      "text-classification-on-sst-5: 0 datapoints\n",
      "Checking text-classification-on-mteb...\n",
      "text-classification-on-mteb: 31 datapoints\n",
      "Added text-classification-on-mteb with 31 datapoints\n",
      "Checking text-classification-on-r8...\n",
      "text-classification-on-r8: 21 datapoints\n",
      "Added text-classification-on-r8 with 21 datapoints\n",
      "Checking text-classification-on-trec-6...\n",
      "text-classification-on-trec-6: 19 datapoints\n",
      "Checking text-classification-on-20news...\n",
      "text-classification-on-20news: 16 datapoints\n",
      "Checking text-classification-on-uk-key-stage...\n",
      "text-classification-on-uk-key-stage: 15 datapoints\n",
      "Checking text-classification-on-ohsumed...\n",
      "text-classification-on-ohsumed: 10 datapoints\n",
      "Checking text-classification-on-yahoo-answers...\n",
      "text-classification-on-yahoo-answers: 10 datapoints\n",
      "Checking text-classification-on-mr...\n",
      "text-classification-on-mr: 10 datapoints\n",
      "Checking text-classification-on-r52...\n",
      "text-classification-on-r52: 8 datapoints\n",
      "Checking text-classification-on-newsdiscourse...\n",
      "text-classification-on-newsdiscourse: 8 datapoints\n",
      "Checking text-classification-on-yelp-5...\n",
      "text-classification-on-yelp-5: 7 datapoints\n",
      "Checking text-classification-on-yelp-2...\n",
      "text-classification-on-yelp-2: 5 datapoints\n",
      "Checking text-classification-on-dodf-data...\n",
      "text-classification-on-dodf-data: 5 datapoints\n",
      "Checking text-classification-on-mvictor-type...\n",
      "text-classification-on-mvictor-type: 5 datapoints\n",
      "Checking text-classification-on-svictor-type...\n",
      "text-classification-on-svictor-type: 5 datapoints\n",
      "Checking text-classification-on-weebit-readability...\n",
      "text-classification-on-weebit-readability: 5 datapoints\n",
      "Checking text-classification-on-onestopenglish...\n",
      "text-classification-on-onestopenglish: 5 datapoints\n",
      "Checking text-classification-on-lot-insts...\n",
      "text-classification-on-lot-insts: 5 datapoints\n",
      "Checking text-classification-on-amazon-2...\n",
      "text-classification-on-amazon-2: 4 datapoints\n",
      "Checking text-classification-on-rcv1...\n",
      "text-classification-on-rcv1: 4 datapoints\n",
      "Checking text-classification-on-arxiv-10...\n",
      "text-classification-on-arxiv-10: 4 datapoints\n",
      "Checking text-classification-on-hatexplain-1...\n",
      "text-classification-on-hatexplain-1: 4 datapoints\n",
      "Checking text-classification-on-threatgram-101-extreme...\n",
      "text-classification-on-threatgram-101-extreme: 4 datapoints\n",
      "Checking text-classification-on-sogou-news...\n",
      "text-classification-on-sogou-news: 3 datapoints\n",
      "Checking text-classification-on-amazon-5...\n",
      "text-classification-on-amazon-5: 3 datapoints\n",
      "Checking text-classification-on-overruling...\n",
      "text-classification-on-overruling: 3 datapoints\n",
      "Checking text-classification-on-terms-of-service...\n",
      "text-classification-on-terms-of-service: 3 datapoints\n",
      "Checking text-classification-on-blurb...\n",
      "text-classification-on-blurb: 3 datapoints\n",
      "Checking text-classification-on-twitter...\n",
      "text-classification-on-twitter: 3 datapoints\n",
      "Checking text-classification-on-imdb-movie-reviews-1...\n",
      "text-classification-on-imdb-movie-reviews-1: 3 datapoints\n",
      "Checking text-classification-on-trec-50...\n",
      "text-classification-on-trec-50: 2 datapoints\n",
      "Checking text-classification-on-an-amharic-news-text...\n",
      "text-classification-on-an-amharic-news-text: 2 datapoints\n",
      "Checking text-classification-on-glue-sst2...\n",
      "text-classification-on-glue-sst2: 2 datapoints\n",
      "Checking text-classification-on-muld-character-type...\n",
      "text-classification-on-muld-character-type: 2 datapoints\n",
      "Checking text-classification-on-searchsnippets...\n",
      "text-classification-on-searchsnippets: 2 datapoints\n",
      "Checking text-classification-on-sst-2...\n",
      "text-classification-on-sst-2: 2 datapoints\n",
      "Checking text-classification-on-this-is-not-a-dataset...\n",
      "text-classification-on-this-is-not-a-dataset: 2 datapoints\n",
      "Checking text-classification-on-social-media...\n",
      "text-classification-on-social-media: 2 datapoints\n",
      "Checking text-classification-on-trac2-benghali-task-2...\n",
      "text-classification-on-trac2-benghali-task-2: 1 datapoints\n",
      "Checking text-classification-on-trac2-english-task2...\n",
      "text-classification-on-trac2-english-task2: 1 datapoints\n",
      "Checking text-classification-on-affcon-2020-emotion...\n",
      "text-classification-on-affcon-2020-emotion: 1 datapoints\n",
      "Checking text-classification-on-arxiv...\n",
      "text-classification-on-arxiv: 1 datapoints\n",
      "Checking text-classification-on-patents...\n",
      "text-classification-on-patents: 1 datapoints\n",
      "Checking text-classification-on-facebook-media...\n",
      "text-classification-on-facebook-media: 1 datapoints\n",
      "Checking text-classification-on-twitter-us...\n",
      "text-classification-on-twitter-us: 1 datapoints\n",
      "Checking text-classification-on-rusage-corpus-for-age...\n",
      "text-classification-on-rusage-corpus-for-age: 1 datapoints\n",
      "Checking text-classification-on-20-newsgroups...\n",
      "text-classification-on-20-newsgroups: 1 datapoints\n",
      "Checking text-classification-on-silicone-benchmark...\n",
      "text-classification-on-silicone-benchmark: 1 datapoints\n",
      "Checking text-classification-on-wnut-2020-task-2...\n",
      "text-classification-on-wnut-2020-task-2: 1 datapoints\n",
      "Checking text-classification-on-glue-mrpc...\n",
      "text-classification-on-glue-mrpc: 1 datapoints\n",
      "Checking text-classification-on-glue-cola...\n",
      "text-classification-on-glue-cola: 0 datapoints\n",
      "Checking text-classification-on-glue-rte...\n",
      "text-classification-on-glue-rte: 1 datapoints\n",
      "Checking text-classification-on-glue-stsb...\n",
      "text-classification-on-glue-stsb: 0 datapoints\n",
      "Checking text-classification-on-banking77...\n",
      "text-classification-on-banking77: 1 datapoints\n",
      "Checking text-classification-on-adverse-drug-events...\n",
      "text-classification-on-adverse-drug-events: 1 datapoints\n",
      "Checking text-classification-on-trec-10...\n",
      "text-classification-on-trec-10: 1 datapoints\n",
      "Checking text-classification-on-nice-45...\n",
      "text-classification-on-nice-45: 1 datapoints\n",
      "Checking text-classification-on-nice-2...\n",
      "text-classification-on-nice-2: 1 datapoints\n",
      "Checking text-classification-on-stops-41...\n",
      "text-classification-on-stops-41: 1 datapoints\n",
      "Checking text-classification-on-stops-2...\n",
      "text-classification-on-stops-2: 1 datapoints\n",
      "Checking text-classification-on-twitter-sentiment-1...\n",
      "text-classification-on-twitter-sentiment-1: 1 datapoints\n",
      "Checking text-classification-on-fmc-mwo2kg-1...\n",
      "text-classification-on-fmc-mwo2kg-1: 1 datapoints\n",
      "Checking text-classification-on-hyperpartisan-1...\n",
      "text-classification-on-hyperpartisan-1: 1 datapoints\n",
      "Saved 5 benchmarks with 20+ datapoints\n",
      "Found 5 text classification benchmarks with 20+ datapoints\n",
      "Scraping data for text-classification-on-ag-news...\n",
      "Saved 21 models for text-classification-on-ag-news\n",
      "Scraping data for text-classification-on-dbpedia...\n",
      "Saved 21 models for text-classification-on-dbpedia\n",
      "Scraping data for sentiment-analysis-on-sst-2-binary...\n",
      "Saved 88 models for sentiment-analysis-on-sst-2-binary\n",
      "Scraping data for text-classification-on-mteb...\n",
      "Saved 31 models for text-classification-on-mteb\n",
      "Scraping data for text-classification-on-r8...\n",
      "Saved 21 models for text-classification-on-r8\n",
      "Collected a total of 182 models across all benchmarks\n",
      "Combined 182 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Text classification task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'text_classification'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_text_classification_benchmarks_with_20plus():\n",
    "    \"\"\"Get text classification benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding text classification benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-imdb\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-ag-news\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-trec-fine-grained\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-dbpedia\",\n",
    "        \"https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-sst-5\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main text classification page\n",
    "        driver.get(\"https://paperswithcode.com/task/text-classification\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and (\"text-classification\" in href.lower() or \"sentiment\" in href.lower()):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential text classification benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'text_classification', 'text_classification_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'text_classification', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'text_classification', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all text classification models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'text_classification')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'text_classification', 'all_text_classification_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'text_classification', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting text classification task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_text_classification_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No text classification benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} text classification benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Text classification task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cce90e8-8dad-41bf-843c-19e065b990a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting image generation task data collection (20+ datapoints)...\n",
      "Finding image generation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 91 potential image generation benchmarks\n",
      "Checking image-generation-on-ms-coco...\n",
      "image-generation-on-ms-coco: 0 datapoints\n",
      "Checking image-generation-on-cifar-10...\n",
      "image-generation-on-cifar-10: 70 datapoints\n",
      "Added image-generation-on-cifar-10 with 70 datapoints\n",
      "Checking image-generation-on-celeba-64x64...\n",
      "image-generation-on-celeba-64x64: 39 datapoints\n",
      "Added image-generation-on-celeba-64x64 with 39 datapoints\n",
      "Checking image-generation-on-lsun-bedroom-256-x-256...\n",
      "image-generation-on-lsun-bedroom-256-x-256: 32 datapoints\n",
      "Added image-generation-on-lsun-bedroom-256-x-256 with 32 datapoints\n",
      "Checking image-generation-on-imagenet-64x64...\n",
      "image-generation-on-imagenet-64x64: 64 datapoints\n",
      "Added image-generation-on-imagenet-64x64 with 64 datapoints\n",
      "Checking image-generation-on-stl-10...\n",
      "image-generation-on-stl-10: 31 datapoints\n",
      "Added image-generation-on-stl-10 with 31 datapoints\n",
      "Checking image-generation-on-imagenet-256x256...\n",
      "image-generation-on-imagenet-256x256: 79 datapoints\n",
      "Added image-generation-on-imagenet-256x256 with 79 datapoints\n",
      "Checking image-generation-on-ffhq-256-x-256...\n",
      "image-generation-on-ffhq-256-x-256: 51 datapoints\n",
      "Added image-generation-on-ffhq-256-x-256 with 51 datapoints\n",
      "Checking image-generation-on-imagenet-512x512...\n",
      "image-generation-on-imagenet-512x512: 48 datapoints\n",
      "Added image-generation-on-imagenet-512x512 with 48 datapoints\n",
      "Checking image-generation-on-imagenet-32x32...\n",
      "image-generation-on-imagenet-32x32: 33 datapoints\n",
      "Added image-generation-on-imagenet-32x32 with 33 datapoints\n",
      "Checking image-generation-on-lsun-churches-256-x-256...\n",
      "image-generation-on-lsun-churches-256-x-256: 27 datapoints\n",
      "Added image-generation-on-lsun-churches-256-x-256 with 27 datapoints\n",
      "Checking image-generation-on-imagenet-128x128...\n",
      "image-generation-on-imagenet-128x128: 23 datapoints\n",
      "Added image-generation-on-imagenet-128x128 with 23 datapoints\n",
      "Checking image-generation-on-ffhq-1024-x-1024...\n",
      "image-generation-on-ffhq-1024-x-1024: 20 datapoints\n",
      "Added image-generation-on-ffhq-1024-x-1024 with 20 datapoints\n",
      "Checking image-generation-on-celeba-hq-256x256...\n",
      "image-generation-on-celeba-hq-256x256: 19 datapoints\n",
      "Checking image-generation-on-celeba-256x256...\n",
      "image-generation-on-celeba-256x256: 17 datapoints\n",
      "Checking image-generation-on-ffhq-u...\n",
      "image-generation-on-ffhq-u: 13 datapoints\n",
      "Checking image-generation-on-ffhq...\n",
      "image-generation-on-ffhq: 12 datapoints\n",
      "Checking image-generation-on-mnist...\n",
      "image-generation-on-mnist: 15 datapoints\n",
      "Checking image-generation-on-celeba-hq-1024x1024...\n",
      "image-generation-on-celeba-hq-1024x1024: 10 datapoints\n",
      "Checking image-generation-on-binarized-mnist...\n",
      "image-generation-on-binarized-mnist: 10 datapoints\n",
      "Checking image-generation-on-cifar-100...\n",
      "image-generation-on-cifar-100: 9 datapoints\n",
      "Checking image-generation-on-lsun-cat-256-x-256...\n",
      "image-generation-on-lsun-cat-256-x-256: 8 datapoints\n",
      "Checking image-generation-on-afhq-cat...\n",
      "image-generation-on-afhq-cat: 8 datapoints\n",
      "Checking image-generation-on-celeba-hq-128x128...\n",
      "image-generation-on-celeba-hq-128x128: 7 datapoints\n",
      "Checking image-generation-on-afhqv2...\n",
      "image-generation-on-afhqv2: 7 datapoints\n",
      "Checking image-generation-on-lsun-horse-256-x-256...\n",
      "image-generation-on-lsun-horse-256-x-256: 6 datapoints\n",
      "Checking image-generation-on-clevr...\n",
      "image-generation-on-clevr: 6 datapoints\n",
      "Checking image-generation-on-cityscapes...\n",
      "image-generation-on-cityscapes: 6 datapoints\n",
      "Checking image-generation-on-afhq-dog...\n",
      "image-generation-on-afhq-dog: 6 datapoints\n",
      "Checking image-generation-on-fashion-mnist...\n",
      "image-generation-on-fashion-mnist: 7 datapoints\n",
      "Checking image-generation-on-celeba-128x128...\n",
      "image-generation-on-celeba-128x128: 5 datapoints\n",
      "Checking image-generation-on-afhq-wild...\n",
      "image-generation-on-afhq-wild: 5 datapoints\n",
      "Checking image-generation-on-places50...\n",
      "image-generation-on-places50: 5 datapoints\n",
      "Checking image-generation-on-cub-128-x-128...\n",
      "image-generation-on-cub-128-x-128: 4 datapoints\n",
      "Checking image-generation-on-stanford-dogs...\n",
      "image-generation-on-stanford-dogs: 4 datapoints\n",
      "Checking image-generation-on-stanford-cars...\n",
      "image-generation-on-stanford-cars: 4 datapoints\n",
      "Checking image-generation-on-pokemon-256x256...\n",
      "image-generation-on-pokemon-256x256: 4 datapoints\n",
      "Checking image-generation-on-vizdoom...\n",
      "image-generation-on-vizdoom: 4 datapoints\n",
      "Checking image-generation-on-replica...\n",
      "image-generation-on-replica: 4 datapoints\n",
      "Checking image-generation-on-vln-ce...\n",
      "image-generation-on-vln-ce: 4 datapoints\n",
      "Checking image-generation-on-arkitscenes...\n",
      "image-generation-on-arkitscenes: 4 datapoints\n",
      "Checking image-generation-on-textatlaseval...\n",
      "image-generation-on-textatlaseval: 7 datapoints\n",
      "Checking image-generation-on-cat-256x256...\n",
      "image-generation-on-cat-256x256: 3 datapoints\n",
      "Checking image-generation-on-ade-indoor...\n",
      "image-generation-on-ade-indoor: 3 datapoints\n",
      "Checking image-generation-on-stacked-mnist...\n",
      "image-generation-on-stacked-mnist: 3 datapoints\n",
      "Checking image-generation-on-celeba-hq-64x64...\n",
      "image-generation-on-celeba-hq-64x64: 3 datapoints\n",
      "Checking image-generation-on-lsun-bedroom-64-x-64...\n",
      "image-generation-on-lsun-bedroom-64-x-64: 3 datapoints\n",
      "Checking image-generation-on-cifar-10-20-data...\n",
      "image-generation-on-cifar-10-20-data: 3 datapoints\n",
      "Checking image-generation-on-cifar-10-10-data...\n",
      "image-generation-on-cifar-10-10-data: 3 datapoints\n",
      "Checking image-generation-on-lsun-bedroom-1...\n",
      "image-generation-on-lsun-bedroom-1: 3 datapoints\n",
      "Checking image-generation-on-ffhq-512-x-512...\n",
      "image-generation-on-ffhq-512-x-512: 3 datapoints\n",
      "Checking image-generation-on-ffhq-128-x-128...\n",
      "image-generation-on-ffhq-128-x-128: 3 datapoints\n",
      "Checking image-generation-on-objectsroom...\n",
      "image-generation-on-objectsroom: 3 datapoints\n",
      "Checking image-generation-on-shapestacks...\n",
      "image-generation-on-shapestacks: 3 datapoints\n",
      "Checking image-generation-on-metfaces-u...\n",
      "image-generation-on-metfaces-u: 3 datapoints\n",
      "Checking image-generation-on-metfaces...\n",
      "image-generation-on-metfaces: 3 datapoints\n",
      "Checking image-generation-on-pokemon-1024x1024...\n",
      "image-generation-on-pokemon-1024x1024: 3 datapoints\n",
      "Checking image-generation-on-oxford-102-flowers-256-x...\n",
      "image-generation-on-oxford-102-flowers-256-x: 2 datapoints\n",
      "Checking image-generation-on-lsun-car-512-x-384...\n",
      "image-generation-on-lsun-car-512-x-384: 2 datapoints\n",
      "Checking image-generation-on-lsun-bedroom-128-x-128...\n",
      "image-generation-on-lsun-bedroom-128-x-128: 2 datapoints\n",
      "Checking image-generation-on-rc-49...\n",
      "image-generation-on-rc-49: 2 datapoints\n",
      "Checking image-generation-on-inaturalist-2019...\n",
      "image-generation-on-inaturalist-2019: 2 datapoints\n",
      "Checking image-generation-on-afhq-v2-64x64...\n",
      "image-generation-on-afhq-v2-64x64: 2 datapoints\n",
      "Checking image-generation-on-ffhq-64x64...\n",
      "image-generation-on-ffhq-64x64: 2 datapoints\n",
      "Checking image-generation-on-cityscapes-5k-256x512...\n",
      "image-generation-on-cityscapes-5k-256x512: 1 datapoints\n",
      "Checking image-generation-on-cityscapes-25k-256x512...\n",
      "image-generation-on-cityscapes-25k-256x512: 1 datapoints\n",
      "Checking image-generation-on-indian-celebs-256-x-256...\n",
      "image-generation-on-indian-celebs-256-x-256: 1 datapoints\n",
      "Checking image-generation-on-lsun-car-256-x-256...\n",
      "image-generation-on-lsun-car-256-x-256: 1 datapoints\n",
      "Checking image-generation-on-multi-dsprites...\n",
      "image-generation-on-multi-dsprites: 1 datapoints\n",
      "Checking image-generation-on-gqn...\n",
      "image-generation-on-gqn: 1 datapoints\n",
      "Checking image-generation-on-landscapes-256-x-256...\n",
      "image-generation-on-landscapes-256-x-256: 1 datapoints\n",
      "Checking image-generation-on-satellite-buildings-256-x...\n",
      "image-generation-on-satellite-buildings-256-x: 1 datapoints\n",
      "Checking image-generation-on-satellite-landscapes-256...\n",
      "image-generation-on-satellite-landscapes-256: 1 datapoints\n",
      "Checking image-generation-on-oxford-102-flowers-1...\n",
      "image-generation-on-oxford-102-flowers-1: 1 datapoints\n",
      "Checking image-generation-on-25-imagenet-128x128...\n",
      "image-generation-on-25-imagenet-128x128: 1 datapoints\n",
      "Checking image-generation-on-celeba-hq...\n",
      "image-generation-on-celeba-hq: 1 datapoints\n",
      "Checking image-generation-on-llvip...\n",
      "image-generation-on-llvip: 1 datapoints\n",
      "Checking image-generation-on-sdss-galaxies...\n",
      "image-generation-on-sdss-galaxies: 1 datapoints\n",
      "Checking image-generation-on-nasa-perseverance...\n",
      "image-generation-on-nasa-perseverance: 1 datapoints\n",
      "Checking image-generation-on-1078-people-3d-faces...\n",
      "image-generation-on-1078-people-3d-faces: 1 datapoints\n",
      "Checking image-generation-on-lsun...\n",
      "image-generation-on-lsun: 1 datapoints\n",
      "Checking image-generation-on-celeba-hq-512x512...\n",
      "image-generation-on-celeba-hq-512x512: 1 datapoints\n",
      "Checking image-generation-on-celeba-3...\n",
      "image-generation-on-celeba-3: 0 datapoints\n",
      "Checking image-generation-on-lsun-tower-64x64...\n",
      "image-generation-on-lsun-tower-64x64: 1 datapoints\n",
      "Checking image-generation-on-ffhq-64x64-4x-upscaling...\n",
      "image-generation-on-ffhq-64x64-4x-upscaling: 1 datapoints\n",
      "Checking image-generation-on-kmnist...\n",
      "image-generation-on-kmnist: 1 datapoints\n",
      "Checking image-generation-on-emnist-letters...\n",
      "image-generation-on-emnist-letters: 1 datapoints\n",
      "Checking image-generation-on-imagenet-256x256-1...\n",
      "image-generation-on-imagenet-256x256-1: 1 datapoints\n",
      "Checking image-generation-on-imagenet-256x256-2...\n",
      "image-generation-on-imagenet-256x256-2: 1 datapoints\n",
      "Checking image-generation-on-imagenet-256x256-5...\n",
      "image-generation-on-imagenet-256x256-5: 1 datapoints\n",
      "Checking image-generation-on-imagenet-256x256-1-1...\n",
      "image-generation-on-imagenet-256x256-1-1: 1 datapoints\n",
      "Saved 12 benchmarks with 20+ datapoints\n",
      "Found 12 image generation benchmarks with 20+ datapoints\n",
      "Scraping data for image-generation-on-cifar-10...\n",
      "Saved 67 models for image-generation-on-cifar-10\n",
      "Scraping data for image-generation-on-celeba-64x64...\n",
      "Saved 2 models for image-generation-on-celeba-64x64\n",
      "Scraping data for image-generation-on-lsun-bedroom-256-x-256...\n",
      "Saved 26 models for image-generation-on-lsun-bedroom-256-x-256\n",
      "Scraping data for image-generation-on-imagenet-64x64...\n",
      "Saved 64 models for image-generation-on-imagenet-64x64\n",
      "Scraping data for image-generation-on-stl-10...\n",
      "Saved 5 models for image-generation-on-stl-10\n",
      "Scraping data for image-generation-on-imagenet-256x256...\n",
      "Saved 76 models for image-generation-on-imagenet-256x256\n",
      "Scraping data for image-generation-on-ffhq-256-x-256...\n",
      "Saved 48 models for image-generation-on-ffhq-256-x-256\n",
      "Scraping data for image-generation-on-imagenet-512x512...\n",
      "Saved 48 models for image-generation-on-imagenet-512x512\n",
      "Scraping data for image-generation-on-imagenet-32x32...\n",
      "Saved 33 models for image-generation-on-imagenet-32x32\n",
      "Scraping data for image-generation-on-lsun-churches-256-x-256...\n",
      "Saved 27 models for image-generation-on-lsun-churches-256-x-256\n",
      "Scraping data for image-generation-on-imagenet-128x128...\n",
      "Saved 20 models for image-generation-on-imagenet-128x128\n",
      "Scraping data for image-generation-on-ffhq-1024-x-1024...\n",
      "Saved 20 models for image-generation-on-ffhq-1024-x-1024\n",
      "Collected a total of 436 models across all benchmarks\n",
      "Combined 436 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Image generation task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'image_generation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_image_generation_benchmarks_with_20plus():\n",
    "    \"\"\"Get image generation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding image generation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/image-generation-on-ms-coco\",\n",
    "        \"https://paperswithcode.com/sota/image-generation-on-cifar-10\",\n",
    "        \"https://paperswithcode.com/sota/image-generation-on-celeba-64x64\",\n",
    "        \"https://paperswithcode.com/sota/image-generation-on-lsun-bedroom-256-x-256\",\n",
    "        \"https://paperswithcode.com/sota/image-generation-on-imagenet-64x64\",\n",
    "        \"https://paperswithcode.com/sota/image-generation-on-stl-10\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main image generation page\n",
    "        driver.get(\"https://paperswithcode.com/task/image-generation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and (\"image-generation\" in href.lower() or \"gan\" in href.lower()):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential image generation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'image_generation', 'image_generation_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'image_generation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'image_generation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all image generation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'image_generation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'image_generation', 'all_image_generation_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'image_generation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting image generation task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_image_generation_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No image generation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} image generation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Image generation task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd04178-6f00-496d-9b4f-ca4fdbf60d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting domain adaptation task data collection (20+ datapoints)...\n",
      "Finding domain adaptation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 61 potential domain adaptation benchmarks\n",
      "Checking domain-adaptation-on-office-31...\n",
      "domain-adaptation-on-office-31: 40 datapoints\n",
      "Added domain-adaptation-on-office-31 with 40 datapoints\n",
      "Checking domain-adaptation-on-digits...\n",
      "domain-adaptation-on-digits: 0 datapoints\n",
      "Checking domain-adaptation-on-visda-2017...\n",
      "domain-adaptation-on-visda-2017: 0 datapoints\n",
      "Checking domain-adaptation-on-office-home...\n",
      "domain-adaptation-on-office-home: 29 datapoints\n",
      "Added domain-adaptation-on-office-home with 29 datapoints\n",
      "Checking domain-adaptation-on-imageclef...\n",
      "domain-adaptation-on-imageclef: 0 datapoints\n",
      "Checking domain-adaptation-on-office-caltech...\n",
      "domain-adaptation-on-office-caltech: 8 datapoints\n",
      "Checking domain-adaptation-on-synthia-to-cityscapes...\n",
      "domain-adaptation-on-synthia-to-cityscapes: 33 datapoints\n",
      "Added domain-adaptation-on-synthia-to-cityscapes with 33 datapoints\n",
      "Checking domain-adaptation-on-visda2017...\n",
      "domain-adaptation-on-visda2017: 28 datapoints\n",
      "Added domain-adaptation-on-visda2017 with 28 datapoints\n",
      "Checking domain-adaptation-on-gta5-to-cityscapes...\n",
      "domain-adaptation-on-gta5-to-cityscapes: 28 datapoints\n",
      "Added domain-adaptation-on-gta5-to-cityscapes with 28 datapoints\n",
      "Checking domain-adaptation-on-imageclef-da...\n",
      "domain-adaptation-on-imageclef-da: 17 datapoints\n",
      "Checking domain-adaptation-on-cityscapes-to-acdc...\n",
      "domain-adaptation-on-cityscapes-to-acdc: 16 datapoints\n",
      "Checking domain-adaptation-on-mnist-to-usps...\n",
      "domain-adaptation-on-mnist-to-usps: 14 datapoints\n",
      "Checking domain-adaptation-on-usps-to-mnist...\n",
      "domain-adaptation-on-usps-to-mnist: 14 datapoints\n",
      "Checking domain-adaptation-on-svhn-to-mnist...\n",
      "domain-adaptation-on-svhn-to-mnist: 14 datapoints\n",
      "Checking domain-adaptation-on-svnh-to-mnist...\n",
      "domain-adaptation-on-svnh-to-mnist: 9 datapoints\n",
      "Checking domain-adaptation-on-molane...\n",
      "domain-adaptation-on-molane: 8 datapoints\n",
      "Checking domain-adaptation-on-tulane...\n",
      "domain-adaptation-on-tulane: 8 datapoints\n",
      "Checking domain-adaptation-on-mulane...\n",
      "domain-adaptation-on-mulane: 8 datapoints\n",
      "Checking domain-adaptation-on-synsig-to-gtsrb...\n",
      "domain-adaptation-on-synsig-to-gtsrb: 6 datapoints\n",
      "Checking domain-adaptation-on-cityscapes-to...\n",
      "domain-adaptation-on-cityscapes-to: 6 datapoints\n",
      "Checking domain-adaptation-on-gtav-synscapes-to...\n",
      "domain-adaptation-on-gtav-synscapes-to: 6 datapoints\n",
      "Checking domain-adaptation-on-ucf-to-hmdbfull...\n",
      "domain-adaptation-on-ucf-to-hmdbfull: 5 datapoints\n",
      "Checking domain-adaptation-on-hmdbfull-to-ucf...\n",
      "domain-adaptation-on-hmdbfull-to-ucf: 5 datapoints\n",
      "Checking domain-adaptation-on-mnist-to-mnist-m...\n",
      "domain-adaptation-on-mnist-to-mnist-m: 5 datapoints\n",
      "Checking domain-adaptation-on-cityscapes-to-1...\n",
      "domain-adaptation-on-cityscapes-to-1: 5 datapoints\n",
      "Checking domain-adaptation-on-gta5-synscapes-to...\n",
      "domain-adaptation-on-gta5-synscapes-to: 5 datapoints\n",
      "Checking domain-adaptation-on-panoptic-synthia-to...\n",
      "domain-adaptation-on-panoptic-synthia-to: 5 datapoints\n",
      "Checking domain-adaptation-on-panoptic-synthia-to-1...\n",
      "domain-adaptation-on-panoptic-synthia-to-1: 5 datapoints\n",
      "Checking domain-adaptation-on-ucf-hmdb-full...\n",
      "domain-adaptation-on-ucf-hmdb-full: 5 datapoints\n",
      "Checking domain-adaptation-on-synth-digits-to-svhn...\n",
      "domain-adaptation-on-synth-digits-to-svhn: 4 datapoints\n",
      "Checking domain-adaptation-on-synth-signs-to-gtsrb...\n",
      "domain-adaptation-on-synth-signs-to-gtsrb: 4 datapoints\n",
      "Checking domain-adaptation-on-gtav-to-cityscapes-1...\n",
      "domain-adaptation-on-gtav-to-cityscapes-1: 4 datapoints\n",
      "Checking domain-adaptation-on-hmdb-ucf-full...\n",
      "domain-adaptation-on-hmdb-ucf-full: 4 datapoints\n",
      "Checking domain-adaptation-on-ucf-to-olympic...\n",
      "domain-adaptation-on-ucf-to-olympic: 3 datapoints\n",
      "Checking domain-adaptation-on-olympic-to-hmdbsmall...\n",
      "domain-adaptation-on-olympic-to-hmdbsmall: 3 datapoints\n",
      "Checking domain-adaptation-on-ucf-to-hmdbsmall...\n",
      "domain-adaptation-on-ucf-to-hmdbsmall: 3 datapoints\n",
      "Checking domain-adaptation-on-hmdbsmall-to-ucf...\n",
      "domain-adaptation-on-hmdbsmall-to-ucf: 3 datapoints\n",
      "Checking domain-adaptation-on-synscapes-to-cityscapes...\n",
      "domain-adaptation-on-synscapes-to-cityscapes: 3 datapoints\n",
      "Checking domain-adaptation-on-domainnet-1...\n",
      "domain-adaptation-on-domainnet-1: 3 datapoints\n",
      "Checking domain-adaptation-on-rotating-mnist...\n",
      "domain-adaptation-on-rotating-mnist: 2 datapoints\n",
      "Checking domain-adaptation-on-synth-objects-to-linemod...\n",
      "domain-adaptation-on-synth-objects-to-linemod: 1 datapoints\n",
      "Checking domain-adaptation-on-office-caltech-10...\n",
      "domain-adaptation-on-office-caltech-10: 1 datapoints\n",
      "Checking domain-adaptation-on-mnist-m-to-mnist...\n",
      "domain-adaptation-on-mnist-m-to-mnist: 1 datapoints\n",
      "Checking domain-adaptation-on-noisy-mnist-to-synd...\n",
      "domain-adaptation-on-noisy-mnist-to-synd: 1 datapoints\n",
      "Checking domain-adaptation-on-noisy-synd-to-mnist...\n",
      "domain-adaptation-on-noisy-synd-to-mnist: 1 datapoints\n",
      "Checking domain-adaptation-on-noisy-amazon-20...\n",
      "domain-adaptation-on-noisy-amazon-20: 1 datapoints\n",
      "Checking domain-adaptation-on-noisy-amazon-45...\n",
      "domain-adaptation-on-noisy-amazon-45: 1 datapoints\n",
      "Checking domain-adaptation-on-msda...\n",
      "domain-adaptation-on-msda: 1 datapoints\n",
      "Checking domain-adaptation-on-synthia-to-cityscapes-1...\n",
      "domain-adaptation-on-synthia-to-cityscapes-1: 1 datapoints\n",
      "Checking domain-adaptation-on-gta-to-foggycityscapes...\n",
      "domain-adaptation-on-gta-to-foggycityscapes: 1 datapoints\n",
      "Checking domain-adaptation-on-synthia-to...\n",
      "domain-adaptation-on-synthia-to: 1 datapoints\n",
      "Checking domain-adaptation-on-pacs...\n",
      "domain-adaptation-on-pacs: 1 datapoints\n",
      "Checking domain-adaptation-on-nikon-raw-low-light...\n",
      "domain-adaptation-on-nikon-raw-low-light: 1 datapoints\n",
      "Checking domain-adaptation-on-canon-raw-low-light...\n",
      "domain-adaptation-on-canon-raw-low-light: 1 datapoints\n",
      "Checking domain-adaptation-on-foggy-cityscapes...\n",
      "domain-adaptation-on-foggy-cityscapes: 1 datapoints\n",
      "Checking domain-adaptation-on-comic2k...\n",
      "domain-adaptation-on-comic2k: 1 datapoints\n",
      "Checking domain-adaptation-on-sim10k...\n",
      "domain-adaptation-on-sim10k: 1 datapoints\n",
      "Checking domain-adaptation-on-viper-to-cityscapes...\n",
      "domain-adaptation-on-viper-to-cityscapes: 1 datapoints\n",
      "Checking domain-adaptation-on-leukemiaattri...\n",
      "domain-adaptation-on-leukemiaattri: 1 datapoints\n",
      "Checking domain-adaptation-on-s2rda-49...\n",
      "domain-adaptation-on-s2rda-49: 1 datapoints\n",
      "Checking domain-adaptation-on-s2rda-ms-39...\n",
      "domain-adaptation-on-s2rda-ms-39: 1 datapoints\n",
      "Saved 5 benchmarks with 20+ datapoints\n",
      "Found 5 domain adaptation benchmarks with 20+ datapoints\n",
      "Scraping data for domain-adaptation-on-office-31...\n",
      "Saved 40 models for domain-adaptation-on-office-31\n",
      "Scraping data for domain-adaptation-on-office-home...\n",
      "Saved 29 models for domain-adaptation-on-office-home\n",
      "Scraping data for domain-adaptation-on-synthia-to-cityscapes...\n",
      "Saved 33 models for domain-adaptation-on-synthia-to-cityscapes\n",
      "Scraping data for domain-adaptation-on-visda2017...\n",
      "Saved 28 models for domain-adaptation-on-visda2017\n",
      "Scraping data for domain-adaptation-on-gta5-to-cityscapes...\n",
      "Saved 28 models for domain-adaptation-on-gta5-to-cityscapes\n",
      "Collected a total of 158 models across all benchmarks\n",
      "Combined 158 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Domain adaptation task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'domain_adaptation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_domain_adaptation_benchmarks_with_20plus():\n",
    "    \"\"\"Get domain adaptation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding domain adaptation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-office-31\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-digits\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-visda-2017\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-office-home\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-imageclef\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-office-caltech\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main domain adaptation page\n",
    "        driver.get(\"https://paperswithcode.com/task/domain-adaptation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"domain-adaptation\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential domain adaptation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'domain_adaptation', 'domain_adaptation_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'domain_adaptation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'domain_adaptation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all domain adaptation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'domain_adaptation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'domain_adaptation', 'all_domain_adaptation_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'domain_adaptation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting domain adaptation task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_domain_adaptation_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No domain adaptation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} domain adaptation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Domain adaptation task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "609a802b-20cc-4b32-b1c7-fc0a4f9806a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting domain adaptation task data collection (20+ datapoints)...\n",
      "Finding domain adaptation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 61 potential domain adaptation benchmarks\n",
      "Checking domain-adaptation-on-office-31...\n",
      "domain-adaptation-on-office-31: 40 datapoints\n",
      "Added domain-adaptation-on-office-31 with 40 datapoints\n",
      "Checking domain-adaptation-on-digits...\n",
      "domain-adaptation-on-digits: 0 datapoints\n",
      "Checking domain-adaptation-on-visda-2017...\n",
      "domain-adaptation-on-visda-2017: 0 datapoints\n",
      "Checking domain-adaptation-on-office-home...\n",
      "domain-adaptation-on-office-home: 29 datapoints\n",
      "Added domain-adaptation-on-office-home with 29 datapoints\n",
      "Checking domain-adaptation-on-imageclef...\n",
      "domain-adaptation-on-imageclef: 0 datapoints\n",
      "Checking domain-adaptation-on-office-caltech...\n",
      "domain-adaptation-on-office-caltech: 8 datapoints\n",
      "Checking domain-adaptation-on-synthia-to-cityscapes...\n",
      "domain-adaptation-on-synthia-to-cityscapes: 33 datapoints\n",
      "Added domain-adaptation-on-synthia-to-cityscapes with 33 datapoints\n",
      "Checking domain-adaptation-on-visda2017...\n",
      "domain-adaptation-on-visda2017: 28 datapoints\n",
      "Added domain-adaptation-on-visda2017 with 28 datapoints\n",
      "Checking domain-adaptation-on-gta5-to-cityscapes...\n",
      "domain-adaptation-on-gta5-to-cityscapes: 28 datapoints\n",
      "Added domain-adaptation-on-gta5-to-cityscapes with 28 datapoints\n",
      "Checking domain-adaptation-on-imageclef-da...\n",
      "domain-adaptation-on-imageclef-da: 17 datapoints\n",
      "Checking domain-adaptation-on-cityscapes-to-acdc...\n",
      "domain-adaptation-on-cityscapes-to-acdc: 16 datapoints\n",
      "Checking domain-adaptation-on-mnist-to-usps...\n",
      "domain-adaptation-on-mnist-to-usps: 14 datapoints\n",
      "Checking domain-adaptation-on-usps-to-mnist...\n",
      "domain-adaptation-on-usps-to-mnist: 14 datapoints\n",
      "Checking domain-adaptation-on-svhn-to-mnist...\n",
      "domain-adaptation-on-svhn-to-mnist: 14 datapoints\n",
      "Checking domain-adaptation-on-svnh-to-mnist...\n",
      "domain-adaptation-on-svnh-to-mnist: 9 datapoints\n",
      "Checking domain-adaptation-on-molane...\n",
      "domain-adaptation-on-molane: 8 datapoints\n",
      "Checking domain-adaptation-on-tulane...\n",
      "domain-adaptation-on-tulane: 8 datapoints\n",
      "Checking domain-adaptation-on-mulane...\n",
      "domain-adaptation-on-mulane: 8 datapoints\n",
      "Checking domain-adaptation-on-synsig-to-gtsrb...\n",
      "domain-adaptation-on-synsig-to-gtsrb: 6 datapoints\n",
      "Checking domain-adaptation-on-cityscapes-to...\n",
      "domain-adaptation-on-cityscapes-to: 6 datapoints\n",
      "Checking domain-adaptation-on-gtav-synscapes-to...\n",
      "domain-adaptation-on-gtav-synscapes-to: 6 datapoints\n",
      "Checking domain-adaptation-on-ucf-to-hmdbfull...\n",
      "domain-adaptation-on-ucf-to-hmdbfull: 5 datapoints\n",
      "Checking domain-adaptation-on-hmdbfull-to-ucf...\n",
      "domain-adaptation-on-hmdbfull-to-ucf: 5 datapoints\n",
      "Checking domain-adaptation-on-mnist-to-mnist-m...\n",
      "domain-adaptation-on-mnist-to-mnist-m: 5 datapoints\n",
      "Checking domain-adaptation-on-cityscapes-to-1...\n",
      "domain-adaptation-on-cityscapes-to-1: 5 datapoints\n",
      "Checking domain-adaptation-on-gta5-synscapes-to...\n",
      "domain-adaptation-on-gta5-synscapes-to: 5 datapoints\n",
      "Checking domain-adaptation-on-panoptic-synthia-to...\n",
      "domain-adaptation-on-panoptic-synthia-to: 5 datapoints\n",
      "Checking domain-adaptation-on-panoptic-synthia-to-1...\n",
      "domain-adaptation-on-panoptic-synthia-to-1: 5 datapoints\n",
      "Checking domain-adaptation-on-ucf-hmdb-full...\n",
      "domain-adaptation-on-ucf-hmdb-full: 5 datapoints\n",
      "Checking domain-adaptation-on-synth-digits-to-svhn...\n",
      "domain-adaptation-on-synth-digits-to-svhn: 4 datapoints\n",
      "Checking domain-adaptation-on-synth-signs-to-gtsrb...\n",
      "domain-adaptation-on-synth-signs-to-gtsrb: 4 datapoints\n",
      "Checking domain-adaptation-on-gtav-to-cityscapes-1...\n",
      "domain-adaptation-on-gtav-to-cityscapes-1: 4 datapoints\n",
      "Checking domain-adaptation-on-hmdb-ucf-full...\n",
      "domain-adaptation-on-hmdb-ucf-full: 4 datapoints\n",
      "Checking domain-adaptation-on-ucf-to-olympic...\n",
      "domain-adaptation-on-ucf-to-olympic: 3 datapoints\n",
      "Checking domain-adaptation-on-olympic-to-hmdbsmall...\n",
      "domain-adaptation-on-olympic-to-hmdbsmall: 3 datapoints\n",
      "Checking domain-adaptation-on-ucf-to-hmdbsmall...\n",
      "domain-adaptation-on-ucf-to-hmdbsmall: 3 datapoints\n",
      "Checking domain-adaptation-on-hmdbsmall-to-ucf...\n",
      "domain-adaptation-on-hmdbsmall-to-ucf: 3 datapoints\n",
      "Checking domain-adaptation-on-synscapes-to-cityscapes...\n",
      "domain-adaptation-on-synscapes-to-cityscapes: 3 datapoints\n",
      "Checking domain-adaptation-on-domainnet-1...\n",
      "domain-adaptation-on-domainnet-1: 3 datapoints\n",
      "Checking domain-adaptation-on-rotating-mnist...\n",
      "domain-adaptation-on-rotating-mnist: 2 datapoints\n",
      "Checking domain-adaptation-on-synth-objects-to-linemod...\n",
      "domain-adaptation-on-synth-objects-to-linemod: 1 datapoints\n",
      "Checking domain-adaptation-on-office-caltech-10...\n",
      "domain-adaptation-on-office-caltech-10: 1 datapoints\n",
      "Checking domain-adaptation-on-mnist-m-to-mnist...\n",
      "domain-adaptation-on-mnist-m-to-mnist: 1 datapoints\n",
      "Checking domain-adaptation-on-noisy-mnist-to-synd...\n",
      "domain-adaptation-on-noisy-mnist-to-synd: 1 datapoints\n",
      "Checking domain-adaptation-on-noisy-synd-to-mnist...\n",
      "domain-adaptation-on-noisy-synd-to-mnist: 1 datapoints\n",
      "Checking domain-adaptation-on-noisy-amazon-20...\n",
      "domain-adaptation-on-noisy-amazon-20: 1 datapoints\n",
      "Checking domain-adaptation-on-noisy-amazon-45...\n",
      "domain-adaptation-on-noisy-amazon-45: 1 datapoints\n",
      "Checking domain-adaptation-on-msda...\n",
      "domain-adaptation-on-msda: 1 datapoints\n",
      "Checking domain-adaptation-on-synthia-to-cityscapes-1...\n",
      "domain-adaptation-on-synthia-to-cityscapes-1: 1 datapoints\n",
      "Checking domain-adaptation-on-gta-to-foggycityscapes...\n",
      "domain-adaptation-on-gta-to-foggycityscapes: 1 datapoints\n",
      "Checking domain-adaptation-on-synthia-to...\n",
      "domain-adaptation-on-synthia-to: 1 datapoints\n",
      "Checking domain-adaptation-on-pacs...\n",
      "domain-adaptation-on-pacs: 1 datapoints\n",
      "Checking domain-adaptation-on-nikon-raw-low-light...\n",
      "domain-adaptation-on-nikon-raw-low-light: 1 datapoints\n",
      "Checking domain-adaptation-on-canon-raw-low-light...\n",
      "domain-adaptation-on-canon-raw-low-light: 1 datapoints\n",
      "Checking domain-adaptation-on-foggy-cityscapes...\n",
      "domain-adaptation-on-foggy-cityscapes: 1 datapoints\n",
      "Checking domain-adaptation-on-comic2k...\n",
      "domain-adaptation-on-comic2k: 1 datapoints\n",
      "Checking domain-adaptation-on-sim10k...\n",
      "domain-adaptation-on-sim10k: 1 datapoints\n",
      "Checking domain-adaptation-on-viper-to-cityscapes...\n",
      "domain-adaptation-on-viper-to-cityscapes: 1 datapoints\n",
      "Checking domain-adaptation-on-leukemiaattri...\n",
      "domain-adaptation-on-leukemiaattri: 1 datapoints\n",
      "Checking domain-adaptation-on-s2rda-49...\n",
      "domain-adaptation-on-s2rda-49: 1 datapoints\n",
      "Checking domain-adaptation-on-s2rda-ms-39...\n",
      "domain-adaptation-on-s2rda-ms-39: 1 datapoints\n",
      "Saved 5 benchmarks with 20+ datapoints\n",
      "Found 5 domain adaptation benchmarks with 20+ datapoints\n",
      "Scraping data for domain-adaptation-on-office-31...\n",
      "Saved 40 models for domain-adaptation-on-office-31\n",
      "Scraping data for domain-adaptation-on-office-home...\n",
      "Saved 29 models for domain-adaptation-on-office-home\n",
      "Scraping data for domain-adaptation-on-synthia-to-cityscapes...\n",
      "Saved 33 models for domain-adaptation-on-synthia-to-cityscapes\n",
      "Scraping data for domain-adaptation-on-visda2017...\n",
      "Saved 28 models for domain-adaptation-on-visda2017\n",
      "Scraping data for domain-adaptation-on-gta5-to-cityscapes...\n",
      "Saved 28 models for domain-adaptation-on-gta5-to-cityscapes\n",
      "Collected a total of 158 models across all benchmarks\n",
      "Combined 158 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Domain adaptation task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'domain_adaptation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_domain_adaptation_benchmarks_with_20plus():\n",
    "    \"\"\"Get domain adaptation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding domain adaptation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-office-31\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-digits\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-visda-2017\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-office-home\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-imageclef\",\n",
    "        \"https://paperswithcode.com/sota/domain-adaptation-on-office-caltech\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main domain adaptation page\n",
    "        driver.get(\"https://paperswithcode.com/task/domain-adaptation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"domain-adaptation\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential domain adaptation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'domain_adaptation', 'domain_adaptation_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'domain_adaptation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'domain_adaptation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all domain adaptation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'domain_adaptation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'domain_adaptation', 'all_domain_adaptation_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'domain_adaptation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting domain adaptation task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_domain_adaptation_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No domain adaptation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} domain adaptation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Domain adaptation task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3460f6ec-b119-4ba4-8a09-3e327f728d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting data augmentation task data collection (20+ datapoints)...\n",
      "Finding data augmentation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 7 potential data augmentation benchmarks\n",
      "Checking data-augmentation-on-cifar-10...\n",
      "data-augmentation-on-cifar-10: 5 datapoints\n",
      "Checking data-augmentation-on-cifar-100...\n",
      "data-augmentation-on-cifar-100: 0 datapoints\n",
      "Checking data-augmentation-on-imagenet...\n",
      "data-augmentation-on-imagenet: 17 datapoints\n",
      "Checking data-augmentation-on-svhn...\n",
      "data-augmentation-on-svhn: 0 datapoints\n",
      "Checking data-augmentation-on-tiny-imagenet-200...\n",
      "data-augmentation-on-tiny-imagenet-200: 0 datapoints\n",
      "Checking data-augmentation-on-mnist...\n",
      "data-augmentation-on-mnist: 0 datapoints\n",
      "Checking data-augmentation-on-ga1457...\n",
      "data-augmentation-on-ga1457: 4 datapoints\n",
      "No data augmentation benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'data_augmentation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_data_augmentation_benchmarks_with_20plus():\n",
    "    \"\"\"Get data augmentation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding data augmentation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/data-augmentation-on-cifar-10\",\n",
    "        \"https://paperswithcode.com/sota/data-augmentation-on-cifar-100\",\n",
    "        \"https://paperswithcode.com/sota/data-augmentation-on-imagenet\",\n",
    "        \"https://paperswithcode.com/sota/data-augmentation-on-svhn\",\n",
    "        \"https://paperswithcode.com/sota/data-augmentation-on-tiny-imagenet-200\",\n",
    "        \"https://paperswithcode.com/sota/data-augmentation-on-mnist\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main data augmentation page\n",
    "        driver.get(\"https://paperswithcode.com/task/data-augmentation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"data-augmentation\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential data augmentation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'data_augmentation', 'data_augmentation_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'data_augmentation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'data_augmentation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all data augmentation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'data_augmentation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'data_augmentation', 'all_data_augmentation_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'data_augmentation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting data augmentation task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_data_augmentation_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No data augmentation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} data augmentation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Data augmentation task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e339259-99f9-4ab0-8c7a-7015f15bca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting retrieval task data collection (20+ datapoints)...\n",
      "Finding retrieval benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 14 potential retrieval benchmarks\n",
      "Checking image-retrieval-on-oxford-5k...\n",
      "image-retrieval-on-oxford-5k: 0 datapoints\n",
      "Checking image-retrieval-on-paris-6k...\n",
      "image-retrieval-on-paris-6k: 0 datapoints\n",
      "Checking cross-modal-retrieval-on-ms-coco-1k-test...\n",
      "cross-modal-retrieval-on-ms-coco-1k-test: 0 datapoints\n",
      "Checking image-retrieval-on-roxford-medium...\n",
      "image-retrieval-on-roxford-medium: 23 datapoints\n",
      "Added image-retrieval-on-roxford-medium with 23 datapoints\n",
      "Checking image-retrieval-on-rparis-medium...\n",
      "image-retrieval-on-rparis-medium: 23 datapoints\n",
      "Added image-retrieval-on-rparis-medium with 23 datapoints\n",
      "Checking document-retrieval-on-robust04...\n",
      "document-retrieval-on-robust04: 0 datapoints\n",
      "Checking retrieval-on-quora-question-pairs...\n",
      "retrieval-on-quora-question-pairs: 4 datapoints\n",
      "Checking retrieval-on-hotpotqa...\n",
      "retrieval-on-hotpotqa: 3 datapoints\n",
      "Checking retrieval-on-natural-questions...\n",
      "retrieval-on-natural-questions: 3 datapoints\n",
      "Checking retrieval-on-ok-vqa...\n",
      "retrieval-on-ok-vqa: 2 datapoints\n",
      "Checking retrieval-on-mvk...\n",
      "retrieval-on-mvk: 1 datapoints\n",
      "Checking retrieval-on-infoseek...\n",
      "retrieval-on-infoseek: 1 datapoints\n",
      "Checking retrieval-on-polyvore...\n",
      "retrieval-on-polyvore: 1 datapoints\n",
      "Checking retrieval-on-toollens...\n",
      "retrieval-on-toollens: 1 datapoints\n",
      "Saved 2 benchmarks with 20+ datapoints\n",
      "Found 2 retrieval benchmarks with 20+ datapoints\n",
      "Scraping data for image-retrieval-on-roxford-medium...\n",
      "Saved 23 models for image-retrieval-on-roxford-medium\n",
      "Scraping data for image-retrieval-on-rparis-medium...\n",
      "Saved 23 models for image-retrieval-on-rparis-medium\n",
      "Collected a total of 46 models across all benchmarks\n",
      "Combined 46 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Retrieval task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'retrieval'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_retrieval_benchmarks_with_20plus():\n",
    "    \"\"\"Get retrieval benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding retrieval benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/image-retrieval-on-oxford-5k\",\n",
    "        \"https://paperswithcode.com/sota/image-retrieval-on-paris-6k\",\n",
    "        \"https://paperswithcode.com/sota/cross-modal-retrieval-on-ms-coco-1k-test\",\n",
    "        \"https://paperswithcode.com/sota/image-retrieval-on-roxford-medium\",\n",
    "        \"https://paperswithcode.com/sota/image-retrieval-on-rparis-medium\",\n",
    "        \"https://paperswithcode.com/sota/document-retrieval-on-robust04\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main retrieval page\n",
    "        driver.get(\"https://paperswithcode.com/task/retrieval\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and (\"retrieval\" in href.lower() or \"search\" in href.lower()):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential retrieval benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'retrieval', 'retrieval_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'retrieval', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'retrieval', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all retrieval models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'retrieval')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'retrieval', 'all_retrieval_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'retrieval', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting retrieval task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_retrieval_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No retrieval benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} retrieval benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Retrieval task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe08a47-e154-4404-87f4-7dcfb56fd834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting denoising task data collection (20+ datapoints)...\n",
      "Finding denoising benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 12 potential denoising benchmarks\n",
      "Checking image-denoising-on-cbsd68-sigma15...\n",
      "image-denoising-on-cbsd68-sigma15: 0 datapoints\n",
      "Checking image-denoising-on-cbsd68-sigma25...\n",
      "image-denoising-on-cbsd68-sigma25: 0 datapoints\n",
      "Checking image-denoising-on-cbsd68-sigma50...\n",
      "image-denoising-on-cbsd68-sigma50: 0 datapoints\n",
      "Checking denoising-on-set12-sigma15...\n",
      "denoising-on-set12-sigma15: 0 datapoints\n",
      "Checking denoising-on-set12-sigma25...\n",
      "denoising-on-set12-sigma25: 0 datapoints\n",
      "Checking denoising-on-set12-sigma50...\n",
      "denoising-on-set12-sigma50: 0 datapoints\n",
      "Checking denoising-on-darmstadt-noise-dataset...\n",
      "denoising-on-darmstadt-noise-dataset: 10 datapoints\n",
      "Checking denoising-on-aapm...\n",
      "denoising-on-aapm: 1 datapoints\n",
      "Checking denoising-on-iris...\n",
      "denoising-on-iris: 1 datapoints\n",
      "Checking denoising-on-dnd-1...\n",
      "denoising-on-dnd-1: 1 datapoints\n",
      "Checking denoising-on-cbsd68-sigm75...\n",
      "denoising-on-cbsd68-sigm75: 1 datapoints\n",
      "Checking denoising-on-div2k...\n",
      "denoising-on-div2k: 1 datapoints\n",
      "No denoising benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'denoising'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_denoising_benchmarks_with_20plus():\n",
    "    \"\"\"Get denoising benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding denoising benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/image-denoising-on-cbsd68-sigma15\",\n",
    "        \"https://paperswithcode.com/sota/image-denoising-on-cbsd68-sigma25\",\n",
    "        \"https://paperswithcode.com/sota/image-denoising-on-cbsd68-sigma50\",\n",
    "        \"https://paperswithcode.com/sota/denoising-on-set12-sigma15\",\n",
    "        \"https://paperswithcode.com/sota/denoising-on-set12-sigma25\",\n",
    "        \"https://paperswithcode.com/sota/denoising-on-set12-sigma50\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main denoising page\n",
    "        driver.get(\"https://paperswithcode.com/task/denoising\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"denoising\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential denoising benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'denoising', 'denoising_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'denoising', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'denoising', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all denoising models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'denoising')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'denoising', 'all_denoising_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'denoising', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting denoising task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_denoising_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No denoising benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} denoising benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Denoising task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6140f110-cae4-4194-8a4a-f17f40807721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting anomaly detection task data collection (20+ datapoints)...\n",
      "Finding anomaly detection benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 78 potential anomaly detection benchmarks\n",
      "Checking anomaly-detection-on-mvtec...\n",
      "anomaly-detection-on-mvtec: 0 datapoints\n",
      "Checking anomaly-detection-on-mvtec-ad...\n",
      "anomaly-detection-on-mvtec-ad: 144 datapoints\n",
      "Added anomaly-detection-on-mvtec-ad with 144 datapoints\n",
      "Checking anomaly-detection-on-cifar-10...\n",
      "anomaly-detection-on-cifar-10: 1 datapoints\n",
      "Checking anomaly-detection-on-kdd-cup-99...\n",
      "anomaly-detection-on-kdd-cup-99: 0 datapoints\n",
      "Checking anomaly-detection-on-ucsd-ped1...\n",
      "anomaly-detection-on-ucsd-ped1: 0 datapoints\n",
      "Checking anomaly-detection-on-ucsd-ped2...\n",
      "anomaly-detection-on-ucsd-ped2: 13 datapoints\n",
      "Checking anomaly-detection-on-visa...\n",
      "anomaly-detection-on-visa: 47 datapoints\n",
      "Added anomaly-detection-on-visa with 47 datapoints\n",
      "Checking anomaly-detection-on-mvtec-loco-ad...\n",
      "anomaly-detection-on-mvtec-loco-ad: 40 datapoints\n",
      "Added anomaly-detection-on-mvtec-loco-ad with 40 datapoints\n",
      "Checking anomaly-detection-on-one-class-cifar-10...\n",
      "anomaly-detection-on-one-class-cifar-10: 36 datapoints\n",
      "Added anomaly-detection-on-one-class-cifar-10 with 36 datapoints\n",
      "Checking anomaly-detection-on-chuk-avenue...\n",
      "anomaly-detection-on-chuk-avenue: 35 datapoints\n",
      "Added anomaly-detection-on-chuk-avenue with 35 datapoints\n",
      "Checking anomaly-detection-on-shanghaitech...\n",
      "anomaly-detection-on-shanghaitech: 31 datapoints\n",
      "Added anomaly-detection-on-shanghaitech with 31 datapoints\n",
      "Checking anomaly-detection-on-ucr-anomaly-archive...\n",
      "anomaly-detection-on-ucr-anomaly-archive: 24 datapoints\n",
      "Added anomaly-detection-on-ucr-anomaly-archive with 24 datapoints\n",
      "Checking anomaly-detection-on-fishyscapes-l-f...\n",
      "anomaly-detection-on-fishyscapes-l-f: 18 datapoints\n",
      "Checking anomaly-detection-on-one-class-cifar-100...\n",
      "anomaly-detection-on-one-class-cifar-100: 15 datapoints\n",
      "Checking anomaly-detection-on-mpdd...\n",
      "anomaly-detection-on-mpdd: 15 datapoints\n",
      "Checking anomaly-detection-on-ubnormal...\n",
      "anomaly-detection-on-ubnormal: 14 datapoints\n",
      "Checking anomaly-detection-on-btad...\n",
      "anomaly-detection-on-btad: 14 datapoints\n",
      "Checking anomaly-detection-on-unlabeled-cifar-10-vs...\n",
      "anomaly-detection-on-unlabeled-cifar-10-vs: 13 datapoints\n",
      "Checking anomaly-detection-on-fashion-mnist...\n",
      "anomaly-detection-on-fashion-mnist: 12 datapoints\n",
      "Checking anomaly-detection-on-one-class-imagenet-30...\n",
      "anomaly-detection-on-one-class-imagenet-30: 11 datapoints\n",
      "Checking anomaly-detection-on-numenta-anomaly...\n",
      "anomaly-detection-on-numenta-anomaly: 10 datapoints\n",
      "Checking anomaly-detection-on-road-anomaly...\n",
      "anomaly-detection-on-road-anomaly: 10 datapoints\n",
      "Checking anomaly-detection-on-anomaly-detection-on...\n",
      "anomaly-detection-on-anomaly-detection-on: 8 datapoints\n",
      "Checking anomaly-detection-on-fishyscapes-1...\n",
      "anomaly-detection-on-fishyscapes-1: 8 datapoints\n",
      "Checking anomaly-detection-on-aebad-s...\n",
      "anomaly-detection-on-aebad-s: 8 datapoints\n",
      "Checking anomaly-detection-on-aebad-v...\n",
      "anomaly-detection-on-aebad-v: 7 datapoints\n",
      "Checking anomaly-detection-on-mnist...\n",
      "anomaly-detection-on-mnist: 6 datapoints\n",
      "Checking anomaly-detection-on-hyper-kvasir-dataset...\n",
      "anomaly-detection-on-hyper-kvasir-dataset: 6 datapoints\n",
      "Checking anomaly-detection-on-leave-one-class-out...\n",
      "anomaly-detection-on-leave-one-class-out: 6 datapoints\n",
      "Checking anomaly-detection-on-leave-one-class-out-1...\n",
      "anomaly-detection-on-leave-one-class-out-1: 6 datapoints\n",
      "Checking anomaly-detection-on-anomaly-detection-on-1...\n",
      "anomaly-detection-on-anomaly-detection-on-1: 5 datapoints\n",
      "Checking anomaly-detection-on-anomaly-detection-on-2...\n",
      "anomaly-detection-on-anomaly-detection-on-2: 5 datapoints\n",
      "Checking anomaly-detection-on-lag...\n",
      "anomaly-detection-on-lag: 5 datapoints\n",
      "Checking anomaly-detection-on-insplad...\n",
      "anomaly-detection-on-insplad: 5 datapoints\n",
      "Checking anomaly-detection-on-cats-and-dogs...\n",
      "anomaly-detection-on-cats-and-dogs: 4 datapoints\n",
      "Checking anomaly-detection-on-dior...\n",
      "anomaly-detection-on-dior: 4 datapoints\n",
      "Checking anomaly-detection-on-surface-defect-saliency...\n",
      "anomaly-detection-on-surface-defect-saliency: 4 datapoints\n",
      "Checking anomaly-detection-on-lost-and-found...\n",
      "anomaly-detection-on-lost-and-found: 4 datapoints\n",
      "Checking anomaly-detection-on-mvtec-ad-textures...\n",
      "anomaly-detection-on-mvtec-ad-textures: 4 datapoints\n",
      "Checking anomaly-detection-on-pheva...\n",
      "anomaly-detection-on-pheva: 4 datapoints\n",
      "Checking anomaly-detection-on-ucsd-peds2...\n",
      "anomaly-detection-on-ucsd-peds2: 3 datapoints\n",
      "Checking anomaly-detection-on-corridor...\n",
      "anomaly-detection-on-corridor: 3 datapoints\n",
      "Checking anomaly-detection-on-odds...\n",
      "anomaly-detection-on-odds: 3 datapoints\n",
      "Checking anomaly-detection-on-uea-time-series-datasets...\n",
      "anomaly-detection-on-uea-time-series-datasets: 3 datapoints\n",
      "Checking anomaly-detection-on-mvtec-ad-textures-domain...\n",
      "anomaly-detection-on-mvtec-ad-textures-domain: 3 datapoints\n",
      "Checking anomaly-detection-on-voraus-ad...\n",
      "anomaly-detection-on-voraus-ad: 3 datapoints\n",
      "Checking anomaly-detection-on-thyroid...\n",
      "anomaly-detection-on-thyroid: 2 datapoints\n",
      "Checking anomaly-detection-on-mvtec-3d-ad-1...\n",
      "anomaly-detection-on-mvtec-3d-ad-1: 2 datapoints\n",
      "Checking anomaly-detection-on-vehicle-claims...\n",
      "anomaly-detection-on-vehicle-claims: 2 datapoints\n",
      "Checking anomaly-detection-on-pad-dataset...\n",
      "anomaly-detection-on-pad-dataset: 2 datapoints\n",
      "Checking anomaly-detection-on-smd...\n",
      "anomaly-detection-on-smd: 2 datapoints\n",
      "Checking anomaly-detection-on-census...\n",
      "anomaly-detection-on-census: 1 datapoints\n",
      "Checking anomaly-detection-on-mnist-test...\n",
      "anomaly-detection-on-mnist-test: 1 datapoints\n",
      "Checking anomaly-detection-on-ag-news...\n",
      "anomaly-detection-on-ag-news: 1 datapoints\n",
      "Checking anomaly-detection-on-assira-cat-vs-dog...\n",
      "anomaly-detection-on-assira-cat-vs-dog: 1 datapoints\n",
      "Checking anomaly-detection-on-stl-10...\n",
      "anomaly-detection-on-stl-10: 1 datapoints\n",
      "Checking anomaly-detection-on-forest-covertype...\n",
      "anomaly-detection-on-forest-covertype: 1 datapoints\n",
      "Checking anomaly-detection-on-kaggle-credit-card-fraud...\n",
      "anomaly-detection-on-kaggle-credit-card-fraud: 1 datapoints\n",
      "Checking anomaly-detection-on-nb15-analysis...\n",
      "anomaly-detection-on-nb15-analysis: 1 datapoints\n",
      "Checking anomaly-detection-on-nb15-backdoor...\n",
      "anomaly-detection-on-nb15-backdoor: 1 datapoints\n",
      "Checking anomaly-detection-on-nb15-dos...\n",
      "anomaly-detection-on-nb15-dos: 1 datapoints\n",
      "Checking anomaly-detection-on-bottlecap...\n",
      "anomaly-detection-on-bottlecap: 1 datapoints\n",
      "Checking anomaly-detection-on-kdd-99...\n",
      "anomaly-detection-on-kdd-99: 1 datapoints\n",
      "Checking anomaly-detection-on-mvtec-3d-ad-rgb...\n",
      "anomaly-detection-on-mvtec-3d-ad-rgb: 1 datapoints\n",
      "Checking anomaly-detection-on-kdd-cup-1999...\n",
      "anomaly-detection-on-kdd-cup-1999: 1 datapoints\n",
      "Checking anomaly-detection-on-mit-bih-arrhythmia...\n",
      "anomaly-detection-on-mit-bih-arrhythmia: 1 datapoints\n",
      "Checking anomaly-detection-on-musk-v1...\n",
      "anomaly-detection-on-musk-v1: 1 datapoints\n",
      "Checking anomaly-detection-on-svhn...\n",
      "anomaly-detection-on-svhn: 1 datapoints\n",
      "Checking anomaly-detection-on-ksdd2...\n",
      "anomaly-detection-on-ksdd2: 1 datapoints\n",
      "Checking anomaly-detection-on-tii-ssrc-23...\n",
      "anomaly-detection-on-tii-ssrc-23: 1 datapoints\n",
      "Checking anomaly-detection-on-adni...\n",
      "anomaly-detection-on-adni: 1 datapoints\n",
      "Checking anomaly-detection-on-coco-ooc...\n",
      "anomaly-detection-on-coco-ooc: 1 datapoints\n",
      "Checking anomaly-detection-on-ucf-crime-1...\n",
      "anomaly-detection-on-ucf-crime-1: 1 datapoints\n",
      "Checking anomaly-detection-on-wfdd...\n",
      "anomaly-detection-on-wfdd: 1 datapoints\n",
      "Checking anomaly-detection-on-shanghaitech-campus-2...\n",
      "anomaly-detection-on-shanghaitech-campus-2: 1 datapoints\n",
      "Checking anomaly-detection-on-iitb-corridor-1...\n",
      "anomaly-detection-on-iitb-corridor-1: 1 datapoints\n",
      "Checking anomaly-detection-on-street-scene...\n",
      "anomaly-detection-on-street-scene: 1 datapoints\n",
      "Checking anomaly-detection-on-real-3d-ad...\n",
      "anomaly-detection-on-real-3d-ad: 1 datapoints\n",
      "Saved 7 benchmarks with 20+ datapoints\n",
      "Found 7 anomaly detection benchmarks with 20+ datapoints\n",
      "Scraping data for anomaly-detection-on-mvtec-ad...\n",
      "Saved 144 models for anomaly-detection-on-mvtec-ad\n",
      "Scraping data for anomaly-detection-on-visa...\n",
      "Saved 46 models for anomaly-detection-on-visa\n",
      "Scraping data for anomaly-detection-on-mvtec-loco-ad...\n",
      "Saved 40 models for anomaly-detection-on-mvtec-loco-ad\n",
      "Scraping data for anomaly-detection-on-one-class-cifar-10...\n",
      "Saved 36 models for anomaly-detection-on-one-class-cifar-10\n",
      "Scraping data for anomaly-detection-on-chuk-avenue...\n",
      "Saved 34 models for anomaly-detection-on-chuk-avenue\n",
      "Scraping data for anomaly-detection-on-shanghaitech...\n",
      "Saved 31 models for anomaly-detection-on-shanghaitech\n",
      "Scraping data for anomaly-detection-on-ucr-anomaly-archive...\n",
      "Saved 24 models for anomaly-detection-on-ucr-anomaly-archive\n",
      "Collected a total of 355 models across all benchmarks\n",
      "Combined 355 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Anomaly detection task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'anomaly_detection'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_anomaly_detection_benchmarks_with_20plus():\n",
    "    \"\"\"Get anomaly detection benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding anomaly detection benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-mvtec\",\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad\",\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-cifar-10\",\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-kdd-cup-99\",\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-ucsd-ped1\",\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-ucsd-ped2\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main anomaly detection page\n",
    "        driver.get(\"https://paperswithcode.com/task/anomaly-detection\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"anomaly\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential anomaly detection benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'anomaly_detection', 'anomaly_detection_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'anomaly_detection', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'anomaly_detection', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all anomaly detection models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'anomaly_detection')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'anomaly_detection', 'all_anomaly_detection_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'anomaly_detection', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting anomaly detection task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_anomaly_detection_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No anomaly detection benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} anomaly detection benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Anomaly detection task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbd33ef-f6af-41ee-b141-4c4025791605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting sentiment analysis task data collection (20+ datapoints)...\n",
      "Finding sentiment analysis benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 43 potential sentiment analysis benchmarks\n",
      "Checking sentiment-analysis-on-sst-2-binary...\n",
      "sentiment-analysis-on-sst-2-binary: 88 datapoints\n",
      "Added sentiment-analysis-on-sst-2-binary with 88 datapoints\n",
      "Checking sentiment-analysis-on-imdb...\n",
      "sentiment-analysis-on-imdb: 48 datapoints\n",
      "Added sentiment-analysis-on-imdb with 48 datapoints\n",
      "Checking sentiment-analysis-on-sst-5-fine-grained...\n",
      "sentiment-analysis-on-sst-5-fine-grained: 30 datapoints\n",
      "Added sentiment-analysis-on-sst-5-fine-grained with 30 datapoints\n",
      "Checking sentiment-analysis-on-yelp-binary...\n",
      "sentiment-analysis-on-yelp-binary: 20 datapoints\n",
      "Added sentiment-analysis-on-yelp-binary with 20 datapoints\n",
      "Checking sentiment-analysis-on-yelp-fine-grained...\n",
      "sentiment-analysis-on-yelp-fine-grained: 17 datapoints\n",
      "Checking aspect-based-sentiment-analysis-on-semeval...\n",
      "aspect-based-sentiment-analysis-on-semeval: 48 datapoints\n",
      "Added aspect-based-sentiment-analysis-on-semeval with 48 datapoints\n",
      "Checking sentiment-analysis-on-mr...\n",
      "sentiment-analysis-on-mr: 19 datapoints\n",
      "Checking sentiment-analysis-on-banglabook...\n",
      "sentiment-analysis-on-banglabook: 13 datapoints\n",
      "Checking sentiment-analysis-on-dynasent...\n",
      "sentiment-analysis-on-dynasent: 12 datapoints\n",
      "Checking sentiment-analysis-on-sst-3...\n",
      "sentiment-analysis-on-sst-3: 11 datapoints\n",
      "Checking sentiment-analysis-on-user-and-product...\n",
      "sentiment-analysis-on-user-and-product: 10 datapoints\n",
      "Checking sentiment-analysis-on-sentiment-merged...\n",
      "sentiment-analysis-on-sentiment-merged: 10 datapoints\n",
      "Checking sentiment-analysis-on-cr...\n",
      "sentiment-analysis-on-cr: 9 datapoints\n",
      "Checking sentiment-analysis-on-amazon-review-polarity...\n",
      "sentiment-analysis-on-amazon-review-polarity: 9 datapoints\n",
      "Checking sentiment-analysis-on-amazon-review-full...\n",
      "sentiment-analysis-on-amazon-review-full: 9 datapoints\n",
      "Checking sentiment-analysis-on-semeval-2014-task-4...\n",
      "sentiment-analysis-on-semeval-2014-task-4: 8 datapoints\n",
      "Checking sentiment-analysis-on-slue...\n",
      "sentiment-analysis-on-slue: 8 datapoints\n",
      "Checking sentiment-analysis-on-tweeteval...\n",
      "sentiment-analysis-on-tweeteval: 7 datapoints\n",
      "Checking sentiment-analysis-on-multi-domain-sentiment...\n",
      "sentiment-analysis-on-multi-domain-sentiment: 6 datapoints\n",
      "Checking sentiment-analysis-on-mpqa...\n",
      "sentiment-analysis-on-mpqa: 4 datapoints\n",
      "Checking sentiment-analysis-on-fiqa...\n",
      "sentiment-analysis-on-fiqa: 4 datapoints\n",
      "Checking sentiment-analysis-on-iitp-product-reviews...\n",
      "sentiment-analysis-on-iitp-product-reviews: 4 datapoints\n",
      "Checking sentiment-analysis-on-semeval-2017-task-4-a...\n",
      "sentiment-analysis-on-semeval-2017-task-4-a: 3 datapoints\n",
      "Checking sentiment-analysis-on-dbrd...\n",
      "sentiment-analysis-on-dbrd: 4 datapoints\n",
      "Checking sentiment-analysis-on-twitter...\n",
      "sentiment-analysis-on-twitter: 3 datapoints\n",
      "Checking sentiment-analysis-on-rusentiment...\n",
      "sentiment-analysis-on-rusentiment: 3 datapoints\n",
      "Checking sentiment-analysis-on-iitp-movie-reviews...\n",
      "sentiment-analysis-on-iitp-movie-reviews: 3 datapoints\n",
      "Checking sentiment-analysis-on-semeval...\n",
      "sentiment-analysis-on-semeval: 2 datapoints\n",
      "Checking sentiment-analysis-on-financial-phrasebank...\n",
      "sentiment-analysis-on-financial-phrasebank: 2 datapoints\n",
      "Checking sentiment-analysis-on-imdb-movie-reviews-1...\n",
      "sentiment-analysis-on-imdb-movie-reviews-1: 2 datapoints\n",
      "Checking sentiment-analysis-on-sogou-news...\n",
      "sentiment-analysis-on-sogou-news: 1 datapoints\n",
      "Checking sentiment-analysis-on-astd...\n",
      "sentiment-analysis-on-astd: 1 datapoints\n",
      "Checking sentiment-analysis-on-arsas...\n",
      "sentiment-analysis-on-arsas: 1 datapoints\n",
      "Checking sentiment-analysis-on-1b-words...\n",
      "sentiment-analysis-on-1b-words: 1 datapoints\n",
      "Checking sentiment-analysis-on-chnsenticorp-dev...\n",
      "sentiment-analysis-on-chnsenticorp-dev: 1 datapoints\n",
      "Checking sentiment-analysis-on-chnsenticorp...\n",
      "sentiment-analysis-on-chnsenticorp: 1 datapoints\n",
      "Checking sentiment-analysis-on-hard-1...\n",
      "sentiment-analysis-on-hard-1: 1 datapoints\n",
      "Checking sentiment-analysis-on-ajgt-1...\n",
      "sentiment-analysis-on-ajgt-1: 1 datapoints\n",
      "Checking sentiment-analysis-on-labr-2-class-unbalanced-1...\n",
      "sentiment-analysis-on-labr-2-class-unbalanced-1: 1 datapoints\n",
      "Checking sentiment-analysis-on-latvian-twitter-eater...\n",
      "sentiment-analysis-on-latvian-twitter-eater: 1 datapoints\n",
      "Checking sentiment-analysis-on-urdu-online-reviews...\n",
      "sentiment-analysis-on-urdu-online-reviews: 1 datapoints\n",
      "Checking sentiment-analysis-on-122-people-passenger...\n",
      "sentiment-analysis-on-122-people-passenger: 1 datapoints\n",
      "Checking sentiment-analysis-on-sail-2017...\n",
      "sentiment-analysis-on-sail-2017: 1 datapoints\n",
      "Saved 5 benchmarks with 20+ datapoints\n",
      "Found 5 sentiment analysis benchmarks with 20+ datapoints\n",
      "Scraping data for sentiment-analysis-on-sst-2-binary...\n",
      "Saved 88 models for sentiment-analysis-on-sst-2-binary\n",
      "Scraping data for sentiment-analysis-on-imdb...\n",
      "Saved 48 models for sentiment-analysis-on-imdb\n",
      "Scraping data for sentiment-analysis-on-sst-5-fine-grained...\n",
      "Saved 30 models for sentiment-analysis-on-sst-5-fine-grained\n",
      "Scraping data for sentiment-analysis-on-yelp-binary...\n",
      "Saved 20 models for sentiment-analysis-on-yelp-binary\n",
      "Scraping data for aspect-based-sentiment-analysis-on-semeval...\n",
      "Saved 48 models for aspect-based-sentiment-analysis-on-semeval\n",
      "Collected a total of 234 models across all benchmarks\n",
      "Combined 234 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Sentiment analysis task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'sentiment_analysis'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_sentiment_analysis_benchmarks_with_20plus():\n",
    "    \"\"\"Get sentiment analysis benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding sentiment analysis benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary\",\n",
    "        \"https://paperswithcode.com/sota/sentiment-analysis-on-imdb\",\n",
    "        \"https://paperswithcode.com/sota/sentiment-analysis-on-sst-5-fine-grained\",\n",
    "        \"https://paperswithcode.com/sota/sentiment-analysis-on-yelp-binary\",\n",
    "        \"https://paperswithcode.com/sota/sentiment-analysis-on-yelp-fine-grained\",\n",
    "        \"https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main sentiment analysis page\n",
    "        driver.get(\"https://paperswithcode.com/task/sentiment-analysis\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"sentiment\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential sentiment analysis benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'sentiment_analysis', 'sentiment_analysis_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'sentiment_analysis', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'sentiment_analysis', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all sentiment analysis models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'sentiment_analysis')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'sentiment_analysis', 'all_sentiment_analysis_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'sentiment_analysis', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting sentiment analysis task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_sentiment_analysis_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No sentiment analysis benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} sentiment analysis benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Sentiment analysis task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "235b3cd1-d077-4e16-9303-14d7e218b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting medical image segmentation task data collection (20+ datapoints)...\n",
      "Finding medical image segmentation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 51 potential medical image segmentation benchmarks\n",
      "Checking medical-image-segmentation-on-brats...\n",
      "medical-image-segmentation-on-brats: 0 datapoints\n",
      "Checking medical-image-segmentation-on-isic-2017...\n",
      "medical-image-segmentation-on-isic-2017: 0 datapoints\n",
      "Checking medical-image-segmentation-on-isic-2018...\n",
      "medical-image-segmentation-on-isic-2018: 2 datapoints\n",
      "Checking medical-image-segmentation-on-promise12...\n",
      "medical-image-segmentation-on-promise12: 1 datapoints\n",
      "Checking medical-image-segmentation-on-kvasir-seg...\n",
      "medical-image-segmentation-on-kvasir-seg: 56 datapoints\n",
      "Added medical-image-segmentation-on-kvasir-seg with 56 datapoints\n",
      "Checking brain-tumor-segmentation-on-brats-2018...\n",
      "brain-tumor-segmentation-on-brats-2018: 4 datapoints\n",
      "Checking medical-image-segmentation-on-cvc-clinicdb...\n",
      "medical-image-segmentation-on-cvc-clinicdb: 45 datapoints\n",
      "Added medical-image-segmentation-on-cvc-clinicdb with 45 datapoints\n",
      "Checking medical-image-segmentation-on-etis...\n",
      "medical-image-segmentation-on-etis: 24 datapoints\n",
      "Added medical-image-segmentation-on-etis with 24 datapoints\n",
      "Checking medical-image-segmentation-on-synapse-multi...\n",
      "medical-image-segmentation-on-synapse-multi: 23 datapoints\n",
      "Added medical-image-segmentation-on-synapse-multi with 23 datapoints\n",
      "Checking medical-image-segmentation-on-cvc-colondb...\n",
      "medical-image-segmentation-on-cvc-colondb: 23 datapoints\n",
      "Added medical-image-segmentation-on-cvc-colondb with 23 datapoints\n",
      "Checking medical-image-segmentation-on-automatic...\n",
      "medical-image-segmentation-on-automatic: 20 datapoints\n",
      "Added medical-image-segmentation-on-automatic with 20 datapoints\n",
      "Checking medical-image-segmentation-on-monuseg...\n",
      "medical-image-segmentation-on-monuseg: 13 datapoints\n",
      "Checking medical-image-segmentation-on-glas...\n",
      "medical-image-segmentation-on-glas: 10 datapoints\n",
      "Checking medical-image-segmentation-on-2018-data...\n",
      "medical-image-segmentation-on-2018-data: 9 datapoints\n",
      "Checking medical-image-segmentation-on-bkai-igh...\n",
      "medical-image-segmentation-on-bkai-igh: 9 datapoints\n",
      "Checking medical-image-segmentation-on-miccai-2015-1...\n",
      "medical-image-segmentation-on-miccai-2015-1: 8 datapoints\n",
      "Checking medical-image-segmentation-on-acdc...\n",
      "medical-image-segmentation-on-acdc: 6 datapoints\n",
      "Checking medical-image-segmentation-on-drive-1...\n",
      "medical-image-segmentation-on-drive-1: 5 datapoints\n",
      "Checking medical-image-segmentation-on-cvc...\n",
      "medical-image-segmentation-on-cvc: 5 datapoints\n",
      "Checking medical-image-segmentation-on-isic-2018-1...\n",
      "medical-image-segmentation-on-isic-2018-1: 5 datapoints\n",
      "Checking medical-image-segmentation-on-medical...\n",
      "medical-image-segmentation-on-medical: 5 datapoints\n",
      "Checking medical-image-segmentation-on-robust-mis...\n",
      "medical-image-segmentation-on-robust-mis: 4 datapoints\n",
      "Checking medical-image-segmentation-on-isbi-2012-em...\n",
      "medical-image-segmentation-on-isbi-2012-em: 3 datapoints\n",
      "Checking medical-image-segmentation-on-em...\n",
      "medical-image-segmentation-on-em: 3 datapoints\n",
      "Checking medical-image-segmentation-on-rite...\n",
      "medical-image-segmentation-on-rite: 3 datapoints\n",
      "Checking medical-image-segmentation-on-kvasir...\n",
      "medical-image-segmentation-on-kvasir: 3 datapoints\n",
      "Checking medical-image-segmentation-on-brain-us...\n",
      "medical-image-segmentation-on-brain-us: 3 datapoints\n",
      "Checking medical-image-segmentation-on-chase-db1...\n",
      "medical-image-segmentation-on-chase-db1: 3 datapoints\n",
      "Checking medical-image-segmentation-on-isic2018...\n",
      "medical-image-segmentation-on-isic2018: 3 datapoints\n",
      "Checking medical-image-segmentation-on-endotect-polyp...\n",
      "medical-image-segmentation-on-endotect-polyp: 2 datapoints\n",
      "Checking medical-image-segmentation-on-medico...\n",
      "medical-image-segmentation-on-medico: 2 datapoints\n",
      "Checking medical-image-segmentation-on-kvasircapsule...\n",
      "medical-image-segmentation-on-kvasircapsule: 2 datapoints\n",
      "Checking medical-image-segmentation-on-lits2017...\n",
      "medical-image-segmentation-on-lits2017: 2 datapoints\n",
      "Checking medical-image-segmentation-on-iseg-2017...\n",
      "medical-image-segmentation-on-iseg-2017: 1 datapoints\n",
      "Checking medical-image-segmentation-on-chaos-mri...\n",
      "medical-image-segmentation-on-chaos-mri: 1 datapoints\n",
      "Checking medical-image-segmentation-on-hsvm...\n",
      "medical-image-segmentation-on-hsvm: 1 datapoints\n",
      "Checking medical-image-segmentation-on-cell...\n",
      "medical-image-segmentation-on-cell: 1 datapoints\n",
      "Checking medical-image-segmentation-on-2015-miccai...\n",
      "medical-image-segmentation-on-2015-miccai: 1 datapoints\n",
      "Checking medical-image-segmentation-on-synapse...\n",
      "medical-image-segmentation-on-synapse: 1 datapoints\n",
      "Checking medical-image-segmentation-on-segpc-2021...\n",
      "medical-image-segmentation-on-segpc-2021: 1 datapoints\n",
      "Checking medical-image-segmentation-on-autoimmune...\n",
      "medical-image-segmentation-on-autoimmune: 1 datapoints\n",
      "Checking medical-image-segmentation-on-hyper-kvasir...\n",
      "medical-image-segmentation-on-hyper-kvasir: 1 datapoints\n",
      "Checking medical-image-segmentation-on-asu-mayo-clinic-1...\n",
      "medical-image-segmentation-on-asu-mayo-clinic-1: 1 datapoints\n",
      "Checking medical-image-segmentation-on-mosmeddata...\n",
      "medical-image-segmentation-on-mosmeddata: 1 datapoints\n",
      "Checking medical-image-segmentation-on-miccai-2015-2...\n",
      "medical-image-segmentation-on-miccai-2015-2: 1 datapoints\n",
      "Checking medical-image-segmentation-on-monuseg-2018...\n",
      "medical-image-segmentation-on-monuseg-2018: 1 datapoints\n",
      "Checking medical-image-segmentation-on-monusac...\n",
      "medical-image-segmentation-on-monusac: 1 datapoints\n",
      "Checking medical-image-segmentation-on-amos...\n",
      "medical-image-segmentation-on-amos: 1 datapoints\n",
      "Checking medical-image-segmentation-on-extended-task10...\n",
      "medical-image-segmentation-on-extended-task10: 1 datapoints\n",
      "Checking medical-image-segmentation-on-autooral...\n",
      "medical-image-segmentation-on-autooral: 1 datapoints\n",
      "Checking medical-image-segmentation-on-enseg...\n",
      "medical-image-segmentation-on-enseg: 1 datapoints\n",
      "Saved 6 benchmarks with 20+ datapoints\n",
      "Found 6 medical image segmentation benchmarks with 20+ datapoints\n",
      "Scraping data for medical-image-segmentation-on-kvasir-seg...\n",
      "Saved 56 models for medical-image-segmentation-on-kvasir-seg\n",
      "Scraping data for medical-image-segmentation-on-cvc-clinicdb...\n",
      "Saved 45 models for medical-image-segmentation-on-cvc-clinicdb\n",
      "Scraping data for medical-image-segmentation-on-etis...\n",
      "Saved 24 models for medical-image-segmentation-on-etis\n",
      "Scraping data for medical-image-segmentation-on-synapse-multi...\n",
      "Saved 23 models for medical-image-segmentation-on-synapse-multi\n",
      "Scraping data for medical-image-segmentation-on-cvc-colondb...\n",
      "Saved 23 models for medical-image-segmentation-on-cvc-colondb\n",
      "Scraping data for medical-image-segmentation-on-automatic...\n",
      "Saved 20 models for medical-image-segmentation-on-automatic\n",
      "Collected a total of 191 models across all benchmarks\n",
      "Combined 191 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Medical image segmentation task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'medical_image_segmentation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_medical_image_segmentation_benchmarks_with_20plus():\n",
    "    \"\"\"Get medical image segmentation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding medical image segmentation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-brats\",\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-isic-2017\",\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-isic-2018\",\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-promise12\",\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-kvasir-seg\",\n",
    "        \"https://paperswithcode.com/sota/brain-tumor-segmentation-on-brats-2018\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main medical image segmentation page\n",
    "        driver.get(\"https://paperswithcode.com/task/medical-image-segmentation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and (\"medical\" in href.lower() or \"segmentation\" in href.lower()):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential medical image segmentation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'medical_image_segmentation', 'medical_image_segmentation_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'medical_image_segmentation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'medical_image_segmentation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all medical image segmentation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'medical_image_segmentation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'medical_image_segmentation', 'all_medical_image_segmentation_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'medical_image_segmentation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting medical image segmentation task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_medical_image_segmentation_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No medical image segmentation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} medical image segmentation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Medical image segmentation task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89b9ef20-9b54-41b7-839d-8e8b79662fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting few-shot learning task data collection (20+ datapoints)...\n",
      "Finding few-shot learning benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 33 potential few-shot learning benchmarks\n",
      "Checking few-shot-image-classification-on-meta-dataset...\n",
      "few-shot-image-classification-on-meta-dataset: 22 datapoints\n",
      "Added few-shot-image-classification-on-meta-dataset with 22 datapoints\n",
      "Checking few-shot-image-classification-on-omniglot...\n",
      "few-shot-image-classification-on-omniglot: 2 datapoints\n",
      "Checking few-shot-image-classification-on-mini...\n",
      "few-shot-image-classification-on-mini: 0 datapoints\n",
      "Checking few-shot-image-classification-on-tiered...\n",
      "few-shot-image-classification-on-tiered: 49 datapoints\n",
      "Added few-shot-image-classification-on-tiered with 49 datapoints\n",
      "Checking few-shot-image-classification-on-cifarfs...\n",
      "few-shot-image-classification-on-cifarfs: 0 datapoints\n",
      "Checking few-shot-image-classification-on-fc100...\n",
      "few-shot-image-classification-on-fc100: 0 datapoints\n",
      "Checking few-shot-learning-on-medconceptsqa...\n",
      "few-shot-learning-on-medconceptsqa: 12 datapoints\n",
      "Checking few-shot-learning-on-fgvc-aircraft-1...\n",
      "few-shot-learning-on-fgvc-aircraft-1: 4 datapoints\n",
      "Checking few-shot-learning-on-dtd...\n",
      "few-shot-learning-on-dtd: 4 datapoints\n",
      "Checking few-shot-learning-on-mini-imagenet-5-way-1...\n",
      "few-shot-learning-on-mini-imagenet-5-way-1: 3 datapoints\n",
      "Checking few-shot-learning-on-stanford-cars...\n",
      "few-shot-learning-on-stanford-cars: 3 datapoints\n",
      "Checking few-shot-learning-on-mini-imagenet-5-shot...\n",
      "few-shot-learning-on-mini-imagenet-5-shot: 3 datapoints\n",
      "Checking few-shot-learning-on-mini-imagenet-1-shot-2...\n",
      "few-shot-learning-on-mini-imagenet-1-shot-2: 2 datapoints\n",
      "Checking few-shot-learning-on-sst-2-binary...\n",
      "few-shot-learning-on-sst-2-binary: 1 datapoints\n",
      "Checking few-shot-learning-on-mr...\n",
      "few-shot-learning-on-mr: 1 datapoints\n",
      "Checking few-shot-learning-on-cr...\n",
      "few-shot-learning-on-cr: 1 datapoints\n",
      "Checking few-shot-learning-on-mrpc...\n",
      "few-shot-learning-on-mrpc: 1 datapoints\n",
      "Checking few-shot-learning-on-glue-qqp...\n",
      "few-shot-learning-on-glue-qqp: 1 datapoints\n",
      "Checking few-shot-learning-on-caltech101...\n",
      "few-shot-learning-on-caltech101: 1 datapoints\n",
      "Checking few-shot-learning-on-oxfordpets...\n",
      "few-shot-learning-on-oxfordpets: 1 datapoints\n",
      "Checking few-shot-learning-on-stanforcars...\n",
      "few-shot-learning-on-stanforcars: 1 datapoints\n",
      "Checking few-shot-learning-on-flowers-102...\n",
      "few-shot-learning-on-flowers-102: 1 datapoints\n",
      "Checking few-shot-learning-on-food101...\n",
      "few-shot-learning-on-food101: 1 datapoints\n",
      "Checking few-shot-learning-on-sun397...\n",
      "few-shot-learning-on-sun397: 1 datapoints\n",
      "Checking few-shot-learning-on-eurosat...\n",
      "few-shot-learning-on-eurosat: 1 datapoints\n",
      "Checking few-shot-learning-on-ucf101...\n",
      "few-shot-learning-on-ucf101: 1 datapoints\n",
      "Checking few-shot-learning-on-large-covid-19-ct-scan...\n",
      "few-shot-learning-on-large-covid-19-ct-scan: 1 datapoints\n",
      "Checking few-shot-learning-on-casehold...\n",
      "few-shot-learning-on-casehold: 1 datapoints\n",
      "Checking few-shot-learning-on-mednli...\n",
      "few-shot-learning-on-mednli: 1 datapoints\n",
      "Checking few-shot-learning-on-pubmedqa...\n",
      "few-shot-learning-on-pubmedqa: 1 datapoints\n",
      "Checking few-shot-learning-on-mini-imagenet-10-shot...\n",
      "few-shot-learning-on-mini-imagenet-10-shot: 1 datapoints\n",
      "Checking few-shot-learning-on-mini-imagenet-20-shot...\n",
      "few-shot-learning-on-mini-imagenet-20-shot: 1 datapoints\n",
      "Checking few-shot-learning-on-tieredimagenet-5-shot...\n",
      "few-shot-learning-on-tieredimagenet-5-shot: 1 datapoints\n",
      "Saved 2 benchmarks with 20+ datapoints\n",
      "Found 2 few-shot learning benchmarks with 20+ datapoints\n",
      "Scraping data for few-shot-image-classification-on-meta-dataset...\n",
      "Saved 22 models for few-shot-image-classification-on-meta-dataset\n",
      "Scraping data for few-shot-image-classification-on-tiered...\n",
      "Saved 49 models for few-shot-image-classification-on-tiered\n",
      "Collected a total of 71 models across all benchmarks\n",
      "Combined 71 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Few-shot learning task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'few_shot_learning'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_few_shot_learning_benchmarks_with_20plus():\n",
    "    \"\"\"Get few-shot learning benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding few-shot learning benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/few-shot-image-classification-on-meta-dataset\",\n",
    "        \"https://paperswithcode.com/sota/few-shot-image-classification-on-omniglot\",\n",
    "        \"https://paperswithcode.com/sota/few-shot-image-classification-on-mini\",\n",
    "        \"https://paperswithcode.com/sota/few-shot-image-classification-on-tiered\",\n",
    "        \"https://paperswithcode.com/sota/few-shot-image-classification-on-cifarfs\",\n",
    "        \"https://paperswithcode.com/sota/few-shot-image-classification-on-fc100\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main few-shot learning page\n",
    "        driver.get(\"https://paperswithcode.com/task/few-shot-learning\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and (\"few-shot\" in href.lower() or \"fewshot\" in href.lower()):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential few-shot learning benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'few_shot_learning', 'few_shot_learning_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'few_shot_learning', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'few_shot_learning', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all few-shot learning models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'few_shot_learning')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'few_shot_learning', 'all_few_shot_learning_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'few_shot_learning', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting few-shot learning task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_few_shot_learning_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No few-shot learning benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} few-shot learning benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Few-shot learning task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b5f745e-cb0d-4527-9b5f-21caf70404af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting action recognition task data collection (20+ datapoints)...\n",
      "Finding action recognition benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 58 potential action recognition benchmarks\n",
      "Checking action-recognition-in-videos-on-kinetics-400...\n",
      "action-recognition-in-videos-on-kinetics-400: 0 datapoints\n",
      "Checking action-recognition-in-videos-on-hmdb-51...\n",
      "action-recognition-in-videos-on-hmdb-51: 76 datapoints\n",
      "Added action-recognition-in-videos-on-hmdb-51 with 76 datapoints\n",
      "Checking action-recognition-in-videos-on-ucf101...\n",
      "action-recognition-in-videos-on-ucf101: 90 datapoints\n",
      "Added action-recognition-in-videos-on-ucf101 with 90 datapoints\n",
      "Checking action-recognition-in-videos-on-kinetics-600...\n",
      "action-recognition-in-videos-on-kinetics-600: 1 datapoints\n",
      "Checking action-recognition-in-videos-on-kinetics-700...\n",
      "action-recognition-in-videos-on-kinetics-700: 0 datapoints\n",
      "Checking action-recognition-in-videos-on-something...\n",
      "action-recognition-in-videos-on-something: 122 datapoints\n",
      "Added action-recognition-in-videos-on-something with 122 datapoints\n",
      "Checking action-recognition-in-videos-on-something-1...\n",
      "action-recognition-in-videos-on-something-1: 74 datapoints\n",
      "Added action-recognition-in-videos-on-something-1 with 74 datapoints\n",
      "Checking action-recognition-on-ava-v2-2...\n",
      "action-recognition-on-ava-v2-2: 38 datapoints\n",
      "Added action-recognition-on-ava-v2-2 with 38 datapoints\n",
      "Checking action-recognition-on-epic-kitchens-100...\n",
      "action-recognition-on-epic-kitchens-100: 30 datapoints\n",
      "Added action-recognition-on-epic-kitchens-100 with 30 datapoints\n",
      "Checking action-recognition-in-videos-on-ntu-rgbd...\n",
      "action-recognition-in-videos-on-ntu-rgbd: 27 datapoints\n",
      "Added action-recognition-in-videos-on-ntu-rgbd with 27 datapoints\n",
      "Checking action-recognition-in-videos-on-ntu-rgbd-120...\n",
      "action-recognition-in-videos-on-ntu-rgbd-120: 20 datapoints\n",
      "Added action-recognition-in-videos-on-ntu-rgbd-120 with 20 datapoints\n",
      "Checking action-recognition-on-diving-48...\n",
      "action-recognition-on-diving-48: 18 datapoints\n",
      "Checking action-recognition-in-videos-on-activitynet...\n",
      "action-recognition-in-videos-on-activitynet: 16 datapoints\n",
      "Checking action-recognition-in-videos-on-ava-v21...\n",
      "action-recognition-in-videos-on-ava-v21: 15 datapoints\n",
      "Checking action-recognition-on-h2o-2-hands-and-objects...\n",
      "action-recognition-on-h2o-2-hands-and-objects: 11 datapoints\n",
      "Checking action-recognition-in-videos-on-thumos14...\n",
      "action-recognition-in-videos-on-thumos14: 10 datapoints\n",
      "Checking action-recognition-in-videos-on-sports-1m...\n",
      "action-recognition-in-videos-on-sports-1m: 9 datapoints\n",
      "Checking action-recognition-on-hacs...\n",
      "action-recognition-on-hacs: 8 datapoints\n",
      "Checking action-recognition-on-charades-ego...\n",
      "action-recognition-on-charades-ego: 6 datapoints\n",
      "Checking action-recognition-on-haa500...\n",
      "action-recognition-on-haa500: 4 datapoints\n",
      "Checking action-recognition-on-bar...\n",
      "action-recognition-on-bar: 4 datapoints\n",
      "Checking action-recognition-on-uav-human...\n",
      "action-recognition-on-uav-human: 4 datapoints\n",
      "Checking action-recognition-in-videos-on-volleyball...\n",
      "action-recognition-in-videos-on-volleyball: 4 datapoints\n",
      "Checking action-recognition-on-real-life-violence...\n",
      "action-recognition-on-real-life-violence: 3 datapoints\n",
      "Checking action-recognition-on-rareact...\n",
      "action-recognition-on-rareact: 3 datapoints\n",
      "Checking action-recognition-on-jester-gesture...\n",
      "action-recognition-on-jester-gesture: 3 datapoints\n",
      "Checking action-recognition-in-videos-on-minisports...\n",
      "action-recognition-in-videos-on-minisports: 2 datapoints\n",
      "Checking action-recognition-in-videos-on-ird...\n",
      "action-recognition-in-videos-on-ird: 2 datapoints\n",
      "Checking action-recognition-in-videos-on-icvl-4...\n",
      "action-recognition-in-videos-on-icvl-4: 2 datapoints\n",
      "Checking action-recognition-in-videos-on-ucf-101...\n",
      "action-recognition-in-videos-on-ucf-101: 2 datapoints\n",
      "Checking action-recognition-on-mimetics...\n",
      "action-recognition-on-mimetics: 2 datapoints\n",
      "Checking action-recognition-on-penn-action...\n",
      "action-recognition-on-penn-action: 2 datapoints\n",
      "Checking action-recognition-on-drone-action...\n",
      "action-recognition-on-drone-action: 2 datapoints\n",
      "Checking action-recognition-on-okutama-action...\n",
      "action-recognition-on-okutama-action: 2 datapoints\n",
      "Checking action-recognition-on-animal-kingdom...\n",
      "action-recognition-on-animal-kingdom: 4 datapoints\n",
      "Checking action-recognition-on-sl-animals...\n",
      "action-recognition-on-sl-animals: 2 datapoints\n",
      "Checking action-recognition-in-videos-on-charades...\n",
      "action-recognition-in-videos-on-charades: 1 datapoints\n",
      "Checking action-recognition-in-videos-on-virat-ground...\n",
      "action-recognition-in-videos-on-virat-ground: 1 datapoints\n",
      "Checking action-recognition-in-videos-on-actionnet-ve...\n",
      "action-recognition-in-videos-on-actionnet-ve: 1 datapoints\n",
      "Checking action-recognition-in-videos-on-utd-mhad...\n",
      "action-recognition-in-videos-on-utd-mhad: 1 datapoints\n",
      "Checking action-recognition-in-videos-on-egogesture...\n",
      "action-recognition-in-videos-on-egogesture: 1 datapoints\n",
      "Checking action-recognition-in-videos-on-epic-kitchens...\n",
      "action-recognition-in-videos-on-epic-kitchens: 1 datapoints\n",
      "Checking action-recognition-in-videos-on-hmdb51...\n",
      "action-recognition-in-videos-on-hmdb51: 1 datapoints\n",
      "Checking action-recognition-on-meccano...\n",
      "action-recognition-on-meccano: 1 datapoints\n",
      "Checking action-recognition-on-win-fail-action...\n",
      "action-recognition-on-win-fail-action: 1 datapoints\n",
      "Checking action-recognition-on-mtl-aqa...\n",
      "action-recognition-on-mtl-aqa: 1 datapoints\n",
      "Checking action-recognition-on-ucf-101...\n",
      "action-recognition-on-ucf-101: 1 datapoints\n",
      "Checking action-recognition-on-skeleton-mimetics...\n",
      "action-recognition-on-skeleton-mimetics: 1 datapoints\n",
      "Checking action-recognition-on-rocog-v2...\n",
      "action-recognition-on-rocog-v2: 1 datapoints\n",
      "Checking action-recognition-on-nec-drone...\n",
      "action-recognition-on-nec-drone: 1 datapoints\n",
      "Checking action-recognition-on-uav-human-1...\n",
      "action-recognition-on-uav-human-1: 1 datapoints\n",
      "Checking action-recognition-on-thumos14...\n",
      "action-recognition-on-thumos14: 1 datapoints\n",
      "Checking action-recognition-on-hockey...\n",
      "action-recognition-on-hockey: 1 datapoints\n",
      "Checking action-recognition-on-n-ucla...\n",
      "action-recognition-on-n-ucla: 1 datapoints\n",
      "Checking action-recognition-on-industreal...\n",
      "action-recognition-on-industreal: 1 datapoints\n",
      "Checking action-recognition-on-ucfsports...\n",
      "action-recognition-on-ucfsports: 1 datapoints\n",
      "Checking action-recognition-on-kth...\n",
      "action-recognition-on-kth: 1 datapoints\n",
      "Checking action-recognition-on-dvs128-gesture...\n",
      "action-recognition-on-dvs128-gesture: 1 datapoints\n",
      "Saved 8 benchmarks with 20+ datapoints\n",
      "Found 8 action recognition benchmarks with 20+ datapoints\n",
      "Scraping data for action-recognition-in-videos-on-hmdb-51...\n",
      "Saved 76 models for action-recognition-in-videos-on-hmdb-51\n",
      "Scraping data for action-recognition-in-videos-on-ucf101...\n",
      "Saved 89 models for action-recognition-in-videos-on-ucf101\n",
      "Scraping data for action-recognition-in-videos-on-something...\n",
      "Saved 122 models for action-recognition-in-videos-on-something\n",
      "Scraping data for action-recognition-in-videos-on-something-1...\n",
      "Saved 74 models for action-recognition-in-videos-on-something-1\n",
      "Scraping data for action-recognition-on-ava-v2-2...\n",
      "Saved 38 models for action-recognition-on-ava-v2-2\n",
      "Scraping data for action-recognition-on-epic-kitchens-100...\n",
      "Saved 30 models for action-recognition-on-epic-kitchens-100\n",
      "Scraping data for action-recognition-in-videos-on-ntu-rgbd...\n",
      "Saved 27 models for action-recognition-in-videos-on-ntu-rgbd\n",
      "Scraping data for action-recognition-in-videos-on-ntu-rgbd-120...\n",
      "Saved 20 models for action-recognition-in-videos-on-ntu-rgbd-120\n",
      "Collected a total of 476 models across all benchmarks\n",
      "Combined 476 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Action recognition task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'action_recognition'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_action_recognition_benchmarks_with_20plus():\n",
    "    \"\"\"Get action recognition benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding action recognition benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/action-recognition-in-videos-on-kinetics-400\",\n",
    "        \"https://paperswithcode.com/sota/action-recognition-in-videos-on-hmdb-51\",\n",
    "        \"https://paperswithcode.com/sota/action-recognition-in-videos-on-ucf101\",\n",
    "        \"https://paperswithcode.com/sota/action-recognition-in-videos-on-kinetics-600\",\n",
    "        \"https://paperswithcode.com/sota/action-recognition-in-videos-on-kinetics-700\",\n",
    "        \"https://paperswithcode.com/sota/action-recognition-in-videos-on-something\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main action recognition page\n",
    "        driver.get(\"https://paperswithcode.com/task/action-recognition-in-videos\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and (\"action-recognition\" in href.lower() or \"video\" in href.lower()):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential action recognition benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'action_recognition', 'action_recognition_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'action_recognition', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'action_recognition', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all action recognition models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'action_recognition')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'action_recognition', 'all_action_recognition_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'action_recognition', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting action recognition task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_action_recognition_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No action recognition benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} action recognition benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Action recognition task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d959b267-a7c3-4999-84e3-a58dbecdd9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting language modelling task data collection (20+ datapoints)...\n",
      "Finding language modelling benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 57 potential language modelling benchmarks\n",
      "Checking language-modelling-on-penn-treebank-word...\n",
      "language-modelling-on-penn-treebank-word: 43 datapoints\n",
      "Added language-modelling-on-penn-treebank-word with 43 datapoints\n",
      "Checking language-modelling-on-wikitext-103...\n",
      "language-modelling-on-wikitext-103: 89 datapoints\n",
      "Added language-modelling-on-wikitext-103 with 89 datapoints\n",
      "Checking language-modelling-on-wikitext-2...\n",
      "language-modelling-on-wikitext-2: 38 datapoints\n",
      "Added language-modelling-on-wikitext-2 with 38 datapoints\n",
      "Checking language-modelling-on-1b-word...\n",
      "language-modelling-on-1b-word: 0 datapoints\n",
      "Checking language-modelling-on-lambada...\n",
      "language-modelling-on-lambada: 37 datapoints\n",
      "Added language-modelling-on-lambada with 37 datapoints\n",
      "Checking language-modelling-on-ptb-character...\n",
      "language-modelling-on-ptb-character: 0 datapoints\n",
      "Checking language-modelling-on-enwiki8...\n",
      "language-modelling-on-enwiki8: 42 datapoints\n",
      "Added language-modelling-on-enwiki8 with 42 datapoints\n",
      "Checking language-modelling-on-the-pile...\n",
      "language-modelling-on-the-pile: 39 datapoints\n",
      "Added language-modelling-on-the-pile with 39 datapoints\n",
      "Checking language-modelling-on-one-billion-word...\n",
      "language-modelling-on-one-billion-word: 27 datapoints\n",
      "Added language-modelling-on-one-billion-word with 27 datapoints\n",
      "Checking language-modelling-on-text8...\n",
      "language-modelling-on-text8: 24 datapoints\n",
      "Added language-modelling-on-text8 with 24 datapoints\n",
      "Checking language-modelling-on-penn-treebank-character...\n",
      "language-modelling-on-penn-treebank-character: 20 datapoints\n",
      "Added language-modelling-on-penn-treebank-character with 20 datapoints\n",
      "Checking language-modelling-on-hutter-prize...\n",
      "language-modelling-on-hutter-prize: 18 datapoints\n",
      "Checking language-modelling-on-c4...\n",
      "language-modelling-on-c4: 9 datapoints\n",
      "Checking language-modelling-on-salmon...\n",
      "language-modelling-on-salmon: 8 datapoints\n",
      "Checking language-modelling-on-openwebtext...\n",
      "language-modelling-on-openwebtext: 4 datapoints\n",
      "Checking language-modelling-on-wiki-40b...\n",
      "language-modelling-on-wiki-40b: 3 datapoints\n",
      "Checking language-modelling-on-big-bench-lite...\n",
      "language-modelling-on-big-bench-lite: 3 datapoints\n",
      "Checking language-modelling-on-fewclue-eprstmt...\n",
      "language-modelling-on-fewclue-eprstmt: 2 datapoints\n",
      "Checking language-modelling-on-fewclue-ocnli-fc...\n",
      "language-modelling-on-fewclue-ocnli-fc: 2 datapoints\n",
      "Checking language-modelling-on-fewclue-bustm...\n",
      "language-modelling-on-fewclue-bustm: 2 datapoints\n",
      "Checking language-modelling-on-fewclue-chid-fc...\n",
      "language-modelling-on-fewclue-chid-fc: 2 datapoints\n",
      "Checking language-modelling-on-fewclue-cluewsc-fc...\n",
      "language-modelling-on-fewclue-cluewsc-fc: 2 datapoints\n",
      "Checking language-modelling-on-clue-c3...\n",
      "language-modelling-on-clue-c3: 2 datapoints\n",
      "Checking language-modelling-on-clue-wsc1-1...\n",
      "language-modelling-on-clue-wsc1-1: 2 datapoints\n",
      "Checking language-modelling-on-clue-cmnli...\n",
      "language-modelling-on-clue-cmnli: 2 datapoints\n",
      "Checking language-modelling-on-clue-drcd...\n",
      "language-modelling-on-clue-drcd: 2 datapoints\n",
      "Checking language-modelling-on-clue-ocnli-50k...\n",
      "language-modelling-on-clue-ocnli-50k: 2 datapoints\n",
      "Checking language-modelling-on-clue-afqmc...\n",
      "language-modelling-on-clue-afqmc: 2 datapoints\n",
      "Checking language-modelling-on-clue-cmrc2018...\n",
      "language-modelling-on-clue-cmrc2018: 2 datapoints\n",
      "Checking language-modelling-on-vietmed...\n",
      "language-modelling-on-vietmed: 2 datapoints\n",
      "Checking language-modelling-on-enwiki8-1...\n",
      "language-modelling-on-enwiki8-1: 1 datapoints\n",
      "Checking language-modelling-on-ptb...\n",
      "language-modelling-on-ptb: 1 datapoints\n",
      "Checking language-modelling-on-text8-dev...\n",
      "language-modelling-on-text8-dev: 1 datapoints\n",
      "Checking language-modelling-on-enwik8-dev...\n",
      "language-modelling-on-enwik8-dev: 1 datapoints\n",
      "Checking language-modelling-on-pubmed-abstracts...\n",
      "language-modelling-on-pubmed-abstracts: 1 datapoints\n",
      "Checking language-modelling-on-dm-mathematics...\n",
      "language-modelling-on-dm-mathematics: 1 datapoints\n",
      "Checking language-modelling-on-ubuntu-irc...\n",
      "language-modelling-on-ubuntu-irc: 1 datapoints\n",
      "Checking language-modelling-on-opensubtitles...\n",
      "language-modelling-on-opensubtitles: 1 datapoints\n",
      "Checking language-modelling-on-openwebtext2...\n",
      "language-modelling-on-openwebtext2: 1 datapoints\n",
      "Checking language-modelling-on-hackernews...\n",
      "language-modelling-on-hackernews: 1 datapoints\n",
      "Checking language-modelling-on-books3...\n",
      "language-modelling-on-books3: 1 datapoints\n",
      "Checking language-modelling-on-bookcorpus2...\n",
      "language-modelling-on-bookcorpus2: 1 datapoints\n",
      "Checking language-modelling-on-pile-cc...\n",
      "language-modelling-on-pile-cc: 1 datapoints\n",
      "Checking language-modelling-on-philpapers...\n",
      "language-modelling-on-philpapers: 1 datapoints\n",
      "Checking language-modelling-on-gutenberg-pg-19...\n",
      "language-modelling-on-gutenberg-pg-19: 1 datapoints\n",
      "Checking language-modelling-on-arxiv...\n",
      "language-modelling-on-arxiv: 1 datapoints\n",
      "Checking language-modelling-on-stackexchange...\n",
      "language-modelling-on-stackexchange: 1 datapoints\n",
      "Checking language-modelling-on-nih-exporter...\n",
      "language-modelling-on-nih-exporter: 1 datapoints\n",
      "Checking language-modelling-on-uspto-backgrounds...\n",
      "language-modelling-on-uspto-backgrounds: 1 datapoints\n",
      "Checking language-modelling-on-pubmed-central...\n",
      "language-modelling-on-pubmed-central: 1 datapoints\n",
      "Checking language-modelling-on-freelaw...\n",
      "language-modelling-on-freelaw: 1 datapoints\n",
      "Checking language-modelling-on-curation-corpus...\n",
      "language-modelling-on-curation-corpus: 1 datapoints\n",
      "Checking language-modelling-on-github...\n",
      "language-modelling-on-github: 1 datapoints\n",
      "Checking language-modelling-on-100-sleep-nights-of-8...\n",
      "language-modelling-on-100-sleep-nights-of-8: 1 datapoints\n",
      "Checking language-modelling-on-language-modeling...\n",
      "language-modelling-on-language-modeling: 1 datapoints\n",
      "Checking language-modelling-on-2000-hub5-english...\n",
      "language-modelling-on-2000-hub5-english: 1 datapoints\n",
      "Checking language-modelling-on-a1...\n",
      "language-modelling-on-a1: 1 datapoints\n",
      "Saved 9 benchmarks with 20+ datapoints\n",
      "Found 9 language modelling benchmarks with 20+ datapoints\n",
      "Scraping data for language-modelling-on-penn-treebank-word...\n",
      "Saved 43 models for language-modelling-on-penn-treebank-word\n",
      "Scraping data for language-modelling-on-wikitext-103...\n",
      "Saved 89 models for language-modelling-on-wikitext-103\n",
      "Scraping data for language-modelling-on-wikitext-2...\n",
      "Saved 38 models for language-modelling-on-wikitext-2\n",
      "Scraping data for language-modelling-on-lambada...\n",
      "Saved 37 models for language-modelling-on-lambada\n",
      "Scraping data for language-modelling-on-enwiki8...\n",
      "Saved 42 models for language-modelling-on-enwiki8\n",
      "Scraping data for language-modelling-on-the-pile...\n",
      "Saved 39 models for language-modelling-on-the-pile\n",
      "Scraping data for language-modelling-on-one-billion-word...\n",
      "Saved 27 models for language-modelling-on-one-billion-word\n",
      "Scraping data for language-modelling-on-text8...\n",
      "Saved 24 models for language-modelling-on-text8\n",
      "Scraping data for language-modelling-on-penn-treebank-character...\n",
      "Saved 20 models for language-modelling-on-penn-treebank-character\n",
      "Collected a total of 359 models across all benchmarks\n",
      "Combined 359 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Language modelling task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'language_modelling'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_language_modelling_benchmarks_with_20plus():\n",
    "    \"\"\"Get language modelling benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding language modelling benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/language-modelling-on-penn-treebank-word\",\n",
    "        \"https://paperswithcode.com/sota/language-modelling-on-wikitext-103\",\n",
    "        \"https://paperswithcode.com/sota/language-modelling-on-wikitext-2\",\n",
    "        \"https://paperswithcode.com/sota/language-modelling-on-1b-word\",\n",
    "        \"https://paperswithcode.com/sota/language-modelling-on-lambada\",\n",
    "        \"https://paperswithcode.com/sota/language-modelling-on-ptb-character\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main language modelling page\n",
    "        driver.get(\"https://paperswithcode.com/task/language-modelling\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"language-modelling\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential language modelling benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'language_modelling', 'language_modelling_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'language_modelling', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'language_modelling', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all language modelling models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'language_modelling')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'language_modelling', 'all_language_modelling_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'language_modelling', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting language modelling task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_language_modelling_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No language modelling benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} language modelling benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Language modelling task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f1b7cc-c418-4ab4-a464-ca45cd6f3299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting representation learning task data collection (20+ datapoints)...\n",
      "Finding representation learning benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 10 potential representation learning benchmarks\n",
      "Checking representation-learning-on-imagenet...\n",
      "representation-learning-on-imagenet: 0 datapoints\n",
      "Checking representation-learning-on-cifar-10...\n",
      "representation-learning-on-cifar-10: 0 datapoints\n",
      "Checking representation-learning-on-cifar-100...\n",
      "representation-learning-on-cifar-100: 0 datapoints\n",
      "Checking representation-learning-on-stl-10...\n",
      "representation-learning-on-stl-10: 0 datapoints\n",
      "Checking representation-learning-on-coco...\n",
      "representation-learning-on-coco: 0 datapoints\n",
      "Checking representation-learning-on-scidocs...\n",
      "representation-learning-on-scidocs: 7 datapoints\n",
      "Checking representation-learning-on-circle-data...\n",
      "representation-learning-on-circle-data: 1 datapoints\n",
      "Checking representation-learning-on-sports10...\n",
      "representation-learning-on-sports10: 1 datapoints\n",
      "Checking representation-learning-on-cifar10...\n",
      "representation-learning-on-cifar10: 1 datapoints\n",
      "Checking representation-learning-on-animals-10...\n",
      "representation-learning-on-animals-10: 1 datapoints\n",
      "No representation learning benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'representation_learning'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_representation_learning_benchmarks_with_20plus():\n",
    "    \"\"\"Get representation learning benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding representation learning benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/representation-learning-on-imagenet\",\n",
    "        \"https://paperswithcode.com/sota/representation-learning-on-cifar-10\",\n",
    "        \"https://paperswithcode.com/sota/representation-learning-on-cifar-100\",\n",
    "        \"https://paperswithcode.com/sota/representation-learning-on-stl-10\",\n",
    "        \"https://paperswithcode.com/sota/representation-learning-on-coco\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main representation learning page\n",
    "        driver.get(\"https://paperswithcode.com/task/representation-learning\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"representation-learning\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential representation learning benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'representation_learning', 'representation_learning_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'representation_learning', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'representation_learning', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all representation learning models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'representation_learning')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'representation_learning', 'all_representation_learning_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'representation_learning', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting representation learning task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_representation_learning_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No representation learning benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} representation learning benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Representation learning task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0be4c4b-2318-48fe-9247-166ba167ea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting classification task data collection (20+ datapoints)...\n",
      "Finding classification benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 63 potential classification benchmarks\n",
      "Checking image-classification-on-imagenet...\n",
      "image-classification-on-imagenet: 1060 datapoints\n",
      "Added image-classification-on-imagenet with 1060 datapoints\n",
      "Checking image-classification-on-cifar-10...\n",
      "image-classification-on-cifar-10: 264 datapoints\n",
      "Added image-classification-on-cifar-10 with 264 datapoints\n",
      "Checking image-classification-on-cifar-100...\n",
      "image-classification-on-cifar-100: 210 datapoints\n",
      "Added image-classification-on-cifar-100 with 210 datapoints\n",
      "Checking text-classification-on-imdb...\n",
      "text-classification-on-imdb: 0 datapoints\n",
      "Checking text-classification-on-yelp-5...\n",
      "text-classification-on-yelp-5: 7 datapoints\n",
      "Checking classification-on-n-imagenet...\n",
      "classification-on-n-imagenet: 9 datapoints\n",
      "Checking classification-on-indl...\n",
      "classification-on-indl: 9 datapoints\n",
      "Checking classification-on-mhist...\n",
      "classification-on-mhist: 9 datapoints\n",
      "Checking classification-on-spot-10...\n",
      "classification-on-spot-10: 9 datapoints\n",
      "Checking classification-on-full-body-parkinsons...\n",
      "classification-on-full-body-parkinsons: 7 datapoints\n",
      "Checking classification-on-n-cars...\n",
      "classification-on-n-cars: 6 datapoints\n",
      "Checking classification-on-n-imagenet-mini...\n",
      "classification-on-n-imagenet-mini: 6 datapoints\n",
      "Checking classification-on-imagenet-c-ood-class-out-of...\n",
      "classification-on-imagenet-c-ood-class-out-of: 5 datapoints\n",
      "Checking classification-on-autoimmune-dataset...\n",
      "classification-on-autoimmune-dataset: 4 datapoints\n",
      "Checking classification-on-cwru-bearing-dataset...\n",
      "classification-on-cwru-bearing-dataset: 3 datapoints\n",
      "Checking classification-on-forgerynet...\n",
      "classification-on-forgerynet: 3 datapoints\n",
      "Checking classification-on-shd-adding...\n",
      "classification-on-shd-adding: 3 datapoints\n",
      "Checking classification-on-ximagenet-12...\n",
      "classification-on-ximagenet-12: 3 datapoints\n",
      "Checking classification-on-rsscn7...\n",
      "classification-on-rsscn7: 2 datapoints\n",
      "Checking classification-on-reddit-ideology-database...\n",
      "classification-on-reddit-ideology-database: 2 datapoints\n",
      "Checking classification-on-medsecid...\n",
      "classification-on-medsecid: 2 datapoints\n",
      "Checking classification-on-bioscan-1m-insect-dataset...\n",
      "classification-on-bioscan-1m-insect-dataset: 2 datapoints\n",
      "Checking classification-on-sst-2...\n",
      "classification-on-sst-2: 2 datapoints\n",
      "Checking classification-on-cb...\n",
      "classification-on-cb: 2 datapoints\n",
      "Checking classification-on-wsc...\n",
      "classification-on-wsc: 2 datapoints\n",
      "Checking classification-on-wic...\n",
      "classification-on-wic: 2 datapoints\n",
      "Checking classification-on-rte...\n",
      "classification-on-rte: 2 datapoints\n",
      "Checking classification-on-boolq...\n",
      "classification-on-boolq: 2 datapoints\n",
      "Checking classification-on-burr-classification-images...\n",
      "classification-on-burr-classification-images: 3 datapoints\n",
      "Checking classification-on-biasbios...\n",
      "classification-on-biasbios: 1 datapoints\n",
      "Checking classification-on-brain-tumor-mri-dataset...\n",
      "classification-on-brain-tumor-mri-dataset: 1 datapoints\n",
      "Checking classification-on-isic-2019...\n",
      "classification-on-isic-2019: 1 datapoints\n",
      "Checking classification-on-sgd...\n",
      "classification-on-sgd: 1 datapoints\n",
      "Checking classification-on-chest-x-ray-images...\n",
      "classification-on-chest-x-ray-images: 1 datapoints\n",
      "Checking classification-on-bengali-ekman-s-six-basic...\n",
      "classification-on-bengali-ekman-s-six-basic: 1 datapoints\n",
      "Checking classification-on-hows...\n",
      "classification-on-hows: 1 datapoints\n",
      "Checking classification-on-hows-long...\n",
      "classification-on-hows-long: 1 datapoints\n",
      "Checking classification-on-sentiment140...\n",
      "classification-on-sentiment140: 1 datapoints\n",
      "Checking classification-on-mixedwm38...\n",
      "classification-on-mixedwm38: 1 datapoints\n",
      "Checking classification-on-corbel...\n",
      "classification-on-corbel: 1 datapoints\n",
      "Checking classification-on-cifake-real-and-ai...\n",
      "classification-on-cifake-real-and-ai: 1 datapoints\n",
      "Checking classification-on-sound-based-drone-fault...\n",
      "classification-on-sound-based-drone-fault: 1 datapoints\n",
      "Checking classification-on-irfl-image-recognition-of...\n",
      "classification-on-irfl-image-recognition-of: 1 datapoints\n",
      "Checking classification-on-mured-dataset...\n",
      "classification-on-mured-dataset: 1 datapoints\n",
      "Checking classification-on-kepler-exoplanet-search...\n",
      "classification-on-kepler-exoplanet-search: 1 datapoints\n",
      "Checking classification-on-tcga...\n",
      "classification-on-tcga: 1 datapoints\n",
      "Checking classification-on-cifar-100...\n",
      "classification-on-cifar-100: 1 datapoints\n",
      "Checking classification-on-adult...\n",
      "classification-on-adult: 1 datapoints\n",
      "Checking classification-on-hrf...\n",
      "classification-on-hrf: 1 datapoints\n",
      "Checking classification-on-rite...\n",
      "classification-on-rite: 1 datapoints\n",
      "Checking classification-on-les-av...\n",
      "classification-on-les-av: 1 datapoints\n",
      "Checking classification-on-aur-umb-dataset...\n",
      "classification-on-aur-umb-dataset: 1 datapoints\n",
      "Checking classification-on-diat-mradhar-radar-micro...\n",
      "classification-on-diat-mradhar-radar-micro: 1 datapoints\n",
      "Checking classification-on-covid-19-image-data...\n",
      "classification-on-covid-19-image-data: 1 datapoints\n",
      "Checking classification-on-tml1m...\n",
      "classification-on-tml1m: 1 datapoints\n",
      "Checking classification-on-tlf2k...\n",
      "classification-on-tlf2k: 1 datapoints\n",
      "Checking classification-on-tacm12k...\n",
      "classification-on-tacm12k: 1 datapoints\n",
      "Checking classification-on-respiratorydatabase-tr...\n",
      "classification-on-respiratorydatabase-tr: 1 datapoints\n",
      "Checking classification-on-cifar-10c...\n",
      "classification-on-cifar-10c: 1 datapoints\n",
      "Checking classification-on-insider-threat-test-dataset...\n",
      "classification-on-insider-threat-test-dataset: 1 datapoints\n",
      "Checking classification-on-coordinated-reply-attacks...\n",
      "classification-on-coordinated-reply-attacks: 1 datapoints\n",
      "Checking classification-on-liver-us...\n",
      "classification-on-liver-us: 1 datapoints\n",
      "Checking classification-on-simgas...\n",
      "classification-on-simgas: 1 datapoints\n",
      "Saved 3 benchmarks with 20+ datapoints\n",
      "Found 3 classification benchmarks with 20+ datapoints\n",
      "Scraping data for image-classification-on-imagenet...\n",
      "Saved 1046 models for image-classification-on-imagenet\n",
      "Scraping data for image-classification-on-cifar-10...\n",
      "Saved 263 models for image-classification-on-cifar-10\n",
      "Scraping data for image-classification-on-cifar-100...\n",
      "Saved 207 models for image-classification-on-cifar-100\n",
      "Collected a total of 1516 models across all benchmarks\n",
      "Combined 1516 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Classification task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'classification'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_classification_benchmarks_with_20plus():\n",
    "    \"\"\"Get classification benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding classification benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/image-classification-on-imagenet\",\n",
    "        \"https://paperswithcode.com/sota/image-classification-on-cifar-10\",\n",
    "        \"https://paperswithcode.com/sota/image-classification-on-cifar-100\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-imdb\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-yelp-5\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main classification page\n",
    "        driver.get(\"https://paperswithcode.com/task/classification-1\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"classification\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential classification benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'classification', 'classification_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'classification', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'classification', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all classification models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'classification')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'classification', 'all_classification_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'classification', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting classification task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_classification_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No classification benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} classification benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Classification task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252f119e-a893-4b85-abd3-cfe08816c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting text classification task data collection (20+ datapoints)...\n",
      "Finding text classification benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 67 potential text classification benchmarks\n",
      "Checking text-classification-on-imdb...\n",
      "text-classification-on-imdb: 0 datapoints\n",
      "Checking text-classification-on-yelp-5...\n",
      "text-classification-on-yelp-5: 7 datapoints\n",
      "Checking text-classification-on-ag-news...\n",
      "text-classification-on-ag-news: 21 datapoints\n",
      "Added text-classification-on-ag-news with 21 datapoints\n",
      "Checking text-classification-on-dbpedia...\n",
      "text-classification-on-dbpedia: 21 datapoints\n",
      "Added text-classification-on-dbpedia with 21 datapoints\n",
      "Checking text-classification-on-sst-2-binary...\n",
      "text-classification-on-sst-2-binary: 0 datapoints\n",
      "Checking text-classification-on-mteb...\n",
      "text-classification-on-mteb: 31 datapoints\n",
      "Added text-classification-on-mteb with 31 datapoints\n",
      "Checking text-classification-on-r8...\n",
      "text-classification-on-r8: 21 datapoints\n",
      "Added text-classification-on-r8 with 21 datapoints\n",
      "Checking text-classification-on-trec-6...\n",
      "text-classification-on-trec-6: 19 datapoints\n",
      "Checking text-classification-on-20news...\n",
      "text-classification-on-20news: 16 datapoints\n",
      "Checking text-classification-on-uk-key-stage...\n",
      "text-classification-on-uk-key-stage: 15 datapoints\n",
      "Checking text-classification-on-ohsumed...\n",
      "text-classification-on-ohsumed: 10 datapoints\n",
      "Checking text-classification-on-yahoo-answers...\n",
      "text-classification-on-yahoo-answers: 10 datapoints\n",
      "Checking text-classification-on-mr...\n",
      "text-classification-on-mr: 10 datapoints\n",
      "Checking text-classification-on-r52...\n",
      "text-classification-on-r52: 8 datapoints\n",
      "Checking text-classification-on-newsdiscourse...\n",
      "text-classification-on-newsdiscourse: 8 datapoints\n",
      "Checking text-classification-on-yelp-2...\n",
      "text-classification-on-yelp-2: 5 datapoints\n",
      "Checking text-classification-on-dodf-data...\n",
      "text-classification-on-dodf-data: 5 datapoints\n",
      "Checking text-classification-on-mvictor-type...\n",
      "text-classification-on-mvictor-type: 5 datapoints\n",
      "Checking text-classification-on-svictor-type...\n",
      "text-classification-on-svictor-type: 5 datapoints\n",
      "Checking text-classification-on-weebit-readability...\n",
      "text-classification-on-weebit-readability: 5 datapoints\n",
      "Checking text-classification-on-onestopenglish...\n",
      "text-classification-on-onestopenglish: 5 datapoints\n",
      "Checking text-classification-on-lot-insts...\n",
      "text-classification-on-lot-insts: 5 datapoints\n",
      "Checking text-classification-on-amazon-2...\n",
      "text-classification-on-amazon-2: 4 datapoints\n",
      "Checking text-classification-on-rcv1...\n",
      "text-classification-on-rcv1: 4 datapoints\n",
      "Checking text-classification-on-arxiv-10...\n",
      "text-classification-on-arxiv-10: 4 datapoints\n",
      "Checking text-classification-on-hatexplain-1...\n",
      "text-classification-on-hatexplain-1: 4 datapoints\n",
      "Checking text-classification-on-threatgram-101-extreme...\n",
      "text-classification-on-threatgram-101-extreme: 4 datapoints\n",
      "Checking text-classification-on-sogou-news...\n",
      "text-classification-on-sogou-news: 3 datapoints\n",
      "Checking text-classification-on-amazon-5...\n",
      "text-classification-on-amazon-5: 3 datapoints\n",
      "Checking text-classification-on-overruling...\n",
      "text-classification-on-overruling: 3 datapoints\n",
      "Checking text-classification-on-terms-of-service...\n",
      "text-classification-on-terms-of-service: 3 datapoints\n",
      "Checking text-classification-on-blurb...\n",
      "text-classification-on-blurb: 3 datapoints\n",
      "Checking text-classification-on-twitter...\n",
      "text-classification-on-twitter: 3 datapoints\n",
      "Checking text-classification-on-imdb-movie-reviews-1...\n",
      "text-classification-on-imdb-movie-reviews-1: 3 datapoints\n",
      "Checking text-classification-on-trec-50...\n",
      "text-classification-on-trec-50: 2 datapoints\n",
      "Checking text-classification-on-an-amharic-news-text...\n",
      "text-classification-on-an-amharic-news-text: 2 datapoints\n",
      "Checking text-classification-on-glue-sst2...\n",
      "text-classification-on-glue-sst2: 2 datapoints\n",
      "Checking text-classification-on-muld-character-type...\n",
      "text-classification-on-muld-character-type: 2 datapoints\n",
      "Checking text-classification-on-searchsnippets...\n",
      "text-classification-on-searchsnippets: 2 datapoints\n",
      "Checking text-classification-on-sst-2...\n",
      "text-classification-on-sst-2: 2 datapoints\n",
      "Checking text-classification-on-this-is-not-a-dataset...\n",
      "text-classification-on-this-is-not-a-dataset: 2 datapoints\n",
      "Checking text-classification-on-social-media...\n",
      "text-classification-on-social-media: 2 datapoints\n",
      "Checking text-classification-on-trac2-benghali-task-2...\n",
      "text-classification-on-trac2-benghali-task-2: 1 datapoints\n",
      "Checking text-classification-on-trac2-english-task2...\n",
      "text-classification-on-trac2-english-task2: 1 datapoints\n",
      "Checking text-classification-on-affcon-2020-emotion...\n",
      "text-classification-on-affcon-2020-emotion: 1 datapoints\n",
      "Checking text-classification-on-arxiv...\n",
      "text-classification-on-arxiv: 1 datapoints\n",
      "Checking text-classification-on-patents...\n",
      "text-classification-on-patents: 1 datapoints\n",
      "Checking text-classification-on-facebook-media...\n",
      "text-classification-on-facebook-media: 1 datapoints\n",
      "Checking text-classification-on-twitter-us...\n",
      "text-classification-on-twitter-us: 1 datapoints\n",
      "Checking text-classification-on-rusage-corpus-for-age...\n",
      "text-classification-on-rusage-corpus-for-age: 1 datapoints\n",
      "Checking text-classification-on-20-newsgroups...\n",
      "text-classification-on-20-newsgroups: 1 datapoints\n",
      "Checking text-classification-on-silicone-benchmark...\n",
      "text-classification-on-silicone-benchmark: 1 datapoints\n",
      "Checking text-classification-on-wnut-2020-task-2...\n",
      "text-classification-on-wnut-2020-task-2: 1 datapoints\n",
      "Checking text-classification-on-glue-mrpc...\n",
      "text-classification-on-glue-mrpc: 1 datapoints\n",
      "Checking text-classification-on-glue-cola...\n",
      "text-classification-on-glue-cola: 0 datapoints\n",
      "Checking text-classification-on-glue-rte...\n",
      "text-classification-on-glue-rte: 1 datapoints\n",
      "Checking text-classification-on-glue-stsb...\n",
      "text-classification-on-glue-stsb: 0 datapoints\n",
      "Checking text-classification-on-banking77...\n",
      "text-classification-on-banking77: 1 datapoints\n",
      "Checking text-classification-on-adverse-drug-events...\n",
      "text-classification-on-adverse-drug-events: 1 datapoints\n",
      "Checking text-classification-on-trec-10...\n",
      "text-classification-on-trec-10: 1 datapoints\n",
      "Checking text-classification-on-nice-45...\n",
      "text-classification-on-nice-45: 1 datapoints\n",
      "Checking text-classification-on-nice-2...\n",
      "text-classification-on-nice-2: 1 datapoints\n",
      "Checking text-classification-on-stops-41...\n",
      "text-classification-on-stops-41: 1 datapoints\n",
      "Checking text-classification-on-stops-2...\n",
      "text-classification-on-stops-2: 1 datapoints\n",
      "Checking text-classification-on-twitter-sentiment-1...\n",
      "text-classification-on-twitter-sentiment-1: 1 datapoints\n",
      "Checking text-classification-on-fmc-mwo2kg-1...\n",
      "text-classification-on-fmc-mwo2kg-1: 1 datapoints\n",
      "Checking text-classification-on-hyperpartisan-1...\n",
      "text-classification-on-hyperpartisan-1: 1 datapoints\n",
      "Saved 4 benchmarks with 20+ datapoints\n",
      "Found 4 text classification benchmarks with 20+ datapoints\n",
      "Scraping data for text-classification-on-ag-news...\n",
      "Saved 21 models for text-classification-on-ag-news\n",
      "Scraping data for text-classification-on-dbpedia...\n",
      "Saved 21 models for text-classification-on-dbpedia\n",
      "Scraping data for text-classification-on-mteb...\n",
      "Saved 31 models for text-classification-on-mteb\n",
      "Scraping data for text-classification-on-r8...\n",
      "Saved 21 models for text-classification-on-r8\n",
      "Collected a total of 94 models across all benchmarks\n",
      "Combined 94 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Text classification task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'text_classification'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_text_classification_benchmarks_with_20plus():\n",
    "    \"\"\"Get text classification benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding text classification benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-imdb\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-yelp-5\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-ag-news\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-dbpedia\",\n",
    "        \"https://paperswithcode.com/sota/text-classification-on-sst-2-binary\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main text classification page\n",
    "        driver.get(\"https://paperswithcode.com/task/text-classification\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"text-classification\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential text classification benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'text_classification', 'text_classification_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'text_classification', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'text_classification', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all text classification models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'text_classification')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'text_classification', 'all_text_classification_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'text_classification', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting text classification task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_text_classification_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No text classification benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} text classification benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Text classification task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b4a26e-7067-4a36-b37b-7ff38e6ad65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting deep reinforcement learning task data collection (20+ datapoints)...\n",
      "Finding deep reinforcement learning benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 5 potential deep reinforcement learning benchmarks\n",
      "Checking atari-games-on-atari-2600-breakout...\n",
      "atari-games-on-atari-2600-breakout: 58 datapoints\n",
      "Added atari-games-on-atari-2600-breakout with 58 datapoints\n",
      "Checking atari-games-on-atari-2600-pong...\n",
      "atari-games-on-atari-2600-pong: 52 datapoints\n",
      "Added atari-games-on-atari-2600-pong with 52 datapoints\n",
      "Checking atari-games-on-atari-2600-seaquest...\n",
      "atari-games-on-atari-2600-seaquest: 57 datapoints\n",
      "Added atari-games-on-atari-2600-seaquest with 57 datapoints\n",
      "Checking continuous-control-on-mujoco...\n",
      "continuous-control-on-mujoco: 0 datapoints\n",
      "Checking atari-games-on-atari-2600-qbert...\n",
      "atari-games-on-atari-2600-qbert: 57 datapoints\n",
      "Added atari-games-on-atari-2600-qbert with 57 datapoints\n",
      "Saved 4 benchmarks with 20+ datapoints\n",
      "Found 4 deep reinforcement learning benchmarks with 20+ datapoints\n",
      "Scraping data for atari-games-on-atari-2600-breakout...\n",
      "Saved 58 models for atari-games-on-atari-2600-breakout\n",
      "Scraping data for atari-games-on-atari-2600-pong...\n",
      "Saved 52 models for atari-games-on-atari-2600-pong\n",
      "Scraping data for atari-games-on-atari-2600-seaquest...\n",
      "Saved 57 models for atari-games-on-atari-2600-seaquest\n",
      "Scraping data for atari-games-on-atari-2600-qbert...\n",
      "Saved 57 models for atari-games-on-atari-2600-qbert\n",
      "Collected a total of 224 models across all benchmarks\n",
      "Combined 224 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Deep reinforcement learning task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'deep_reinforcement_learning'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_deep_reinforcement_learning_benchmarks_with_20plus():\n",
    "    \"\"\"Get deep reinforcement learning benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding deep reinforcement learning benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/atari-games-on-atari-2600-breakout\",\n",
    "        \"https://paperswithcode.com/sota/atari-games-on-atari-2600-pong\",\n",
    "        \"https://paperswithcode.com/sota/atari-games-on-atari-2600-seaquest\",\n",
    "        \"https://paperswithcode.com/sota/continuous-control-on-mujoco\",\n",
    "        \"https://paperswithcode.com/sota/atari-games-on-atari-2600-qbert\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main deep reinforcement learning page\n",
    "        driver.get(\"https://paperswithcode.com/task/deep-reinforcement-learning\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and (\"reinforcement-learning\" in href.lower() or \"atari\" in href.lower() or \"mujoco\" in href.lower()):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential deep reinforcement learning benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'deep_reinforcement_learning', 'deep_reinforcement_learning_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'deep_reinforcement_learning', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'deep_reinforcement_learning', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all deep reinforcement learning models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'deep_reinforcement_learning')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'deep_reinforcement_learning', 'all_deep_reinforcement_learning_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'deep_reinforcement_learning', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting deep reinforcement learning task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_deep_reinforcement_learning_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No deep reinforcement learning benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} deep reinforcement learning benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Deep reinforcement learning task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2928d17-760b-4652-9c90-e55234e5c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting retrieval task data collection (20+ datapoints)...\n",
      "Finding retrieval benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 13 potential retrieval benchmarks\n",
      "Checking zero-shot-text-retrieval-on-ms-marco...\n",
      "zero-shot-text-retrieval-on-ms-marco: 0 datapoints\n",
      "Checking image-retrieval-on-oxford-5k...\n",
      "image-retrieval-on-oxford-5k: 0 datapoints\n",
      "Checking image-retrieval-on-paris-6k...\n",
      "image-retrieval-on-paris-6k: 0 datapoints\n",
      "Checking text-retrieval-on-natural-questions...\n",
      "text-retrieval-on-natural-questions: 1 datapoints\n",
      "Checking text-retrieval-on-ms-marco...\n",
      "text-retrieval-on-ms-marco: 1 datapoints\n",
      "Checking retrieval-on-quora-question-pairs...\n",
      "retrieval-on-quora-question-pairs: 4 datapoints\n",
      "Checking retrieval-on-hotpotqa...\n",
      "retrieval-on-hotpotqa: 3 datapoints\n",
      "Checking retrieval-on-natural-questions...\n",
      "retrieval-on-natural-questions: 3 datapoints\n",
      "Checking retrieval-on-ok-vqa...\n",
      "retrieval-on-ok-vqa: 2 datapoints\n",
      "Checking retrieval-on-mvk...\n",
      "retrieval-on-mvk: 1 datapoints\n",
      "Checking retrieval-on-infoseek...\n",
      "retrieval-on-infoseek: 1 datapoints\n",
      "Checking retrieval-on-polyvore...\n",
      "retrieval-on-polyvore: 1 datapoints\n",
      "Checking retrieval-on-toollens...\n",
      "retrieval-on-toollens: 1 datapoints\n",
      "No retrieval benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'retrieval'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_retrieval_benchmarks_with_20plus():\n",
    "    \"\"\"Get retrieval benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding retrieval benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/zero-shot-text-retrieval-on-ms-marco\",\n",
    "        \"https://paperswithcode.com/sota/image-retrieval-on-oxford-5k\",\n",
    "        \"https://paperswithcode.com/sota/image-retrieval-on-paris-6k\",\n",
    "        \"https://paperswithcode.com/sota/text-retrieval-on-natural-questions\",\n",
    "        \"https://paperswithcode.com/sota/text-retrieval-on-ms-marco\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main retrieval page\n",
    "        driver.get(\"https://paperswithcode.com/task/retrieval\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"retrieval\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential retrieval benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'retrieval', 'retrieval_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'retrieval', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'retrieval', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all retrieval models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'retrieval')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'retrieval', 'all_retrieval_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'retrieval', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting retrieval task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_retrieval_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No retrieval benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} retrieval benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Retrieval task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76de1dc-7fd1-4ea2-81df-6ef0751c741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting question answering task data collection (20+ datapoints)...\n",
      "Finding question answering benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 143 potential question answering benchmarks\n",
      "Checking question-answering-on-squad11...\n",
      "question-answering-on-squad11: 213 datapoints\n",
      "Added question-answering-on-squad11 with 213 datapoints\n",
      "Checking question-answering-on-squad20...\n",
      "question-answering-on-squad20: 286 datapoints\n",
      "Added question-answering-on-squad20 with 286 datapoints\n",
      "Checking question-answering-on-natural-questions...\n",
      "question-answering-on-natural-questions: 47 datapoints\n",
      "Added question-answering-on-natural-questions with 47 datapoints\n",
      "Checking question-answering-on-hotpotqa...\n",
      "question-answering-on-hotpotqa: 72 datapoints\n",
      "Added question-answering-on-hotpotqa with 72 datapoints\n",
      "Checking open-domain-question-answering-on-triviaqa...\n",
      "open-domain-question-answering-on-triviaqa: 1 datapoints\n",
      "Checking question-answering-on-piqa...\n",
      "question-answering-on-piqa: 67 datapoints\n",
      "Added question-answering-on-piqa with 67 datapoints\n",
      "Checking question-answering-on-boolq...\n",
      "question-answering-on-boolq: 65 datapoints\n",
      "Added question-answering-on-boolq with 65 datapoints\n",
      "Checking question-answering-on-copa...\n",
      "question-answering-on-copa: 60 datapoints\n",
      "Added question-answering-on-copa with 60 datapoints\n",
      "Checking question-answering-on-triviaqa...\n",
      "question-answering-on-triviaqa: 56 datapoints\n",
      "Added question-answering-on-triviaqa with 56 datapoints\n",
      "Checking question-answering-on-squad11-dev...\n",
      "question-answering-on-squad11-dev: 55 datapoints\n",
      "Added question-answering-on-squad11-dev with 55 datapoints\n",
      "Checking question-answering-on-openbookqa...\n",
      "question-answering-on-openbookqa: 45 datapoints\n",
      "Added question-answering-on-openbookqa with 45 datapoints\n",
      "Checking question-answering-on-webquestions...\n",
      "question-answering-on-webquestions: 37 datapoints\n",
      "Added question-answering-on-webquestions with 37 datapoints\n",
      "Checking question-answering-on-multirc...\n",
      "question-answering-on-multirc: 30 datapoints\n",
      "Added question-answering-on-multirc with 30 datapoints\n",
      "Checking question-answering-on-truthfulqa...\n",
      "question-answering-on-truthfulqa: 33 datapoints\n",
      "Added question-answering-on-truthfulqa with 33 datapoints\n",
      "Checking question-answering-on-pubmedqa...\n",
      "question-answering-on-pubmedqa: 29 datapoints\n",
      "Added question-answering-on-pubmedqa with 29 datapoints\n",
      "Checking question-answering-on-medqa-usmle...\n",
      "question-answering-on-medqa-usmle: 27 datapoints\n",
      "Added question-answering-on-medqa-usmle with 27 datapoints\n",
      "Checking question-answering-on-wikiqa...\n",
      "question-answering-on-wikiqa: 25 datapoints\n",
      "Added question-answering-on-wikiqa with 25 datapoints\n",
      "Checking question-answering-on-social-iqa...\n",
      "question-answering-on-social-iqa: 24 datapoints\n",
      "Added question-answering-on-social-iqa with 24 datapoints\n",
      "Checking question-answering-on-storycloze...\n",
      "question-answering-on-storycloze: 23 datapoints\n",
      "Added question-answering-on-storycloze with 23 datapoints\n",
      "Checking question-answering-on-quora-question-pairs...\n",
      "question-answering-on-quora-question-pairs: 19 datapoints\n",
      "Checking question-answering-on-cnn-daily-mail...\n",
      "question-answering-on-cnn-daily-mail: 16 datapoints\n",
      "Checking question-answering-on-newsqa...\n",
      "question-answering-on-newsqa: 16 datapoints\n",
      "Checking question-answering-on-drop-test...\n",
      "question-answering-on-drop-test: 16 datapoints\n",
      "Checking question-answering-on-timequestions...\n",
      "question-answering-on-timequestions: 21 datapoints\n",
      "Added question-answering-on-timequestions with 21 datapoints\n",
      "Checking question-answering-on-babi...\n",
      "question-answering-on-babi: 14 datapoints\n",
      "Checking question-answering-on-trecqa...\n",
      "question-answering-on-trecqa: 13 datapoints\n",
      "Checking question-answering-on-squad20-dev...\n",
      "question-answering-on-squad20-dev: 13 datapoints\n",
      "Checking question-answering-on-natural-questions-long...\n",
      "question-answering-on-natural-questions-long: 13 datapoints\n",
      "Checking question-answering-on-cronquestions...\n",
      "question-answering-on-cronquestions: 29 datapoints\n",
      "Added question-answering-on-cronquestions with 29 datapoints\n",
      "Checking question-answering-on-strategyqa...\n",
      "question-answering-on-strategyqa: 12 datapoints\n",
      "Checking question-answering-on-narrativeqa...\n",
      "question-answering-on-narrativeqa: 10 datapoints\n",
      "Checking question-answering-on-wikihop...\n",
      "question-answering-on-wikihop: 9 datapoints\n",
      "Checking question-answering-on-coqa...\n",
      "question-answering-on-coqa: 9 datapoints\n",
      "Checking question-answering-on-obqa...\n",
      "question-answering-on-obqa: 9 datapoints\n",
      "Checking question-answering-on-bamboogle...\n",
      "question-answering-on-bamboogle: 9 datapoints\n",
      "Checking question-answering-on-multitq...\n",
      "question-answering-on-multitq: 11 datapoints\n",
      "Checking question-answering-on-tiq...\n",
      "question-answering-on-tiq: 9 datapoints\n",
      "Checking question-answering-on-childrens-book-test...\n",
      "question-answering-on-childrens-book-test: 8 datapoints\n",
      "Checking question-answering-on-fever...\n",
      "question-answering-on-fever: 8 datapoints\n",
      "Checking question-answering-on-race...\n",
      "question-answering-on-race: 7 datapoints\n",
      "Checking question-answering-on-qasent...\n",
      "question-answering-on-qasent: 7 datapoints\n",
      "Checking question-answering-on-yahoocqa...\n",
      "question-answering-on-yahoocqa: 7 datapoints\n",
      "Checking question-answering-on-quasart-t...\n",
      "question-answering-on-quasart-t: 7 datapoints\n",
      "Checking question-answering-on-bioasq...\n",
      "question-answering-on-bioasq: 7 datapoints\n",
      "Checking question-answering-on-sqa3d...\n",
      "question-answering-on-sqa3d: 7 datapoints\n",
      "Checking question-answering-on-kilt-eli5...\n",
      "question-answering-on-kilt-eli5: 7 datapoints\n",
      "Checking question-answering-on-story-cloze...\n",
      "question-answering-on-story-cloze: 7 datapoints\n",
      "Checking question-answering-on-fquad-1...\n",
      "question-answering-on-fquad-1: 7 datapoints\n",
      "Checking question-answering-on-nq-beir...\n",
      "question-answering-on-nq-beir: 6 datapoints\n",
      "Checking question-answering-on-danetqa...\n",
      "question-answering-on-danetqa: 22 datapoints\n",
      "Added question-answering-on-danetqa with 22 datapoints\n",
      "Checking question-answering-on-finqa...\n",
      "question-answering-on-finqa: 6 datapoints\n",
      "Checking question-answering-on-friendsqa...\n",
      "question-answering-on-friendsqa: 6 datapoints\n",
      "Checking question-answering-on-drop...\n",
      "question-answering-on-drop: 6 datapoints\n",
      "Checking question-answering-on-next-qa-open-ended...\n",
      "question-answering-on-next-qa-open-ended: 6 datapoints\n",
      "Checking question-answering-on-tempquestions...\n",
      "question-answering-on-tempquestions: 8 datapoints\n",
      "Checking question-answering-on-semevalcqa...\n",
      "question-answering-on-semevalcqa: 5 datapoints\n",
      "Checking question-answering-on-peerqa...\n",
      "question-answering-on-peerqa: 6 datapoints\n",
      "Checking question-answering-on-ms-marco...\n",
      "question-answering-on-ms-marco: 4 datapoints\n",
      "Checking question-answering-on-ai2-kaggle-dataset...\n",
      "question-answering-on-ai2-kaggle-dataset: 4 datapoints\n",
      "Checking question-answering-on-naturalqa...\n",
      "question-answering-on-naturalqa: 4 datapoints\n",
      "Checking question-answering-on-hotpotqa-beir...\n",
      "question-answering-on-hotpotqa-beir: 4 datapoints\n",
      "Checking question-answering-on-fiqa-2018-beir...\n",
      "question-answering-on-fiqa-2018-beir: 4 datapoints\n",
      "Checking question-answering-on-catbabi-qa-mode...\n",
      "question-answering-on-catbabi-qa-mode: 4 datapoints\n",
      "Checking question-answering-on-catbabi-lm-mode...\n",
      "question-answering-on-catbabi-lm-mode: 4 datapoints\n",
      "Checking question-answering-on-molweni...\n",
      "question-answering-on-molweni: 4 datapoints\n",
      "Checking question-answering-on-blurb...\n",
      "question-answering-on-blurb: 4 datapoints\n",
      "Checking question-answering-on-fairytaleqa...\n",
      "question-answering-on-fairytaleqa: 4 datapoints\n",
      "Checking question-answering-on-hybridqa...\n",
      "question-answering-on-hybridqa: 4 datapoints\n",
      "Checking question-answering-on-ruopenbookqa...\n",
      "question-answering-on-ruopenbookqa: 4 datapoints\n",
      "Checking question-answering-on-multiq...\n",
      "question-answering-on-multiq: 4 datapoints\n",
      "Checking question-answering-on-chegeka...\n",
      "question-answering-on-chegeka: 4 datapoints\n",
      "Checking question-answering-on-egotaskqa...\n",
      "question-answering-on-egotaskqa: 4 datapoints\n",
      "Checking question-answering-on-quality...\n",
      "question-answering-on-quality: 4 datapoints\n",
      "Checking question-answering-on-complex-cronquestions...\n",
      "question-answering-on-complex-cronquestions: 4 datapoints\n",
      "Checking question-answering-on-reclor...\n",
      "question-answering-on-reclor: 3 datapoints\n",
      "Checking question-answering-on-casehold...\n",
      "question-answering-on-casehold: 3 datapoints\n",
      "Checking question-answering-on-mathematics-dataset...\n",
      "question-answering-on-mathematics-dataset: 3 datapoints\n",
      "Checking question-answering-on-tweetqa...\n",
      "question-answering-on-tweetqa: 3 datapoints\n",
      "Checking question-answering-on-sberquad...\n",
      "question-answering-on-sberquad: 3 datapoints\n",
      "Checking question-answering-on-conditionalqa...\n",
      "question-answering-on-conditionalqa: 3 datapoints\n",
      "Checking question-answering-on-ott-qa...\n",
      "question-answering-on-ott-qa: 3 datapoints\n",
      "Checking question-answering-on-convfinqa...\n",
      "question-answering-on-convfinqa: 3 datapoints\n",
      "Checking question-answering-on-vnhsge-english...\n",
      "question-answering-on-vnhsge-english: 3 datapoints\n",
      "Checking question-answering-on-duorc...\n",
      "question-answering-on-duorc: 3 datapoints\n",
      "Checking question-answering-on-clicr...\n",
      "question-answering-on-clicr: 2 datapoints\n",
      "Checking question-answering-on-quac...\n",
      "question-answering-on-quac: 2 datapoints\n",
      "Checking question-answering-on-reverb...\n",
      "question-answering-on-reverb: 2 datapoints\n",
      "Checking question-answering-on-mctest-500...\n",
      "question-answering-on-mctest-500: 2 datapoints\n",
      "Checking question-answering-on-complexquestions...\n",
      "question-answering-on-complexquestions: 2 datapoints\n",
      "Checking question-answering-on-codah...\n",
      "question-answering-on-codah: 2 datapoints\n",
      "Checking question-answering-on-squad-1...\n",
      "question-answering-on-squad-1: 4 datapoints\n",
      "Checking question-answering-on-muld-narrativeqa...\n",
      "question-answering-on-muld-narrativeqa: 2 datapoints\n",
      "Checking question-answering-on-muld-hotpotqa...\n",
      "question-answering-on-muld-hotpotqa: 2 datapoints\n",
      "Checking question-answering-on-mrqa-2019...\n",
      "question-answering-on-mrqa-2019: 2 datapoints\n",
      "Checking question-answering-on-torque...\n",
      "question-answering-on-torque: 2 datapoints\n",
      "Checking question-answering-on-aristo-kaggle-allen-ai...\n",
      "question-answering-on-aristo-kaggle-allen-ai: 2 datapoints\n",
      "Checking question-answering-on-vnhsge-physics...\n",
      "question-answering-on-vnhsge-physics: 2 datapoints\n",
      "Checking question-answering-on-vnhsge-chemistry...\n",
      "question-answering-on-vnhsge-chemistry: 2 datapoints\n",
      "Checking question-answering-on-vnhsge-biology...\n",
      "question-answering-on-vnhsge-biology: 2 datapoints\n",
      "Checking question-answering-on-vnhsge-history...\n",
      "question-answering-on-vnhsge-history: 2 datapoints\n",
      "Checking question-answering-on-vnhsge-geography...\n",
      "question-answering-on-vnhsge-geography: 2 datapoints\n",
      "Checking question-answering-on-vnhsge-literature...\n",
      "question-answering-on-vnhsge-literature: 2 datapoints\n",
      "Checking question-answering-on-vnhsge-mathematics-1...\n",
      "question-answering-on-vnhsge-mathematics-1: 2 datapoints\n",
      "Checking question-answering-on-vnhsge-civic...\n",
      "question-answering-on-vnhsge-civic: 2 datapoints\n",
      "Checking question-answering-on-uniprotqa...\n",
      "question-answering-on-uniprotqa: 2 datapoints\n",
      "Checking question-answering-on-pubchemqa...\n",
      "question-answering-on-pubchemqa: 2 datapoints\n",
      "Checking question-answering-on-agi-eval...\n",
      "question-answering-on-agi-eval: 2 datapoints\n",
      "Checking question-answering-on-geoquestions1089...\n",
      "question-answering-on-geoquestions1089: 2 datapoints\n",
      "Checking question-answering-on-popqa...\n",
      "question-answering-on-popqa: 2 datapoints\n",
      "Checking question-answering-on-wikitablequestions...\n",
      "question-answering-on-wikitablequestions: 2 datapoints\n",
      "Checking question-answering-on-medturkquad-medical...\n",
      "question-answering-on-medturkquad-medical: 2 datapoints\n",
      "Checking question-answering-on-wikisql...\n",
      "question-answering-on-wikisql: 2 datapoints\n",
      "Checking question-answering-on-mapeval-api-1...\n",
      "question-answering-on-mapeval-api-1: 2 datapoints\n",
      "Checking question-answering-on-recipeqa...\n",
      "question-answering-on-recipeqa: 1 datapoints\n",
      "Checking question-answering-on-simplequestions...\n",
      "question-answering-on-simplequestions: 1 datapoints\n",
      "Checking question-answering-on-mctest-160...\n",
      "question-answering-on-mctest-160: 1 datapoints\n",
      "Checking product-question-answering-on-jd-product...\n",
      "product-question-answering-on-jd-product: 1 datapoints\n",
      "Checking question-answering-on-swag...\n",
      "question-answering-on-swag: 1 datapoints\n",
      "Checking question-answering-on-scde-1...\n",
      "question-answering-on-scde-1: 3 datapoints\n",
      "Checking question-answering-on-efficientqa-dev...\n",
      "question-answering-on-efficientqa-dev: 1 datapoints\n",
      "Checking question-answering-on-efficientqa-test...\n",
      "question-answering-on-efficientqa-test: 1 datapoints\n",
      "Checking question-answering-on-complexwebquestions...\n",
      "question-answering-on-complexwebquestions: 1 datapoints\n",
      "Checking question-answering-on-qasper...\n",
      "question-answering-on-qasper: 1 datapoints\n",
      "Checking question-answering-on-jaquad...\n",
      "question-answering-on-jaquad: 1 datapoints\n",
      "Checking question-answering-on-stepgame...\n",
      "question-answering-on-stepgame: 1 datapoints\n",
      "Checking question-answering-on-chaii-hindi-and-tamil...\n",
      "question-answering-on-chaii-hindi-and-tamil: 1 datapoints\n",
      "Checking question-answering-on-mrqa-out-of-domain...\n",
      "question-answering-on-mrqa-out-of-domain: 1 datapoints\n",
      "Checking question-answering-on-tat-qa...\n",
      "question-answering-on-tat-qa: 1 datapoints\n",
      "Checking question-answering-on-mmlu...\n",
      "question-answering-on-mmlu: 1 datapoints\n",
      "Checking question-answering-on-medmcqa-dev...\n",
      "question-answering-on-medmcqa-dev: 1 datapoints\n",
      "Checking question-answering-on-aviationqa...\n",
      "question-answering-on-aviationqa: 1 datapoints\n",
      "Checking question-answering-on-metaqa...\n",
      "question-answering-on-metaqa: 1 datapoints\n",
      "Checking question-answering-on-kqa-pro...\n",
      "question-answering-on-kqa-pro: 1 datapoints\n",
      "Checking question-answering-on-webquestionssp...\n",
      "question-answering-on-webquestionssp: 1 datapoints\n",
      "Checking question-answering-on-graphquestions...\n",
      "question-answering-on-graphquestions: 1 datapoints\n",
      "Checking question-answering-on-multispanqa...\n",
      "question-answering-on-multispanqa: 1 datapoints\n",
      "Checking question-answering-on-coco-visual-question...\n",
      "question-answering-on-coco-visual-question: 1 datapoints\n",
      "Checking question-answering-on-schizzosquad...\n",
      "question-answering-on-schizzosquad: 1 datapoints\n",
      "Checking question-answering-on-websrc...\n",
      "question-answering-on-websrc: 1 datapoints\n",
      "Checking question-answering-on-bbh...\n",
      "question-answering-on-bbh: 1 datapoints\n",
      "Checking question-answering-on-hellaswag...\n",
      "question-answering-on-hellaswag: 1 datapoints\n",
      "Checking question-answering-on-mapeval-textual...\n",
      "question-answering-on-mapeval-textual: 1 datapoints\n",
      "Checking question-answering-on-tempqa-wd...\n",
      "question-answering-on-tempqa-wd: 2 datapoints\n",
      "Saved 21 benchmarks with 20+ datapoints\n",
      "Found 21 question answering benchmarks with 20+ datapoints\n",
      "Scraping data for question-answering-on-squad11...\n",
      "Saved 212 models for question-answering-on-squad11\n",
      "Scraping data for question-answering-on-squad20...\n",
      "Saved 285 models for question-answering-on-squad20\n",
      "Scraping data for question-answering-on-natural-questions...\n",
      "Saved 47 models for question-answering-on-natural-questions\n",
      "Scraping data for question-answering-on-hotpotqa...\n",
      "Saved 71 models for question-answering-on-hotpotqa\n",
      "Scraping data for question-answering-on-piqa...\n",
      "Saved 67 models for question-answering-on-piqa\n",
      "Scraping data for question-answering-on-boolq...\n",
      "Saved 65 models for question-answering-on-boolq\n",
      "Scraping data for question-answering-on-copa...\n",
      "Saved 60 models for question-answering-on-copa\n",
      "Scraping data for question-answering-on-triviaqa...\n",
      "Saved 56 models for question-answering-on-triviaqa\n",
      "Scraping data for question-answering-on-squad11-dev...\n",
      "Saved 55 models for question-answering-on-squad11-dev\n",
      "Scraping data for question-answering-on-openbookqa...\n",
      "Saved 45 models for question-answering-on-openbookqa\n",
      "Scraping data for question-answering-on-webquestions...\n",
      "Saved 37 models for question-answering-on-webquestions\n",
      "Scraping data for question-answering-on-multirc...\n",
      "Saved 30 models for question-answering-on-multirc\n",
      "Scraping data for question-answering-on-truthfulqa...\n",
      "Saved 30 models for question-answering-on-truthfulqa\n",
      "Scraping data for question-answering-on-pubmedqa...\n",
      "Saved 29 models for question-answering-on-pubmedqa\n",
      "Scraping data for question-answering-on-medqa-usmle...\n",
      "Saved 27 models for question-answering-on-medqa-usmle\n",
      "Scraping data for question-answering-on-wikiqa...\n",
      "Saved 25 models for question-answering-on-wikiqa\n",
      "Scraping data for question-answering-on-social-iqa...\n",
      "Saved 24 models for question-answering-on-social-iqa\n",
      "Scraping data for question-answering-on-storycloze...\n",
      "Saved 23 models for question-answering-on-storycloze\n",
      "Scraping data for question-answering-on-timequestions...\n",
      "Saved 21 models for question-answering-on-timequestions\n",
      "Scraping data for question-answering-on-cronquestions...\n",
      "Saved 29 models for question-answering-on-cronquestions\n",
      "Scraping data for question-answering-on-danetqa...\n",
      "Saved 22 models for question-answering-on-danetqa\n",
      "Collected a total of 1260 models across all benchmarks\n",
      "Combined 1260 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Question answering task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'question_answering'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_question_answering_benchmarks_with_20plus():\n",
    "    \"\"\"Get question answering benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding question answering benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/question-answering-on-squad11\",\n",
    "        \"https://paperswithcode.com/sota/question-answering-on-squad20\",\n",
    "        \"https://paperswithcode.com/sota/question-answering-on-natural-questions\",\n",
    "        \"https://paperswithcode.com/sota/question-answering-on-hotpotqa\",\n",
    "        \"https://paperswithcode.com/sota/open-domain-question-answering-on-triviaqa\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main question answering page\n",
    "        driver.get(\"https://paperswithcode.com/task/question-answering\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"question-answering\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential question answering benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'question_answering', 'question_answering_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'question_answering', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'question_answering', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all question answering models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'question_answering')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'question_answering', 'all_question_answering_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'question_answering', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting question answering task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_question_answering_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No question answering benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} question answering benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Question answering task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af60936-e6f3-4d19-8d7a-1f817a0b4646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting knowledge graphs task data collection (20+ datapoints)...\n",
      "Finding knowledge graphs benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 9 potential knowledge graphs benchmarks\n",
      "Checking link-prediction-on-fb15k-237...\n",
      "link-prediction-on-fb15k-237: 74 datapoints\n",
      "Added link-prediction-on-fb15k-237 with 74 datapoints\n",
      "Checking link-prediction-on-wn18rr...\n",
      "link-prediction-on-wn18rr: 74 datapoints\n",
      "Added link-prediction-on-wn18rr with 74 datapoints\n",
      "Checking knowledge-graph-completion-on-fb15k...\n",
      "knowledge-graph-completion-on-fb15k: 0 datapoints\n",
      "Checking knowledge-graph-completion-on-wn18...\n",
      "knowledge-graph-completion-on-wn18: 0 datapoints\n",
      "Checking link-prediction-on-nell-995...\n",
      "link-prediction-on-nell-995: 4 datapoints\n",
      "Checking knowledge-graphs-on-mars-multimodal...\n",
      "knowledge-graphs-on-mars-multimodal: 8 datapoints\n",
      "Checking knowledge-graphs-on-jerichoworld...\n",
      "knowledge-graphs-on-jerichoworld: 5 datapoints\n",
      "Checking knowledge-graphs-on-wikikg90m-lsc...\n",
      "knowledge-graphs-on-wikikg90m-lsc: 4 datapoints\n",
      "Checking knowledge-graphs-on-fb15k...\n",
      "knowledge-graphs-on-fb15k: 2 datapoints\n",
      "Saved 2 benchmarks with 20+ datapoints\n",
      "Found 2 knowledge graphs benchmarks with 20+ datapoints\n",
      "Scraping data for link-prediction-on-fb15k-237...\n",
      "Saved 74 models for link-prediction-on-fb15k-237\n",
      "Scraping data for link-prediction-on-wn18rr...\n",
      "Saved 74 models for link-prediction-on-wn18rr\n",
      "Collected a total of 148 models across all benchmarks\n",
      "Combined 148 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Knowledge graphs task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'knowledge_graphs'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_knowledge_graphs_benchmarks_with_20plus():\n",
    "    \"\"\"Get knowledge graphs benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding knowledge graphs benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/link-prediction-on-fb15k-237\",\n",
    "        \"https://paperswithcode.com/sota/link-prediction-on-wn18rr\",\n",
    "        \"https://paperswithcode.com/sota/knowledge-graph-completion-on-fb15k\",\n",
    "        \"https://paperswithcode.com/sota/knowledge-graph-completion-on-wn18\",\n",
    "        \"https://paperswithcode.com/sota/link-prediction-on-nell-995\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main knowledge graphs page\n",
    "        driver.get(\"https://paperswithcode.com/task/knowledge-graphs\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and (\"knowledge-graph\" in href.lower() or \"link-prediction\" in href.lower()):\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential knowledge graphs benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'knowledge_graphs', 'knowledge_graphs_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'knowledge_graphs', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'knowledge_graphs', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all knowledge graphs models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'knowledge_graphs')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'knowledge_graphs', 'all_knowledge_graphs_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'knowledge_graphs', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting knowledge graphs task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_knowledge_graphs_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No knowledge graphs benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} knowledge graphs benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Knowledge graphs task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af68ac2-569b-4070-9462-780686ea17de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting machine translation task data collection (20+ datapoints)...\n",
      "Finding machine translation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 84 potential machine translation benchmarks\n",
      "Checking machine-translation-on-wmt2014-english-german...\n",
      "machine-translation-on-wmt2014-english-german: 91 datapoints\n",
      "Added machine-translation-on-wmt2014-english-german with 91 datapoints\n",
      "Checking machine-translation-on-wmt2014-english-french...\n",
      "machine-translation-on-wmt2014-english-french: 57 datapoints\n",
      "Added machine-translation-on-wmt2014-english-french with 57 datapoints\n",
      "Checking machine-translation-on-wmt2016-english-german...\n",
      "machine-translation-on-wmt2016-english-german: 12 datapoints\n",
      "Checking machine-translation-on-wmt2016-english-romanian...\n",
      "machine-translation-on-wmt2016-english-romanian: 0 datapoints\n",
      "Checking machine-translation-on-wmt2019-english-german...\n",
      "machine-translation-on-wmt2019-english-german: 2 datapoints\n",
      "Checking machine-translation-on-iwslt2014-german...\n",
      "machine-translation-on-iwslt2014-german: 34 datapoints\n",
      "Added machine-translation-on-iwslt2014-german with 34 datapoints\n",
      "Checking machine-translation-on-wmt2016-english-1...\n",
      "machine-translation-on-wmt2016-english-1: 21 datapoints\n",
      "Added machine-translation-on-wmt2016-english-1 with 21 datapoints\n",
      "Checking machine-translation-on-wmt2016-romanian...\n",
      "machine-translation-on-wmt2016-romanian: 21 datapoints\n",
      "Added machine-translation-on-wmt2016-romanian with 21 datapoints\n",
      "Checking machine-translation-on-aces...\n",
      "machine-translation-on-aces: 21 datapoints\n",
      "Added machine-translation-on-aces with 21 datapoints\n",
      "Checking machine-translation-on-wmt2014-german-english...\n",
      "machine-translation-on-wmt2014-german-english: 16 datapoints\n",
      "Checking machine-translation-on-iwslt2015-german...\n",
      "machine-translation-on-iwslt2015-german: 15 datapoints\n",
      "Checking machine-translation-on-iwslt2015-english-1...\n",
      "machine-translation-on-iwslt2015-english-1: 11 datapoints\n",
      "Checking machine-translation-on-iwslt2015-english...\n",
      "machine-translation-on-iwslt2015-english: 8 datapoints\n",
      "Checking machine-translation-on-wmt2016-german-english...\n",
      "machine-translation-on-wmt2016-german-english: 8 datapoints\n",
      "Checking machine-translation-on-iwslt2014-english...\n",
      "machine-translation-on-iwslt2014-english: 6 datapoints\n",
      "Checking machine-translation-on-wmt2015-english-german...\n",
      "machine-translation-on-wmt2015-english-german: 6 datapoints\n",
      "Checking machine-translation-on-flores-200...\n",
      "machine-translation-on-flores-200: 5 datapoints\n",
      "Checking machine-translation-on-wmt2016-english...\n",
      "machine-translation-on-wmt2016-english: 4 datapoints\n",
      "Checking machine-translation-on-wmt-2017-latvian...\n",
      "machine-translation-on-wmt-2017-latvian: 4 datapoints\n",
      "Checking machine-translation-on-wmt2014-french-english...\n",
      "machine-translation-on-wmt2014-french-english: 3 datapoints\n",
      "Checking machine-translation-on-wmt2017-chinese...\n",
      "machine-translation-on-wmt2017-chinese: 3 datapoints\n",
      "Checking machine-translation-on-wmt-2017-english-1...\n",
      "machine-translation-on-wmt-2017-english-1: 3 datapoints\n",
      "Checking machine-translation-on-frmt-portuguese...\n",
      "machine-translation-on-frmt-portuguese: 3 datapoints\n",
      "Checking machine-translation-on-frmt-chinese-mainland...\n",
      "machine-translation-on-frmt-chinese-mainland: 3 datapoints\n",
      "Checking machine-translation-on-frmt-chinese-taiwan...\n",
      "machine-translation-on-frmt-chinese-taiwan: 3 datapoints\n",
      "Checking machine-translation-on-frmt-portuguese-brazil...\n",
      "machine-translation-on-frmt-portuguese-brazil: 3 datapoints\n",
      "Checking machine-translation-on-flores95-devtest-x-eng...\n",
      "machine-translation-on-flores95-devtest-x-eng: 3 datapoints\n",
      "Checking machine-translation-on-flores95-devtest-eng-x...\n",
      "machine-translation-on-flores95-devtest-eng-x: 3 datapoints\n",
      "Checking machine-translation-on-wmt-2018-finnish...\n",
      "machine-translation-on-wmt-2018-finnish: 2 datapoints\n",
      "Checking machine-translation-on-20news...\n",
      "machine-translation-on-20news: 2 datapoints\n",
      "Checking machine-translation-on-wmt2014-english-czech...\n",
      "machine-translation-on-wmt2014-english-czech: 2 datapoints\n",
      "Checking machine-translation-on-iwslt2017-english...\n",
      "machine-translation-on-iwslt2017-english: 2 datapoints\n",
      "Checking machine-translation-on-iwslt2017-english-1...\n",
      "machine-translation-on-iwslt2017-english-1: 2 datapoints\n",
      "Checking machine-translation-on-iwslt2017-arabic...\n",
      "machine-translation-on-iwslt2017-arabic: 2 datapoints\n",
      "Checking machine-translation-on-iwslt2017-french...\n",
      "machine-translation-on-iwslt2017-french: 2 datapoints\n",
      "Checking machine-translation-on-itihasa...\n",
      "machine-translation-on-itihasa: 2 datapoints\n",
      "Checking machine-translation-on-arba-sicula...\n",
      "machine-translation-on-arba-sicula: 2 datapoints\n",
      "Checking machine-translation-on-iwslt2017-german...\n",
      "machine-translation-on-iwslt2017-german: 2 datapoints\n",
      "Checking machine-translation-on-iwslt2015-vietnamese...\n",
      "machine-translation-on-iwslt2015-vietnamese: 2 datapoints\n",
      "Checking machine-translation-on-wmt2017-turkish...\n",
      "machine-translation-on-wmt2017-turkish: 2 datapoints\n",
      "Checking machine-translation-on-wmt2015-english...\n",
      "machine-translation-on-wmt2015-english: 1 datapoints\n",
      "Checking machine-translation-on-iwslt2015-thai-english...\n",
      "machine-translation-on-iwslt2015-thai-english: 1 datapoints\n",
      "Checking machine-translation-on-wmt2016-russian...\n",
      "machine-translation-on-wmt2016-russian: 1 datapoints\n",
      "Checking machine-translation-on-wmt2016-english-czech...\n",
      "machine-translation-on-wmt2016-english-czech: 1 datapoints\n",
      "Checking machine-translation-on-wmt2016-czech-english...\n",
      "machine-translation-on-wmt2016-czech-english: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2018-english...\n",
      "machine-translation-on-wmt-2018-english: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2018-english-1...\n",
      "machine-translation-on-wmt-2018-english-1: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2018-estonian...\n",
      "machine-translation-on-wmt-2018-estonian: 1 datapoints\n",
      "Checking machine-translation-on-accurat-balanced-test-1...\n",
      "machine-translation-on-accurat-balanced-test-1: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2017-english...\n",
      "machine-translation-on-wmt-2017-english: 1 datapoints\n",
      "Checking machine-translation-on-accurat-balanced-test-2...\n",
      "machine-translation-on-accurat-balanced-test-2: 1 datapoints\n",
      "Checking machine-translation-on-wmt2019-finnish...\n",
      "machine-translation-on-wmt2019-finnish: 1 datapoints\n",
      "Checking machine-translation-on-wmt2017-finnish...\n",
      "machine-translation-on-wmt2017-finnish: 1 datapoints\n",
      "Checking machine-translation-on-wmt2016-finnish...\n",
      "machine-translation-on-wmt2016-finnish: 1 datapoints\n",
      "Checking machine-translation-on-iwslt2015-chinese...\n",
      "machine-translation-on-iwslt2015-chinese: 1 datapoints\n",
      "Checking machine-translation-on-wmt2019-german-english...\n",
      "machine-translation-on-wmt2019-german-english: 1 datapoints\n",
      "Checking machine-translation-on-wmt2016-english-french...\n",
      "machine-translation-on-wmt2016-english-french: 1 datapoints\n",
      "Checking machine-translation-on-business-scene...\n",
      "machine-translation-on-business-scene: 1 datapoints\n",
      "Checking machine-translation-on-business-scene-1...\n",
      "machine-translation-on-business-scene-1: 1 datapoints\n",
      "Checking machine-translation-on-v-a-trained-on-t-h...\n",
      "machine-translation-on-v-a-trained-on-t-h: 1 datapoints\n",
      "Checking machine-translation-on-v-b-trained-on-t-h...\n",
      "machine-translation-on-v-b-trained-on-t-h: 1 datapoints\n",
      "Checking machine-translation-on-v-c-trained-on-t-h...\n",
      "machine-translation-on-v-c-trained-on-t-h: 1 datapoints\n",
      "Checking machine-translation-on-wmt2017-english-german...\n",
      "machine-translation-on-wmt2017-english-german: 1 datapoints\n",
      "Checking machine-translation-on-wmt2017-english...\n",
      "machine-translation-on-wmt2017-english: 1 datapoints\n",
      "Checking machine-translation-on-wmt2017-english-french...\n",
      "machine-translation-on-wmt2017-english-french: 1 datapoints\n",
      "Checking machine-translation-on-wmt2017-russian...\n",
      "machine-translation-on-wmt2017-russian: 1 datapoints\n",
      "Checking machine-translation-on-wmt2019-english-1...\n",
      "machine-translation-on-wmt2019-english-1: 1 datapoints\n",
      "Checking machine-translation-on-tatoeba-en-to-el...\n",
      "machine-translation-on-tatoeba-en-to-el: 1 datapoints\n",
      "Checking machine-translation-on-tatoeba-el-to-en...\n",
      "machine-translation-on-tatoeba-el-to-en: 1 datapoints\n",
      "Checking machine-translation-on-slone-myv-ru-2022-ru...\n",
      "machine-translation-on-slone-myv-ru-2022-ru: 1 datapoints\n",
      "Checking machine-translation-on-slone-myv-ru-2022-myv...\n",
      "machine-translation-on-slone-myv-ru-2022-myv: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-english-czech...\n",
      "machine-translation-on-wmt-2022-english-czech: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-english...\n",
      "machine-translation-on-wmt-2022-english: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-english-1...\n",
      "machine-translation-on-wmt-2022-english-1: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-english-2...\n",
      "machine-translation-on-wmt-2022-english-2: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-english-3...\n",
      "machine-translation-on-wmt-2022-english-3: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-czech-english...\n",
      "machine-translation-on-wmt-2022-czech-english: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-german...\n",
      "machine-translation-on-wmt-2022-german: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-japanese...\n",
      "machine-translation-on-wmt-2022-japanese: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-russian...\n",
      "machine-translation-on-wmt-2022-russian: 1 datapoints\n",
      "Checking machine-translation-on-wmt-2022-chinese...\n",
      "machine-translation-on-wmt-2022-chinese: 1 datapoints\n",
      "Checking machine-translation-on-alexa-point-of-view...\n",
      "machine-translation-on-alexa-point-of-view: 1 datapoints\n",
      "Checking machine-translation-on-iwslt-2017...\n",
      "machine-translation-on-iwslt-2017: 1 datapoints\n",
      "Checking machine-translation-on-multi-lingual-bug...\n",
      "machine-translation-on-multi-lingual-bug: 1 datapoints\n",
      "Saved 6 benchmarks with 20+ datapoints\n",
      "Found 6 machine translation benchmarks with 20+ datapoints\n",
      "Scraping data for machine-translation-on-wmt2014-english-german...\n",
      "Saved 90 models for machine-translation-on-wmt2014-english-german\n",
      "Scraping data for machine-translation-on-wmt2014-english-french...\n",
      "Saved 56 models for machine-translation-on-wmt2014-english-french\n",
      "Scraping data for machine-translation-on-iwslt2014-german...\n",
      "Saved 34 models for machine-translation-on-iwslt2014-german\n",
      "Scraping data for machine-translation-on-wmt2016-english-1...\n",
      "Saved 21 models for machine-translation-on-wmt2016-english-1\n",
      "Scraping data for machine-translation-on-wmt2016-romanian...\n",
      "Saved 21 models for machine-translation-on-wmt2016-romanian\n",
      "Scraping data for machine-translation-on-aces...\n",
      "Saved 21 models for machine-translation-on-aces\n",
      "Collected a total of 243 models across all benchmarks\n",
      "Combined 243 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Machine translation task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'machine_translation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_machine_translation_benchmarks_with_20plus():\n",
    "    \"\"\"Get machine translation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding machine translation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german\",\n",
    "        \"https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-french\",\n",
    "        \"https://paperswithcode.com/sota/machine-translation-on-wmt2016-english-german\",\n",
    "        \"https://paperswithcode.com/sota/machine-translation-on-wmt2016-english-romanian\",\n",
    "        \"https://paperswithcode.com/sota/machine-translation-on-wmt2019-english-german\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main machine translation page\n",
    "        driver.get(\"https://paperswithcode.com/task/machine-translation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"machine-translation\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential machine translation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'machine_translation', 'machine_translation_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'machine_translation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'machine_translation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all machine translation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'machine_translation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'machine_translation', 'all_machine_translation_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'machine_translation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting machine translation task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_machine_translation_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No machine translation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} machine translation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Machine translation task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e439b983-43b3-49b0-a694-2056f21f446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting image segmentation task data collection (20+ datapoints)...\n",
      "Finding image segmentation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 17 potential image segmentation benchmarks\n",
      "Checking semantic-segmentation-on-cityscapes...\n",
      "semantic-segmentation-on-cityscapes: 105 datapoints\n",
      "Added semantic-segmentation-on-cityscapes with 105 datapoints\n",
      "Checking semantic-segmentation-on-pascal-voc-2012...\n",
      "semantic-segmentation-on-pascal-voc-2012: 51 datapoints\n",
      "Added semantic-segmentation-on-pascal-voc-2012 with 51 datapoints\n",
      "Checking semantic-segmentation-on-ade20k...\n",
      "semantic-segmentation-on-ade20k: 231 datapoints\n",
      "Added semantic-segmentation-on-ade20k with 231 datapoints\n",
      "Checking instance-segmentation-on-coco...\n",
      "instance-segmentation-on-coco: 112 datapoints\n",
      "Added instance-segmentation-on-coco with 112 datapoints\n",
      "Checking panoptic-segmentation-on-coco-test-dev...\n",
      "panoptic-segmentation-on-coco-test-dev: 38 datapoints\n",
      "Added panoptic-segmentation-on-coco-test-dev with 38 datapoints\n",
      "Checking image-segmentation-on-msd-mirror-segmentation...\n",
      "image-segmentation-on-msd-mirror-segmentation: 5 datapoints\n",
      "Checking image-segmentation-on-pmd...\n",
      "image-segmentation-on-pmd: 5 datapoints\n",
      "Checking image-segmentation-on-pascal-panoptic-parts...\n",
      "image-segmentation-on-pascal-panoptic-parts: 4 datapoints\n",
      "Checking image-segmentation-on-mas3k...\n",
      "image-segmentation-on-mas3k: 4 datapoints\n",
      "Checking image-segmentation-on-rmas...\n",
      "image-segmentation-on-rmas: 4 datapoints\n",
      "Checking image-segmentation-on-pascal-voc...\n",
      "image-segmentation-on-pascal-voc: 3 datapoints\n",
      "Checking image-segmentation-on-marida...\n",
      "image-segmentation-on-marida: 2 datapoints\n",
      "Checking image-segmentation-on-hutu-80...\n",
      "image-segmentation-on-hutu-80: 2 datapoints\n",
      "Checking image-segmentation-on-evd4uav...\n",
      "image-segmentation-on-evd4uav: 1 datapoints\n",
      "Checking image-segmentation-on-coco-val2017...\n",
      "image-segmentation-on-coco-val2017: 1 datapoints\n",
      "Checking image-segmentation-on-msd-heart...\n",
      "image-segmentation-on-msd-heart: 1 datapoints\n",
      "Checking image-segmentation-on-oxfordpets...\n",
      "image-segmentation-on-oxfordpets: 1 datapoints\n",
      "Saved 5 benchmarks with 20+ datapoints\n",
      "Found 5 image segmentation benchmarks with 20+ datapoints\n",
      "Scraping data for semantic-segmentation-on-cityscapes...\n",
      "Saved 105 models for semantic-segmentation-on-cityscapes\n",
      "Scraping data for semantic-segmentation-on-pascal-voc-2012...\n",
      "Saved 51 models for semantic-segmentation-on-pascal-voc-2012\n",
      "Scraping data for semantic-segmentation-on-ade20k...\n",
      "Saved 231 models for semantic-segmentation-on-ade20k\n",
      "Scraping data for instance-segmentation-on-coco...\n",
      "Saved 112 models for instance-segmentation-on-coco\n",
      "Scraping data for panoptic-segmentation-on-coco-test-dev...\n",
      "Saved 38 models for panoptic-segmentation-on-coco-test-dev\n",
      "Collected a total of 537 models across all benchmarks\n",
      "Combined 537 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Image segmentation task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'image_segmentation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_image_segmentation_benchmarks_with_20plus():\n",
    "    \"\"\"Get image segmentation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding image segmentation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes\",\n",
    "        \"https://paperswithcode.com/sota/semantic-segmentation-on-pascal-voc-2012\",\n",
    "        \"https://paperswithcode.com/sota/semantic-segmentation-on-ade20k\",\n",
    "        \"https://paperswithcode.com/sota/instance-segmentation-on-coco\",\n",
    "        \"https://paperswithcode.com/sota/panoptic-segmentation-on-coco-test-dev\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main image segmentation page\n",
    "        driver.get(\"https://paperswithcode.com/task/image-segmentation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"segmentation\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential image segmentation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'image_segmentation', 'image_segmentation_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'image_segmentation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'image_segmentation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all image segmentation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'image_segmentation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'image_segmentation', 'all_image_segmentation_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'image_segmentation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting image segmentation task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_image_segmentation_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No image segmentation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} image segmentation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Image segmentation task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "335f6b8b-4f7a-4694-b55a-92b8bfe03db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting continual learning task data collection (20+ datapoints)...\n",
      "Finding continual learning benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 34 potential continual learning benchmarks\n",
      "Checking continual-learning-on-split-cifar-100...\n",
      "continual-learning-on-split-cifar-100: 2 datapoints\n",
      "Checking continual-learning-on-split-mnist...\n",
      "continual-learning-on-split-mnist: 0 datapoints\n",
      "Checking continual-learning-on-permuted-mnist...\n",
      "continual-learning-on-permuted-mnist: 3 datapoints\n",
      "Checking continual-learning-on-rotated-mnist...\n",
      "continual-learning-on-rotated-mnist: 1 datapoints\n",
      "Checking continual-learning-on-core50...\n",
      "continual-learning-on-core50: 0 datapoints\n",
      "Checking continual-learning-on-asc-19-tasks...\n",
      "continual-learning-on-asc-19-tasks: 15 datapoints\n",
      "Checking continual-learning-on-visual-domain-decathlon...\n",
      "continual-learning-on-visual-domain-decathlon: 14 datapoints\n",
      "Checking continual-learning-on-cifar100-20-tasks...\n",
      "continual-learning-on-cifar100-20-tasks: 9 datapoints\n",
      "Checking continual-learning-on-tiny-imagenet-10tasks...\n",
      "continual-learning-on-tiny-imagenet-10tasks: 9 datapoints\n",
      "Checking continual-learning-on-f-celeba-10-tasks...\n",
      "continual-learning-on-f-celeba-10-tasks: 7 datapoints\n",
      "Checking continual-learning-on-imagenet-fine-grained-6...\n",
      "continual-learning-on-imagenet-fine-grained-6: 6 datapoints\n",
      "Checking continual-learning-on-cubs-fine-grained-6...\n",
      "continual-learning-on-cubs-fine-grained-6: 6 datapoints\n",
      "Checking continual-learning-on-stanford-cars-fine...\n",
      "continual-learning-on-stanford-cars-fine: 6 datapoints\n",
      "Checking continual-learning-on-flowers-fine-grained-6...\n",
      "continual-learning-on-flowers-fine-grained-6: 6 datapoints\n",
      "Checking continual-learning-on-wikiart-fine-grained-6...\n",
      "continual-learning-on-wikiart-fine-grained-6: 6 datapoints\n",
      "Checking continual-learning-on-sketch-fine-grained-6...\n",
      "continual-learning-on-sketch-fine-grained-6: 6 datapoints\n",
      "Checking continual-learning-on-dsc-10-tasks...\n",
      "continual-learning-on-dsc-10-tasks: 6 datapoints\n",
      "Checking continual-learning-on-20newsgroup-10-tasks...\n",
      "continual-learning-on-20newsgroup-10-tasks: 6 datapoints\n",
      "Checking continual-learning-on-imagenet-50-5-tasks...\n",
      "continual-learning-on-imagenet-50-5-tasks: 5 datapoints\n",
      "Checking continual-learning-on-cifar100-10-tasks...\n",
      "continual-learning-on-cifar100-10-tasks: 5 datapoints\n",
      "Checking continual-learning-on-coarse-cifar100...\n",
      "continual-learning-on-coarse-cifar100: 1 datapoints\n",
      "Checking continual-learning-on-cifar100-20-tasks-1...\n",
      "continual-learning-on-cifar100-20-tasks-1: 1 datapoints\n",
      "Checking continual-learning-on-cub-200-2011-20-tasks-1...\n",
      "continual-learning-on-cub-200-2011-20-tasks-1: 1 datapoints\n",
      "Checking continual-learning-on-5-dataset-1-epoch...\n",
      "continual-learning-on-5-dataset-1-epoch: 1 datapoints\n",
      "Checking continual-learning-on-mini-imagenet-20-tasks...\n",
      "continual-learning-on-mini-imagenet-20-tasks: 1 datapoints\n",
      "Checking continual-learning-on-mlt17...\n",
      "continual-learning-on-mlt17: 1 datapoints\n",
      "Checking continual-learning-on-split-cifar-10-5-tasks...\n",
      "continual-learning-on-split-cifar-10-5-tasks: 1 datapoints\n",
      "Checking continual-learning-on-split-mnist-5-tasks...\n",
      "continual-learning-on-split-mnist-5-tasks: 1 datapoints\n",
      "Checking continual-learning-on-miniimagenet-resnet-18...\n",
      "continual-learning-on-miniimagenet-resnet-18: 1 datapoints\n",
      "Checking continual-learning-on-tinyimagenet-resnet-18...\n",
      "continual-learning-on-tinyimagenet-resnet-18: 1 datapoints\n",
      "Checking continual-learning-on-cifar-100-alexnet-300...\n",
      "continual-learning-on-cifar-100-alexnet-300: 1 datapoints\n",
      "Checking continual-learning-on-cifar-100-resnet-18-300...\n",
      "continual-learning-on-cifar-100-resnet-18-300: 1 datapoints\n",
      "Checking continual-learning-on-miniimagenet...\n",
      "continual-learning-on-miniimagenet: 1 datapoints\n",
      "Checking continual-learning-on-5-datasets...\n",
      "continual-learning-on-5-datasets: 1 datapoints\n",
      "No continual learning benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'continual_learning'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_continual_learning_benchmarks_with_20plus():\n",
    "    \"\"\"Get continual learning benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding continual learning benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/continual-learning-on-split-cifar-100\",\n",
    "        \"https://paperswithcode.com/sota/continual-learning-on-split-mnist\",\n",
    "        \"https://paperswithcode.com/sota/continual-learning-on-permuted-mnist\",\n",
    "        \"https://paperswithcode.com/sota/continual-learning-on-rotated-mnist\",\n",
    "        \"https://paperswithcode.com/sota/continual-learning-on-core50\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main continual learning page\n",
    "        driver.get(\"https://paperswithcode.com/task/continual-learning\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"continual-learning\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential continual learning benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'continual_learning', 'continual_learning_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'continual_learning', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'continual_learning', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all continual learning models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'continual_learning')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'continual_learning', 'all_continual_learning_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'continual_learning', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting continual learning task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_continual_learning_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No continual learning benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} continual learning benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Continual learning task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45de8230-5fba-4f63-a58c-564f590d28dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting medical image segmentation task data collection (20+ datapoints)...\n",
      "Finding medical image segmentation benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 52 potential medical image segmentation benchmarks\n",
      "Checking medical-image-segmentation-on-acdc...\n",
      "medical-image-segmentation-on-acdc: 6 datapoints\n",
      "Checking medical-image-segmentation-on-brats...\n",
      "medical-image-segmentation-on-brats: 0 datapoints\n",
      "Checking medical-image-segmentation-on-kvasir-seg...\n",
      "medical-image-segmentation-on-kvasir-seg: 56 datapoints\n",
      "Added medical-image-segmentation-on-kvasir-seg with 56 datapoints\n",
      "Checking medical-image-segmentation-on-isic-2017...\n",
      "medical-image-segmentation-on-isic-2017: 0 datapoints\n",
      "Checking medical-image-segmentation-on-cvc-clinicdb...\n",
      "medical-image-segmentation-on-cvc-clinicdb: 46 datapoints\n",
      "Added medical-image-segmentation-on-cvc-clinicdb with 46 datapoints\n",
      "Checking medical-image-segmentation-on-etis...\n",
      "medical-image-segmentation-on-etis: 24 datapoints\n",
      "Added medical-image-segmentation-on-etis with 24 datapoints\n",
      "Checking medical-image-segmentation-on-synapse-multi...\n",
      "medical-image-segmentation-on-synapse-multi: 23 datapoints\n",
      "Added medical-image-segmentation-on-synapse-multi with 23 datapoints\n",
      "Checking medical-image-segmentation-on-cvc-colondb...\n",
      "medical-image-segmentation-on-cvc-colondb: 23 datapoints\n",
      "Added medical-image-segmentation-on-cvc-colondb with 23 datapoints\n",
      "Checking medical-image-segmentation-on-automatic...\n",
      "medical-image-segmentation-on-automatic: 20 datapoints\n",
      "Added medical-image-segmentation-on-automatic with 20 datapoints\n",
      "Checking medical-image-segmentation-on-monuseg...\n",
      "medical-image-segmentation-on-monuseg: 14 datapoints\n",
      "Checking medical-image-segmentation-on-2018-data...\n",
      "medical-image-segmentation-on-2018-data: 10 datapoints\n",
      "Checking medical-image-segmentation-on-glas...\n",
      "medical-image-segmentation-on-glas: 10 datapoints\n",
      "Checking medical-image-segmentation-on-bkai-igh...\n",
      "medical-image-segmentation-on-bkai-igh: 9 datapoints\n",
      "Checking medical-image-segmentation-on-miccai-2015-1...\n",
      "medical-image-segmentation-on-miccai-2015-1: 8 datapoints\n",
      "Checking medical-image-segmentation-on-drive-1...\n",
      "medical-image-segmentation-on-drive-1: 5 datapoints\n",
      "Checking medical-image-segmentation-on-cvc...\n",
      "medical-image-segmentation-on-cvc: 5 datapoints\n",
      "Checking medical-image-segmentation-on-isic-2018-1...\n",
      "medical-image-segmentation-on-isic-2018-1: 5 datapoints\n",
      "Checking medical-image-segmentation-on-medical...\n",
      "medical-image-segmentation-on-medical: 5 datapoints\n",
      "Checking medical-image-segmentation-on-robust-mis...\n",
      "medical-image-segmentation-on-robust-mis: 4 datapoints\n",
      "Checking medical-image-segmentation-on-isbi-2012-em...\n",
      "medical-image-segmentation-on-isbi-2012-em: 3 datapoints\n",
      "Checking medical-image-segmentation-on-em...\n",
      "medical-image-segmentation-on-em: 3 datapoints\n",
      "Checking medical-image-segmentation-on-rite...\n",
      "medical-image-segmentation-on-rite: 3 datapoints\n",
      "Checking medical-image-segmentation-on-kvasir...\n",
      "medical-image-segmentation-on-kvasir: 3 datapoints\n",
      "Checking medical-image-segmentation-on-brain-us...\n",
      "medical-image-segmentation-on-brain-us: 3 datapoints\n",
      "Checking medical-image-segmentation-on-chase-db1...\n",
      "medical-image-segmentation-on-chase-db1: 3 datapoints\n",
      "Checking medical-image-segmentation-on-isic2018...\n",
      "medical-image-segmentation-on-isic2018: 3 datapoints\n",
      "Checking medical-image-segmentation-on-isic-2018...\n",
      "medical-image-segmentation-on-isic-2018: 2 datapoints\n",
      "Checking medical-image-segmentation-on-endotect-polyp...\n",
      "medical-image-segmentation-on-endotect-polyp: 2 datapoints\n",
      "Checking medical-image-segmentation-on-medico...\n",
      "medical-image-segmentation-on-medico: 2 datapoints\n",
      "Checking medical-image-segmentation-on-kvasircapsule...\n",
      "medical-image-segmentation-on-kvasircapsule: 2 datapoints\n",
      "Checking medical-image-segmentation-on-lits2017...\n",
      "medical-image-segmentation-on-lits2017: 2 datapoints\n",
      "Checking medical-image-segmentation-on-iseg-2017...\n",
      "medical-image-segmentation-on-iseg-2017: 1 datapoints\n",
      "Checking medical-image-segmentation-on-chaos-mri...\n",
      "medical-image-segmentation-on-chaos-mri: 1 datapoints\n",
      "Checking medical-image-segmentation-on-hsvm...\n",
      "medical-image-segmentation-on-hsvm: 1 datapoints\n",
      "Checking medical-image-segmentation-on-cell...\n",
      "medical-image-segmentation-on-cell: 1 datapoints\n",
      "Checking medical-image-segmentation-on-2015-miccai...\n",
      "medical-image-segmentation-on-2015-miccai: 1 datapoints\n",
      "Checking medical-image-segmentation-on-synapse...\n",
      "medical-image-segmentation-on-synapse: 1 datapoints\n",
      "Checking medical-image-segmentation-on-segpc-2021...\n",
      "medical-image-segmentation-on-segpc-2021: 1 datapoints\n",
      "Checking medical-image-segmentation-on-autoimmune...\n",
      "medical-image-segmentation-on-autoimmune: 1 datapoints\n",
      "Checking medical-image-segmentation-on-hyper-kvasir...\n",
      "medical-image-segmentation-on-hyper-kvasir: 1 datapoints\n",
      "Checking medical-image-segmentation-on-asu-mayo-clinic-1...\n",
      "medical-image-segmentation-on-asu-mayo-clinic-1: 1 datapoints\n",
      "Checking medical-image-segmentation-on-mosmeddata...\n",
      "medical-image-segmentation-on-mosmeddata: 1 datapoints\n",
      "Checking medical-image-segmentation-on-miccai-2015-2...\n",
      "medical-image-segmentation-on-miccai-2015-2: 1 datapoints\n",
      "Checking medical-image-segmentation-on-monuseg-2018...\n",
      "medical-image-segmentation-on-monuseg-2018: 1 datapoints\n",
      "Checking medical-image-segmentation-on-monusac...\n",
      "medical-image-segmentation-on-monusac: 1 datapoints\n",
      "Checking medical-image-segmentation-on-amos...\n",
      "medical-image-segmentation-on-amos: 1 datapoints\n",
      "Checking medical-image-segmentation-on-promise12...\n",
      "medical-image-segmentation-on-promise12: 1 datapoints\n",
      "Checking medical-image-segmentation-on-extended-task10...\n",
      "medical-image-segmentation-on-extended-task10: 1 datapoints\n",
      "Checking medical-image-segmentation-on-autooral...\n",
      "medical-image-segmentation-on-autooral: 1 datapoints\n",
      "Checking medical-image-segmentation-on-enseg...\n",
      "medical-image-segmentation-on-enseg: 1 datapoints\n",
      "Checking medical-image-segmentation-on-tnbc...\n",
      "medical-image-segmentation-on-tnbc: 1 datapoints\n",
      "Checking medical-image-segmentation-on-electron...\n",
      "medical-image-segmentation-on-electron: 1 datapoints\n",
      "Saved 6 benchmarks with 20+ datapoints\n",
      "Found 6 medical image segmentation benchmarks with 20+ datapoints\n",
      "Scraping data for medical-image-segmentation-on-kvasir-seg...\n",
      "Saved 56 models for medical-image-segmentation-on-kvasir-seg\n",
      "Scraping data for medical-image-segmentation-on-cvc-clinicdb...\n",
      "Saved 46 models for medical-image-segmentation-on-cvc-clinicdb\n",
      "Scraping data for medical-image-segmentation-on-etis...\n",
      "Saved 24 models for medical-image-segmentation-on-etis\n",
      "Scraping data for medical-image-segmentation-on-synapse-multi...\n",
      "Saved 23 models for medical-image-segmentation-on-synapse-multi\n",
      "Scraping data for medical-image-segmentation-on-cvc-colondb...\n",
      "Saved 23 models for medical-image-segmentation-on-cvc-colondb\n",
      "Scraping data for medical-image-segmentation-on-automatic...\n",
      "Saved 20 models for medical-image-segmentation-on-automatic\n",
      "Collected a total of 192 models across all benchmarks\n",
      "Combined 192 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Medical image segmentation task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'medical_image_segmentation'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_medical_image_segmentation_benchmarks_with_20plus():\n",
    "    \"\"\"Get medical image segmentation benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding medical image segmentation benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-acdc\",\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-brats\",\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-kvasir-seg\",\n",
    "        \"https://paperswithcode.com/sota/medical-image-segmentation-on-isic-2017\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main medical image segmentation page\n",
    "        driver.get(\"https://paperswithcode.com/task/medical-image-segmentation\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"medical-image-segmentation\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential medical image segmentation benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'medical_image_segmentation', 'medical_image_segmentation_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'medical_image_segmentation', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'medical_image_segmentation', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all medical image segmentation models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'medical_image_segmentation')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'medical_image_segmentation', 'all_medical_image_segmentation_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'medical_image_segmentation', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting medical image segmentation task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_medical_image_segmentation_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No medical image segmentation benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} medical image segmentation benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Medical image segmentation task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6df1dc2-c83a-4fcc-b003-e1c8b737d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting transfer learning task data collection (20+ datapoints)...\n",
      "Finding transfer learning benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 10 potential transfer learning benchmarks\n",
      "Checking transfer-learning-on-imagenet...\n",
      "transfer-learning-on-imagenet: 0 datapoints\n",
      "Checking transfer-learning-on-cifar-10...\n",
      "transfer-learning-on-cifar-10: 0 datapoints\n",
      "Checking transfer-learning-on-cifar-100...\n",
      "transfer-learning-on-cifar-100: 0 datapoints\n",
      "Checking transfer-learning-on-pascal-voc-2007...\n",
      "transfer-learning-on-pascal-voc-2007: 0 datapoints\n",
      "Checking transfer-learning-on-office-home...\n",
      "transfer-learning-on-office-home: 5 datapoints\n",
      "Checking transfer-learning-on-banglalekha-isolated...\n",
      "transfer-learning-on-banglalekha-isolated: 1 datapoints\n",
      "Checking transfer-learning-on-coco70...\n",
      "transfer-learning-on-coco70: 1 datapoints\n",
      "Checking transfer-learning-on-100-sleep-nights-of-8...\n",
      "transfer-learning-on-100-sleep-nights-of-8: 1 datapoints\n",
      "Checking transfer-learning-on-kitti-object-tracking...\n",
      "transfer-learning-on-kitti-object-tracking: 1 datapoints\n",
      "Checking transfer-learning-on-retinal-fundus...\n",
      "transfer-learning-on-retinal-fundus: 1 datapoints\n",
      "No transfer learning benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'transfer_learning'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_transfer_learning_benchmarks_with_20plus():\n",
    "    \"\"\"Get transfer learning benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding transfer learning benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/transfer-learning-on-imagenet\",\n",
    "        \"https://paperswithcode.com/sota/transfer-learning-on-cifar-10\",\n",
    "        \"https://paperswithcode.com/sota/transfer-learning-on-cifar-100\",\n",
    "        \"https://paperswithcode.com/sota/transfer-learning-on-pascal-voc-2007\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main transfer learning page\n",
    "        driver.get(\"https://paperswithcode.com/task/transfer-learning\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"transfer-learning\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential transfer learning benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'transfer_learning', 'transfer_learning_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'transfer_learning', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'transfer_learning', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all transfer learning models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'transfer_learning')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'transfer_learning', 'all_transfer_learning_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'transfer_learning', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting transfer learning task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_transfer_learning_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No transfer learning benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} transfer learning benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Transfer learning task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f90d47-03a0-43b4-90c6-10cb17b510ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting multi-task learning task data collection (20+ datapoints)...\n",
      "Finding multi-task learning benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 11 potential multi-task learning benchmarks\n",
      "Checking multi-task-learning-on-glue...\n",
      "multi-task-learning-on-glue: 0 datapoints\n",
      "Checking multi-task-learning-on-decanlp...\n",
      "multi-task-learning-on-decanlp: 0 datapoints\n",
      "Checking multi-task-learning-on-nyu-depth-v2...\n",
      "multi-task-learning-on-nyu-depth-v2: 0 datapoints\n",
      "Checking multi-task-learning-on-cityscapes...\n",
      "multi-task-learning-on-cityscapes: 3 datapoints\n",
      "Checking multi-task-learning-on-omniglot...\n",
      "multi-task-learning-on-omniglot: 2 datapoints\n",
      "Checking multi-task-learning-on-nyuv2...\n",
      "multi-task-learning-on-nyuv2: 2 datapoints\n",
      "Checking multi-task-learning-on-qm9...\n",
      "multi-task-learning-on-qm9: 5 datapoints\n",
      "Checking multi-task-learning-on-celeba...\n",
      "multi-task-learning-on-celeba: 1 datapoints\n",
      "Checking multi-task-learning-on-wireframe-dataset...\n",
      "multi-task-learning-on-wireframe-dataset: 1 datapoints\n",
      "Checking multi-task-learning-on-utkface...\n",
      "multi-task-learning-on-utkface: 1 datapoints\n",
      "Checking multi-task-learning-on-chestx-ray14...\n",
      "multi-task-learning-on-chestx-ray14: 1 datapoints\n",
      "No multi-task learning benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'multi_task_learning'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_multi_task_learning_benchmarks_with_20plus():\n",
    "    \"\"\"Get multi-task learning benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding multi-task learning benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/multi-task-learning-on-glue\",\n",
    "        \"https://paperswithcode.com/sota/multi-task-learning-on-decanlp\",\n",
    "        \"https://paperswithcode.com/sota/multi-task-learning-on-nyu-depth-v2\",\n",
    "        \"https://paperswithcode.com/sota/multi-task-learning-on-cityscapes\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main multi-task learning page\n",
    "        driver.get(\"https://paperswithcode.com/task/multi-task-learning\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"multi-task-learning\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential multi-task learning benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'multi_task_learning', 'multi_task_learning_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'multi_task_learning', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'multi_task_learning', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all multi-task learning models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'multi_task_learning')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'multi_task_learning', 'all_multi_task_learning_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'multi_task_learning', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting multi-task learning task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_multi_task_learning_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No multi-task learning benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} multi-task learning benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Multi-task learning task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6dc987-43be-4828-bf5c-639c70d096ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting anomaly detection task data collection (20+ datapoints)...\n",
      "Finding anomaly detection benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 77 potential anomaly detection benchmarks\n",
      "Checking anomaly-detection-on-mvtec-ad...\n",
      "anomaly-detection-on-mvtec-ad: 144 datapoints\n",
      "Added anomaly-detection-on-mvtec-ad with 144 datapoints\n",
      "Checking anomaly-detection-on-kdd-cup-99...\n",
      "anomaly-detection-on-kdd-cup-99: 0 datapoints\n",
      "Checking anomaly-detection-on-nsl-kdd...\n",
      "anomaly-detection-on-nsl-kdd: 0 datapoints\n",
      "Checking anomaly-detection-on-odds...\n",
      "anomaly-detection-on-odds: 3 datapoints\n",
      "Checking anomaly-detection-on-visa...\n",
      "anomaly-detection-on-visa: 47 datapoints\n",
      "Added anomaly-detection-on-visa with 47 datapoints\n",
      "Checking anomaly-detection-on-mvtec-loco-ad...\n",
      "anomaly-detection-on-mvtec-loco-ad: 40 datapoints\n",
      "Added anomaly-detection-on-mvtec-loco-ad with 40 datapoints\n",
      "Checking anomaly-detection-on-one-class-cifar-10...\n",
      "anomaly-detection-on-one-class-cifar-10: 36 datapoints\n",
      "Added anomaly-detection-on-one-class-cifar-10 with 36 datapoints\n",
      "Checking anomaly-detection-on-chuk-avenue...\n",
      "anomaly-detection-on-chuk-avenue: 35 datapoints\n",
      "Added anomaly-detection-on-chuk-avenue with 35 datapoints\n",
      "Checking anomaly-detection-on-shanghaitech...\n",
      "anomaly-detection-on-shanghaitech: 31 datapoints\n",
      "Added anomaly-detection-on-shanghaitech with 31 datapoints\n",
      "Checking anomaly-detection-on-ucr-anomaly-archive...\n",
      "anomaly-detection-on-ucr-anomaly-archive: 24 datapoints\n",
      "Added anomaly-detection-on-ucr-anomaly-archive with 24 datapoints\n",
      "Checking anomaly-detection-on-fishyscapes-l-f...\n",
      "anomaly-detection-on-fishyscapes-l-f: 18 datapoints\n",
      "Checking anomaly-detection-on-one-class-cifar-100...\n",
      "anomaly-detection-on-one-class-cifar-100: 15 datapoints\n",
      "Checking anomaly-detection-on-mpdd...\n",
      "anomaly-detection-on-mpdd: 15 datapoints\n",
      "Checking anomaly-detection-on-ubnormal...\n",
      "anomaly-detection-on-ubnormal: 14 datapoints\n",
      "Checking anomaly-detection-on-btad...\n",
      "anomaly-detection-on-btad: 14 datapoints\n",
      "Checking anomaly-detection-on-unlabeled-cifar-10-vs...\n",
      "anomaly-detection-on-unlabeled-cifar-10-vs: 13 datapoints\n",
      "Checking anomaly-detection-on-ucsd-ped2...\n",
      "anomaly-detection-on-ucsd-ped2: 13 datapoints\n",
      "Checking anomaly-detection-on-fashion-mnist...\n",
      "anomaly-detection-on-fashion-mnist: 12 datapoints\n",
      "Checking anomaly-detection-on-one-class-imagenet-30...\n",
      "anomaly-detection-on-one-class-imagenet-30: 11 datapoints\n",
      "Checking anomaly-detection-on-numenta-anomaly...\n",
      "anomaly-detection-on-numenta-anomaly: 10 datapoints\n",
      "Checking anomaly-detection-on-road-anomaly...\n",
      "anomaly-detection-on-road-anomaly: 10 datapoints\n",
      "Checking anomaly-detection-on-anomaly-detection-on...\n",
      "anomaly-detection-on-anomaly-detection-on: 8 datapoints\n",
      "Checking anomaly-detection-on-fishyscapes-1...\n",
      "anomaly-detection-on-fishyscapes-1: 8 datapoints\n",
      "Checking anomaly-detection-on-aebad-s...\n",
      "anomaly-detection-on-aebad-s: 8 datapoints\n",
      "Checking anomaly-detection-on-aebad-v...\n",
      "anomaly-detection-on-aebad-v: 7 datapoints\n",
      "Checking anomaly-detection-on-mnist...\n",
      "anomaly-detection-on-mnist: 6 datapoints\n",
      "Checking anomaly-detection-on-hyper-kvasir-dataset...\n",
      "anomaly-detection-on-hyper-kvasir-dataset: 6 datapoints\n",
      "Checking anomaly-detection-on-leave-one-class-out...\n",
      "anomaly-detection-on-leave-one-class-out: 6 datapoints\n",
      "Checking anomaly-detection-on-leave-one-class-out-1...\n",
      "anomaly-detection-on-leave-one-class-out-1: 6 datapoints\n",
      "Checking anomaly-detection-on-anomaly-detection-on-1...\n",
      "anomaly-detection-on-anomaly-detection-on-1: 5 datapoints\n",
      "Checking anomaly-detection-on-anomaly-detection-on-2...\n",
      "anomaly-detection-on-anomaly-detection-on-2: 5 datapoints\n",
      "Checking anomaly-detection-on-lag...\n",
      "anomaly-detection-on-lag: 5 datapoints\n",
      "Checking anomaly-detection-on-insplad...\n",
      "anomaly-detection-on-insplad: 5 datapoints\n",
      "Checking anomaly-detection-on-cats-and-dogs...\n",
      "anomaly-detection-on-cats-and-dogs: 4 datapoints\n",
      "Checking anomaly-detection-on-dior...\n",
      "anomaly-detection-on-dior: 4 datapoints\n",
      "Checking anomaly-detection-on-surface-defect-saliency...\n",
      "anomaly-detection-on-surface-defect-saliency: 4 datapoints\n",
      "Checking anomaly-detection-on-lost-and-found...\n",
      "anomaly-detection-on-lost-and-found: 4 datapoints\n",
      "Checking anomaly-detection-on-mvtec-ad-textures...\n",
      "anomaly-detection-on-mvtec-ad-textures: 4 datapoints\n",
      "Checking anomaly-detection-on-pheva...\n",
      "anomaly-detection-on-pheva: 4 datapoints\n",
      "Checking anomaly-detection-on-ucsd-peds2...\n",
      "anomaly-detection-on-ucsd-peds2: 3 datapoints\n",
      "Checking anomaly-detection-on-corridor...\n",
      "anomaly-detection-on-corridor: 3 datapoints\n",
      "Checking anomaly-detection-on-uea-time-series-datasets...\n",
      "anomaly-detection-on-uea-time-series-datasets: 3 datapoints\n",
      "Checking anomaly-detection-on-mvtec-ad-textures-domain...\n",
      "anomaly-detection-on-mvtec-ad-textures-domain: 3 datapoints\n",
      "Checking anomaly-detection-on-voraus-ad...\n",
      "anomaly-detection-on-voraus-ad: 3 datapoints\n",
      "Checking anomaly-detection-on-thyroid...\n",
      "anomaly-detection-on-thyroid: 2 datapoints\n",
      "Checking anomaly-detection-on-mvtec-3d-ad-1...\n",
      "anomaly-detection-on-mvtec-3d-ad-1: 2 datapoints\n",
      "Checking anomaly-detection-on-vehicle-claims...\n",
      "anomaly-detection-on-vehicle-claims: 2 datapoints\n",
      "Checking anomaly-detection-on-pad-dataset...\n",
      "anomaly-detection-on-pad-dataset: 2 datapoints\n",
      "Checking anomaly-detection-on-smd...\n",
      "anomaly-detection-on-smd: 2 datapoints\n",
      "Checking anomaly-detection-on-census...\n",
      "anomaly-detection-on-census: 1 datapoints\n",
      "Checking anomaly-detection-on-mnist-test...\n",
      "anomaly-detection-on-mnist-test: 1 datapoints\n",
      "Checking anomaly-detection-on-cifar-10...\n",
      "anomaly-detection-on-cifar-10: 1 datapoints\n",
      "Checking anomaly-detection-on-ag-news...\n",
      "anomaly-detection-on-ag-news: 1 datapoints\n",
      "Checking anomaly-detection-on-assira-cat-vs-dog...\n",
      "anomaly-detection-on-assira-cat-vs-dog: 1 datapoints\n",
      "Checking anomaly-detection-on-stl-10...\n",
      "anomaly-detection-on-stl-10: 1 datapoints\n",
      "Checking anomaly-detection-on-forest-covertype...\n",
      "anomaly-detection-on-forest-covertype: 1 datapoints\n",
      "Checking anomaly-detection-on-kaggle-credit-card-fraud...\n",
      "anomaly-detection-on-kaggle-credit-card-fraud: 1 datapoints\n",
      "Checking anomaly-detection-on-nb15-analysis...\n",
      "anomaly-detection-on-nb15-analysis: 1 datapoints\n",
      "Checking anomaly-detection-on-nb15-backdoor...\n",
      "anomaly-detection-on-nb15-backdoor: 1 datapoints\n",
      "Checking anomaly-detection-on-nb15-dos...\n",
      "anomaly-detection-on-nb15-dos: 1 datapoints\n",
      "Checking anomaly-detection-on-bottlecap...\n",
      "anomaly-detection-on-bottlecap: 1 datapoints\n",
      "Checking anomaly-detection-on-kdd-99...\n",
      "anomaly-detection-on-kdd-99: 1 datapoints\n",
      "Checking anomaly-detection-on-mvtec-3d-ad-rgb...\n",
      "anomaly-detection-on-mvtec-3d-ad-rgb: 1 datapoints\n",
      "Checking anomaly-detection-on-kdd-cup-1999...\n",
      "anomaly-detection-on-kdd-cup-1999: 1 datapoints\n",
      "Checking anomaly-detection-on-mit-bih-arrhythmia...\n",
      "anomaly-detection-on-mit-bih-arrhythmia: 1 datapoints\n",
      "Checking anomaly-detection-on-musk-v1...\n",
      "anomaly-detection-on-musk-v1: 1 datapoints\n",
      "Checking anomaly-detection-on-svhn...\n",
      "anomaly-detection-on-svhn: 1 datapoints\n",
      "Checking anomaly-detection-on-ksdd2...\n",
      "anomaly-detection-on-ksdd2: 1 datapoints\n",
      "Checking anomaly-detection-on-tii-ssrc-23...\n",
      "anomaly-detection-on-tii-ssrc-23: 1 datapoints\n",
      "Checking anomaly-detection-on-adni...\n",
      "anomaly-detection-on-adni: 1 datapoints\n",
      "Checking anomaly-detection-on-coco-ooc...\n",
      "anomaly-detection-on-coco-ooc: 1 datapoints\n",
      "Checking anomaly-detection-on-ucf-crime-1...\n",
      "anomaly-detection-on-ucf-crime-1: 1 datapoints\n",
      "Checking anomaly-detection-on-wfdd...\n",
      "anomaly-detection-on-wfdd: 1 datapoints\n",
      "Checking anomaly-detection-on-shanghaitech-campus-2...\n",
      "anomaly-detection-on-shanghaitech-campus-2: 1 datapoints\n",
      "Checking anomaly-detection-on-iitb-corridor-1...\n",
      "anomaly-detection-on-iitb-corridor-1: 1 datapoints\n",
      "Checking anomaly-detection-on-street-scene...\n",
      "anomaly-detection-on-street-scene: 1 datapoints\n",
      "Checking anomaly-detection-on-real-3d-ad...\n",
      "anomaly-detection-on-real-3d-ad: 1 datapoints\n",
      "Saved 7 benchmarks with 20+ datapoints\n",
      "Found 7 anomaly detection benchmarks with 20+ datapoints\n",
      "Scraping data for anomaly-detection-on-mvtec-ad...\n",
      "Saved 144 models for anomaly-detection-on-mvtec-ad\n",
      "Scraping data for anomaly-detection-on-visa...\n",
      "Saved 46 models for anomaly-detection-on-visa\n",
      "Scraping data for anomaly-detection-on-mvtec-loco-ad...\n",
      "Saved 40 models for anomaly-detection-on-mvtec-loco-ad\n",
      "Scraping data for anomaly-detection-on-one-class-cifar-10...\n",
      "Saved 36 models for anomaly-detection-on-one-class-cifar-10\n",
      "Scraping data for anomaly-detection-on-chuk-avenue...\n",
      "Saved 34 models for anomaly-detection-on-chuk-avenue\n",
      "Scraping data for anomaly-detection-on-shanghaitech...\n",
      "Saved 31 models for anomaly-detection-on-shanghaitech\n",
      "Scraping data for anomaly-detection-on-ucr-anomaly-archive...\n",
      "Saved 24 models for anomaly-detection-on-ucr-anomaly-archive\n",
      "Collected a total of 355 models across all benchmarks\n",
      "Combined 355 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Anomaly detection task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'anomaly_detection'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_anomaly_detection_benchmarks_with_20plus():\n",
    "    \"\"\"Get anomaly detection benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding anomaly detection benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad\",\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-kdd-cup-99\",\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-nsl-kdd\",\n",
    "        \"https://paperswithcode.com/sota/anomaly-detection-on-odds\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main anomaly detection page\n",
    "        driver.get(\"https://paperswithcode.com/task/anomaly-detection\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"anomaly-detection\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential anomaly detection benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'anomaly_detection', 'anomaly_detection_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'anomaly_detection', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'anomaly_detection', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all anomaly detection models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'anomaly_detection')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'anomaly_detection', 'all_anomaly_detection_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'anomaly_detection', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting anomaly detection task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_anomaly_detection_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No anomaly detection benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} anomaly detection benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Anomaly detection task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b440318-35d2-4f56-9c6a-6bf32553f90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting denoising task data collection (20+ datapoints)...\n",
      "Finding denoising benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 10 potential denoising benchmarks\n",
      "Checking denoising-on-set12...\n",
      "denoising-on-set12: 0 datapoints\n",
      "Checking denoising-on-sidd...\n",
      "denoising-on-sidd: 0 datapoints\n",
      "Checking denoising-on-bsd400...\n",
      "denoising-on-bsd400: 0 datapoints\n",
      "Checking denoising-on-set14...\n",
      "denoising-on-set14: 0 datapoints\n",
      "Checking denoising-on-darmstadt-noise-dataset...\n",
      "denoising-on-darmstadt-noise-dataset: 10 datapoints\n",
      "Checking denoising-on-aapm...\n",
      "denoising-on-aapm: 1 datapoints\n",
      "Checking denoising-on-iris...\n",
      "denoising-on-iris: 1 datapoints\n",
      "Checking denoising-on-dnd-1...\n",
      "denoising-on-dnd-1: 1 datapoints\n",
      "Checking denoising-on-cbsd68-sigm75...\n",
      "denoising-on-cbsd68-sigm75: 1 datapoints\n",
      "Checking denoising-on-div2k...\n",
      "denoising-on-div2k: 1 datapoints\n",
      "No denoising benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'denoising'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_denoising_benchmarks_with_20plus():\n",
    "    \"\"\"Get denoising benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding denoising benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/denoising-on-set12\",\n",
    "        \"https://paperswithcode.com/sota/denoising-on-sidd\",\n",
    "        \"https://paperswithcode.com/sota/denoising-on-bsd400\",\n",
    "        \"https://paperswithcode.com/sota/denoising-on-set14\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main denoising page\n",
    "        driver.get(\"https://paperswithcode.com/task/denoising\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"denoising\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential denoising benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'denoising', 'denoising_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'denoising', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'denoising', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all denoising models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'denoising')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'denoising', 'all_denoising_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'denoising', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting denoising task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_denoising_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No denoising benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} denoising benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Denoising task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7d80f2-38bb-4cf0-a11b-d529f30b983c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting speech recognition task data collection (20+ datapoints)...\n",
      "Finding speech recognition benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 67 potential speech recognition benchmarks\n",
      "Checking speech-recognition-on-librispeech-test-clean...\n",
      "speech-recognition-on-librispeech-test-clean: 63 datapoints\n",
      "Added speech-recognition-on-librispeech-test-clean with 63 datapoints\n",
      "Checking speech-recognition-on-librispeech-test-other...\n",
      "speech-recognition-on-librispeech-test-other: 53 datapoints\n",
      "Added speech-recognition-on-librispeech-test-other with 53 datapoints\n",
      "Checking speech-recognition-on-switchboard...\n",
      "speech-recognition-on-switchboard: 0 datapoints\n",
      "Checking speech-recognition-on-chime-4...\n",
      "speech-recognition-on-chime-4: 0 datapoints\n",
      "Checking speech-recognition-on-timit...\n",
      "speech-recognition-on-timit: 22 datapoints\n",
      "Added speech-recognition-on-timit with 22 datapoints\n",
      "Checking speech-recognition-on-aishell-1...\n",
      "speech-recognition-on-aishell-1: 18 datapoints\n",
      "Checking speech-recognition-on-switchboard-hub500...\n",
      "speech-recognition-on-switchboard-hub500: 30 datapoints\n",
      "Added speech-recognition-on-switchboard-hub500 with 30 datapoints\n",
      "Checking speech-recognition-on-wsj-eval92...\n",
      "speech-recognition-on-wsj-eval92: 17 datapoints\n",
      "Checking speech-recognition-on-common-voice-german...\n",
      "speech-recognition-on-common-voice-german: 14 datapoints\n",
      "Checking speech-recognition-on-common-voice-spanish...\n",
      "speech-recognition-on-common-voice-spanish: 8 datapoints\n",
      "Checking speech-recognition-on-common-voice-french...\n",
      "speech-recognition-on-common-voice-french: 8 datapoints\n",
      "Checking speech-recognition-on-mediaspeech...\n",
      "speech-recognition-on-mediaspeech: 8 datapoints\n",
      "Checking speech-recognition-on-tuda...\n",
      "speech-recognition-on-tuda: 9 datapoints\n",
      "Checking speech-recognition-on-slue...\n",
      "speech-recognition-on-slue: 8 datapoints\n",
      "Checking speech-recognition-on-wenetspeech...\n",
      "speech-recognition-on-wenetspeech: 8 datapoints\n",
      "Checking speech-recognition-on-vietmed...\n",
      "speech-recognition-on-vietmed: 8 datapoints\n",
      "Checking speech-recognition-on-swb_hub_500-wer...\n",
      "speech-recognition-on-swb_hub_500-wer: 12 datapoints\n",
      "Checking speech-recognition-on-hub500-switchboard...\n",
      "speech-recognition-on-hub500-switchboard: 5 datapoints\n",
      "Checking speech-recognition-on-libri-light-test-clean...\n",
      "speech-recognition-on-libri-light-test-clean: 5 datapoints\n",
      "Checking speech-recognition-on-libri-light-test-other...\n",
      "speech-recognition-on-libri-light-test-other: 5 datapoints\n",
      "Checking speech-recognition-on-gigaspeech-dev...\n",
      "speech-recognition-on-gigaspeech-dev: 5 datapoints\n",
      "Checking speech-recognition-on-gigaspeech-test...\n",
      "speech-recognition-on-gigaspeech-test: 5 datapoints\n",
      "Checking speech-recognition-on-easycom...\n",
      "speech-recognition-on-easycom: 5 datapoints\n",
      "Checking speech-recognition-on-chime-6-dev-gss12...\n",
      "speech-recognition-on-chime-6-dev-gss12: 4 datapoints\n",
      "Checking speech-recognition-on-tedlium...\n",
      "speech-recognition-on-tedlium: 4 datapoints\n",
      "Checking speech-recognition-on-wsj-dev93...\n",
      "speech-recognition-on-wsj-dev93: 4 datapoints\n",
      "Checking speech-recognition-on-lrs3-ted...\n",
      "speech-recognition-on-lrs3-ted: 4 datapoints\n",
      "Checking speech-recognition-on-wsj-eval93...\n",
      "speech-recognition-on-wsj-eval93: 3 datapoints\n",
      "Checking speech-recognition-on-fongbe-speech...\n",
      "speech-recognition-on-fongbe-speech: 3 datapoints\n",
      "Checking speech-recognition-on-vivos...\n",
      "speech-recognition-on-vivos: 3 datapoints\n",
      "Checking speech-recognition-on-common-voice-vi...\n",
      "speech-recognition-on-common-voice-vi: 3 datapoints\n",
      "Checking speech-recognition-on-chime-6-eval...\n",
      "speech-recognition-on-chime-6-eval: 3 datapoints\n",
      "Checking speech-recognition-on-europarl-asr-en-guest...\n",
      "speech-recognition-on-europarl-asr-en-guest: 3 datapoints\n",
      "Checking speech-recognition-on-speech-commands-2...\n",
      "speech-recognition-on-speech-commands-2: 3 datapoints\n",
      "Checking speech-recognition-on-common-voice-italian...\n",
      "speech-recognition-on-common-voice-italian: 2 datapoints\n",
      "Checking speech-recognition-on-spgispeech...\n",
      "speech-recognition-on-spgispeech: 3 datapoints\n",
      "Checking speech-recognition-on-common-voice-2...\n",
      "speech-recognition-on-common-voice-2: 2 datapoints\n",
      "Checking speech-recognition-on-common-voice-english...\n",
      "speech-recognition-on-common-voice-english: 2 datapoints\n",
      "Checking speech-recognition-on-ami-imh...\n",
      "speech-recognition-on-ami-imh: 2 datapoints\n",
      "Checking speech-recognition-on-ami-sdm1...\n",
      "speech-recognition-on-ami-sdm1: 2 datapoints\n",
      "Checking speech-recognition-on-europarl-asr-en-mep...\n",
      "speech-recognition-on-europarl-asr-en-mep: 2 datapoints\n",
      "Checking speech-recognition-on-libricss...\n",
      "speech-recognition-on-libricss: 2 datapoints\n",
      "Checking speech-recognition-on-aishell-2...\n",
      "speech-recognition-on-aishell-2: 2 datapoints\n",
      "Checking speech-recognition-on-ted-lium...\n",
      "speech-recognition-on-ted-lium: 2 datapoints\n",
      "Checking speech-recognition-on-switchboard-300hr...\n",
      "speech-recognition-on-switchboard-300hr: 1 datapoints\n",
      "Checking speech-recognition-on-hub500-callhome...\n",
      "speech-recognition-on-hub500-callhome: 1 datapoints\n",
      "Checking speech-recognition-on-hub5-00-fisher-swbd...\n",
      "speech-recognition-on-hub5-00-fisher-swbd: 1 datapoints\n",
      "Checking speech-recognition-on-librispeech-train-clean...\n",
      "speech-recognition-on-librispeech-train-clean: 1 datapoints\n",
      "Checking speech-recognition-on-librispeech-train-clean-1...\n",
      "speech-recognition-on-librispeech-train-clean-1: 1 datapoints\n",
      "Checking speech-recognition-on-common-voice-portuguese...\n",
      "speech-recognition-on-common-voice-portuguese: 1 datapoints\n",
      "Checking speech-recognition-on-common-voice-russian...\n",
      "speech-recognition-on-common-voice-russian: 5 datapoints\n",
      "Checking speech-recognition-on-common-voice-frisian...\n",
      "speech-recognition-on-common-voice-frisian: 1 datapoints\n",
      "Checking speech-recognition-on-common-voice-japanese...\n",
      "speech-recognition-on-common-voice-japanese: 4 datapoints\n",
      "Checking speech-recognition-on-gigaspeech...\n",
      "speech-recognition-on-gigaspeech: 1 datapoints\n",
      "Checking speech-recognition-on-switchboard-swbd...\n",
      "speech-recognition-on-switchboard-swbd: 1 datapoints\n",
      "Checking speech-recognition-on-switchboard-callhome...\n",
      "speech-recognition-on-switchboard-callhome: 1 datapoints\n",
      "Checking speech-recognition-on-google-speech-commands...\n",
      "speech-recognition-on-google-speech-commands: 1 datapoints\n",
      "Checking speech-recognition-on-lrs2...\n",
      "speech-recognition-on-lrs2: 1 datapoints\n",
      "Checking speech-recognition-on-callhome-spanish-speech...\n",
      "speech-recognition-on-callhome-spanish-speech: 1 datapoints\n",
      "Checking speech-recognition-on-facebook-multilingual...\n",
      "speech-recognition-on-facebook-multilingual: 1 datapoints\n",
      "Checking speech-recognition-on-aishell-2-test-ios...\n",
      "speech-recognition-on-aishell-2-test-ios: 1 datapoints\n",
      "Checking speech-recognition-on-aishell-2-test-mic-1...\n",
      "speech-recognition-on-aishell-2-test-mic-1: 1 datapoints\n",
      "Checking speech-recognition-on-aishell-2-test-android-1...\n",
      "speech-recognition-on-aishell-2-test-android-1: 1 datapoints\n",
      "Checking speech-recognition-on-callhome-en...\n",
      "speech-recognition-on-callhome-en: 1 datapoints\n",
      "Checking speech-recognition-on-librispeech-100h-test...\n",
      "speech-recognition-on-librispeech-100h-test: 1 datapoints\n",
      "Checking speech-recognition-on-librispeech-100h-test-1...\n",
      "speech-recognition-on-librispeech-100h-test-1: 1 datapoints\n",
      "Checking speech-recognition-on-cas-vsr-s101...\n",
      "speech-recognition-on-cas-vsr-s101: 1 datapoints\n",
      "Error getting benchmarks: Cannot save file into a non-existent directory: '/Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data/speech_recognition'\n",
      "Found 4 speech recognition benchmarks with 20+ datapoints\n",
      "Scraping data for speech-recognition-on-librispeech-test-clean...\n",
      "Saved 59 models for speech-recognition-on-librispeech-test-clean\n",
      "Scraping data for speech-recognition-on-librispeech-test-other...\n",
      "Saved 53 models for speech-recognition-on-librispeech-test-other\n",
      "Scraping data for speech-recognition-on-timit...\n",
      "Saved 20 models for speech-recognition-on-timit\n",
      "Scraping data for speech-recognition-on-switchboard-hub500...\n",
      "Saved 18 models for speech-recognition-on-switchboard-hub500\n",
      "Collected a total of 150 models across all benchmarks\n",
      "Combined 150 models from all benchmarks\n",
      "Created yearly model count summary\n",
      "Speech recognition task data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'speech_recognition'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_speech_recognition_benchmarks_with_20plus():\n",
    "    \"\"\"Get speech recognition benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding speech recognition benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean\",\n",
    "        \"https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-other\",\n",
    "        \"https://paperswithcode.com/sota/speech-recognition-on-switchboard\",\n",
    "        \"https://paperswithcode.com/sota/speech-recognition-on-chime-4\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main speech recognition page\n",
    "        driver.get(\"https://paperswithcode.com/task/speech-recognition\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"speech-recognition\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential speech recognition benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'speech_recognition', 'speech_recognition_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'speech_recognition', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'speech_recognition', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all speech recognition models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'speech_recognition')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'speech_recognition', 'all_speech_recognition_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'speech_recognition', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting speech recognition task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_speech_recognition_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No speech recognition benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} speech recognition benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Speech recognition task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afd332b-1b80-4a1a-8519-907f73cfff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be stored in: /Users/karmabirchakraborty/Documents/Jupyter Notebooks/RA Task/tech_progress_data\n",
      "Starting knowledge graphs task data collection (20+ datapoints)...\n",
      "Finding knowledge graphs benchmarks with 20+ datapoints...\n",
      "Looking for benchmark links...\n",
      "Found 8 potential knowledge graphs benchmarks\n",
      "Checking knowledge-graph-completion-on-fb15k-237...\n",
      "knowledge-graph-completion-on-fb15k-237: 4 datapoints\n",
      "Checking knowledge-graph-completion-on-wn18rr...\n",
      "knowledge-graph-completion-on-wn18rr: 2 datapoints\n",
      "Checking knowledge-graph-completion-on-fb15k...\n",
      "knowledge-graph-completion-on-fb15k: 0 datapoints\n",
      "Checking knowledge-graph-completion-on-wn18...\n",
      "knowledge-graph-completion-on-wn18: 0 datapoints\n",
      "Checking knowledge-graphs-on-mars-multimodal...\n",
      "knowledge-graphs-on-mars-multimodal: 8 datapoints\n",
      "Checking knowledge-graphs-on-jerichoworld...\n",
      "knowledge-graphs-on-jerichoworld: 5 datapoints\n",
      "Checking knowledge-graphs-on-wikikg90m-lsc...\n",
      "knowledge-graphs-on-wikikg90m-lsc: 4 datapoints\n",
      "Checking knowledge-graphs-on-fb15k...\n",
      "knowledge-graphs-on-fb15k: 2 datapoints\n",
      "No knowledge graphs benchmarks with 20+ datapoints found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil import parser\n",
    "\n",
    "# Base directory for saving data - using the exact path structure\n",
    "home_dir = os.path.expanduser(\"~\")  # Gets the user's home directory\n",
    "data_dir = os.path.join(home_dir, \"Documents\", \"Jupyter Notebooks\", \"RA Task\", \"tech_progress_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'knowledge_graphs'), exist_ok=True)\n",
    "\n",
    "print(f\"Data will be stored in: {data_dir}\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Initialize and return a Chrome WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Updated headless syntax\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Helpful for Mac\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    # Set timeout\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using various formats and heuristics\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Try direct parsing\n",
    "        return parser.parse(text, fuzzy=True).date()\n",
    "    except:\n",
    "        # Look for year patterns\n",
    "        year_match = re.search(r'20[0-2][0-9]', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            # Look for month patterns\n",
    "            month_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*', text, re.IGNORECASE)\n",
    "            if month_match:\n",
    "                month_text = month_match.group().lower()\n",
    "                month_map = {\n",
    "                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                }\n",
    "                for prefix, month_num in month_map.items():\n",
    "                    if month_text.startswith(prefix):\n",
    "                        return datetime.date(year, month_num, 1)\n",
    "            \n",
    "            # If only year was found, default to January\n",
    "            return datetime.date(year, 1, 1)\n",
    "    return None\n",
    "\n",
    "def extract_parameters_from_text(text):\n",
    "    \"\"\"Extract parameter count from model description\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for parameter counts (millions, billions)\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]illion\\s*parameters',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*parameters',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'parameters:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Mm]',\n",
    "        r'params:\\s*(\\d+\\.?\\d*)\\s*[Bb]',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Mm]\\s*params',\n",
    "        r'(\\d+\\.?\\d*)\\s*[Bb]\\s*params'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            param_count = float(match.group(1))\n",
    "            # Convert to millions\n",
    "            if 'billion' in text.lower() or 'b params' in text.lower() or 'b parameters' in text.lower():\n",
    "                param_count *= 1000  # Convert billions to millions\n",
    "            return param_count\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_knowledge_graphs_benchmarks_with_20plus():\n",
    "    \"\"\"Get knowledge graphs benchmarks with at least 20 datapoints\"\"\"\n",
    "    print(\"Finding knowledge graphs benchmarks with 20+ datapoints...\")\n",
    "    \n",
    "    # Start with major benchmarks\n",
    "    major_benchmarks = [\n",
    "        \"https://paperswithcode.com/sota/knowledge-graph-completion-on-fb15k-237\",\n",
    "        \"https://paperswithcode.com/sota/knowledge-graph-completion-on-wn18rr\",\n",
    "        \"https://paperswithcode.com/sota/knowledge-graph-completion-on-fb15k\",\n",
    "        \"https://paperswithcode.com/sota/knowledge-graph-completion-on-wn18\"\n",
    "    ]\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    benchmarks_with_data = []\n",
    "    \n",
    "    try:\n",
    "        # First check the main knowledge graphs page\n",
    "        driver.get(\"https://paperswithcode.com/task/knowledge-graphs\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find all benchmark links\n",
    "        print(\"Looking for benchmark links...\")\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        benchmark_links = []\n",
    "        \n",
    "        # Add major benchmarks first\n",
    "        for url in major_benchmarks:\n",
    "            benchmark_links.append(url)\n",
    "        \n",
    "        # Add additional benchmarks from page\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"/sota/\" in href and \"knowledge-graph\" in href.lower():\n",
    "                if href not in benchmark_links:\n",
    "                    benchmark_links.append(href)\n",
    "        \n",
    "        print(f\"Found {len(benchmark_links)} potential knowledge graphs benchmarks\")\n",
    "        \n",
    "        # Check each benchmark for datapoint count\n",
    "        for benchmark_url in benchmark_links:\n",
    "            try:\n",
    "                benchmark_name = benchmark_url.split(\"/\")[-1]\n",
    "                print(f\"Checking {benchmark_name}...\")\n",
    "                \n",
    "                # Load the benchmark page\n",
    "                driver.get(benchmark_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Try to find timeline link\n",
    "                timeline_url = None\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"sota-over-time\" in href:\n",
    "                        timeline_url = href\n",
    "                        break\n",
    "                \n",
    "                # If found, check the timeline page\n",
    "                if timeline_url:\n",
    "                    print(f\"Found timeline link for {benchmark_name}\")\n",
    "                    driver.get(timeline_url)\n",
    "                    time.sleep(3)\n",
    "                \n",
    "                # Count datapoints in tables\n",
    "                tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "                max_datapoints = 0\n",
    "                \n",
    "                for table in tables:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    datapoints = len(rows) - 1  # Subtract 1 for header\n",
    "                    max_datapoints = max(max_datapoints, datapoints)\n",
    "                \n",
    "                print(f\"{benchmark_name}: {max_datapoints} datapoints\")\n",
    "                \n",
    "                # Add if it has 20+ datapoints\n",
    "                if max_datapoints >= 20:\n",
    "                    benchmarks_with_data.append({\n",
    "                        \"name\": benchmark_name,\n",
    "                        \"url\": benchmark_url,\n",
    "                        \"timeline_url\": timeline_url,\n",
    "                        \"datapoints\": max_datapoints\n",
    "                    })\n",
    "                    print(f\"Added {benchmark_name} with {max_datapoints} datapoints\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {benchmark_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save the benchmark list\n",
    "        if benchmarks_with_data:\n",
    "            df = pd.DataFrame(benchmarks_with_data)\n",
    "            df.to_csv(os.path.join(data_dir, 'knowledge_graphs', 'knowledge_graphs_benchmarks_20plus.csv'), index=False)\n",
    "            print(f\"Saved {len(benchmarks_with_data)} benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting benchmarks: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return benchmarks_with_data\n",
    "\n",
    "def scrape_benchmark_data(benchmark):\n",
    "    \"\"\"Scrape model data from a benchmark\"\"\"\n",
    "    print(f\"Scraping data for {benchmark['name']}...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    models_data = []\n",
    "    \n",
    "    try:\n",
    "        # Use timeline URL if available, otherwise use benchmark URL\n",
    "        url = benchmark.get('timeline_url') if benchmark.get('timeline_url') else benchmark['url']\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Try to extract data from JSON first\n",
    "        scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "        json_data_found = False\n",
    "        \n",
    "        for script in scripts:\n",
    "            script_text = script.get_attribute(\"innerHTML\")\n",
    "            if \"window.INITIAL_DATA\" in script_text:\n",
    "                json_match = re.search(r'window.INITIAL_DATA\\s*=\\s*({.*?});', script_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        data = json.loads(json_match.group(1))\n",
    "                        \n",
    "                        # Save raw JSON for reference\n",
    "                        benchmark_dir = os.path.join(data_dir, 'knowledge_graphs', benchmark['name'])\n",
    "                        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "                        with open(os.path.join(benchmark_dir, \"raw_data.json\"), 'w') as f:\n",
    "                            json.dump(data, f, indent=2)\n",
    "                        \n",
    "                        # Check if we have SOTA data\n",
    "                        if 'sota' in data and 'evaluations' in data['sota']:\n",
    "                            json_data_found = True\n",
    "                            print(f\"Found JSON data with {len(data['sota']['evaluations'])} evaluations\")\n",
    "                            \n",
    "                            for eval_data in data['sota']['evaluations']:\n",
    "                                # Extract date\n",
    "                                date_str = eval_data.get('date')\n",
    "                                pub_date = None\n",
    "                                if date_str:\n",
    "                                    try:\n",
    "                                        pub_date = parser.parse(date_str).date()\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "                                # Extract model info\n",
    "                                model_name = eval_data.get('model', {}).get('name', 'Unknown')\n",
    "                                paper_title = eval_data.get('paper', {}).get('title', '')\n",
    "                                paper_url = eval_data.get('paper', {}).get('url', '')\n",
    "                                code_url = eval_data.get('code', {}).get('url', '') if eval_data.get('code') else ''\n",
    "                                \n",
    "                                # Extract model description to get parameter count\n",
    "                                model_desc = eval_data.get('model', {}).get('description', '')\n",
    "                                param_count = extract_parameters_from_text(model_desc)\n",
    "                                \n",
    "                                # Extract metrics\n",
    "                                metrics = {}\n",
    "                                for metric in eval_data.get('metrics', []):\n",
    "                                    name = metric.get('name', '')\n",
    "                                    value = metric.get('value', '')\n",
    "                                    if name and value:\n",
    "                                        try:\n",
    "                                            # Convert to float if possible\n",
    "                                            if isinstance(value, str):\n",
    "                                                value = value.replace('%', '')\n",
    "                                            metrics[name] = float(value)\n",
    "                                        except:\n",
    "                                            metrics[name] = value\n",
    "                                \n",
    "                                # Only add if we have date and model name\n",
    "                                if pub_date and model_name:\n",
    "                                    model_entry = {\n",
    "                                        'dataset': benchmark['name'],\n",
    "                                        'model_name': model_name,\n",
    "                                        'paper_title': paper_title,\n",
    "                                        'paper_url': paper_url,\n",
    "                                        'code_url': code_url,\n",
    "                                        'description': model_desc,\n",
    "                                        'parameters_millions': param_count,\n",
    "                                        'date': pub_date,\n",
    "                                        'year': pub_date.year,\n",
    "                                        'month': pub_date.month,\n",
    "                                        'day': pub_date.day,\n",
    "                                        **metrics\n",
    "                                    }\n",
    "                                    models_data.append(model_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON data: {e}\")\n",
    "        \n",
    "        # If no JSON data, extract from tables\n",
    "        if not json_data_found:\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if len(rows) <= 1:  # Skip tables with just headers\n",
    "                        continue\n",
    "                    \n",
    "                    # Get headers\n",
    "                    headers = [th.text.strip().lower() for th in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "                    \n",
    "                    # Find date and model column indices\n",
    "                    date_col = None\n",
    "                    model_col = None\n",
    "                    metric_cols = []\n",
    "                    \n",
    "                    for i, header in enumerate(headers):\n",
    "                        if any(term in header for term in ['date', 'published', 'year']):\n",
    "                            date_col = i\n",
    "                        elif any(term in header for term in ['model', 'method', 'name']):\n",
    "                            model_col = i\n",
    "                        elif header and header not in ['paper', 'code', 'link']:\n",
    "                            metric_cols.append((i, header))\n",
    "                    \n",
    "                    # If we found date and model columns, process the rows\n",
    "                    if date_col is not None and model_col is not None:\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(cells) != len(headers):\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract date\n",
    "                            date_text = cells[date_col].text.strip()\n",
    "                            pub_date = extract_date_from_text(date_text)\n",
    "                            \n",
    "                            # Extract model name\n",
    "                            model_name = cells[model_col].text.strip()\n",
    "                            \n",
    "                            # Get paper link if available\n",
    "                            paper_url = \"\"\n",
    "                            paper_links = cells[model_col].find_elements(By.TAG_NAME, \"a\")\n",
    "                            if paper_links:\n",
    "                                paper_url = paper_links[0].get_attribute(\"href\")\n",
    "                            \n",
    "                            # Extract metrics\n",
    "                            metrics = {}\n",
    "                            for col_idx, col_name in metric_cols:\n",
    "                                value = cells[col_idx].text.strip()\n",
    "                                if value:\n",
    "                                    try:\n",
    "                                        # Convert percentage strings to float\n",
    "                                        if '%' in value:\n",
    "                                            value = value.replace('%', '')\n",
    "                                        metrics[col_name] = float(value)\n",
    "                                    except:\n",
    "                                        metrics[col_name] = value\n",
    "                            \n",
    "                            # Only add if we have date and model name\n",
    "                            if pub_date and model_name:\n",
    "                                model_entry = {\n",
    "                                    'dataset': benchmark['name'],\n",
    "                                    'model_name': model_name,\n",
    "                                    'paper_url': paper_url,\n",
    "                                    'date': pub_date,\n",
    "                                    'year': pub_date.year,\n",
    "                                    'month': pub_date.month,\n",
    "                                    'day': pub_date.day,\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                models_data.append(model_entry)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table: {e}\")\n",
    "        \n",
    "        # Sort models by date\n",
    "        models_data.sort(key=lambda x: x.get('date'))\n",
    "        \n",
    "        # Save the models\n",
    "        if models_data:\n",
    "            benchmark_dir = os.path.join(data_dir, 'knowledge_graphs', benchmark['name'])\n",
    "            os.makedirs(benchmark_dir, exist_ok=True)\n",
    "            \n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            models_df.to_csv(os.path.join(benchmark_dir, \"models.csv\"), index=False)\n",
    "            print(f\"Saved {len(models_data)} models for {benchmark['name']}\")\n",
    "        \n",
    "        return models_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping benchmark data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def combine_all_models():\n",
    "    \"\"\"Combine all knowledge graphs models into a single file\"\"\"\n",
    "    try:\n",
    "        all_models = []\n",
    "        \n",
    "        # Get all model CSV files\n",
    "        for root, dirs, files in os.walk(os.path.join(data_dir, 'knowledge_graphs')):\n",
    "            for file in files:\n",
    "                if file == \"models.csv\":\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        all_models.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and save\n",
    "        if all_models:\n",
    "            combined_df = pd.concat(all_models, ignore_index=True)\n",
    "            \n",
    "            # Make sure we have full date info\n",
    "            if 'date' in combined_df.columns:\n",
    "                combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "            \n",
    "            # Sort by date\n",
    "            combined_df = combined_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            combined_df.to_csv(os.path.join(data_dir, 'knowledge_graphs', 'all_knowledge_graphs_models.csv'), index=False)\n",
    "            print(f\"Combined {len(combined_df)} models from all benchmarks\")\n",
    "            \n",
    "            # Create yearly count summary\n",
    "            yearly_summary = combined_df.groupby(['dataset', 'year']).size().reset_index(name='model_count')\n",
    "            yearly_summary.to_csv(os.path.join(data_dir, 'knowledge_graphs', 'yearly_model_count.csv'), index=False)\n",
    "            print(f\"Created yearly model count summary\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error combining models: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting knowledge graphs task data collection (20+ datapoints)...\")\n",
    "    \n",
    "    # Get benchmarks with 20+ datapoints\n",
    "    benchmarks = get_knowledge_graphs_benchmarks_with_20plus()\n",
    "    \n",
    "    if not benchmarks:\n",
    "        print(\"No knowledge graphs benchmarks with 20+ datapoints found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(benchmarks)} knowledge graphs benchmarks with 20+ datapoints\")\n",
    "    \n",
    "    # Process each benchmark\n",
    "    all_models_count = 0\n",
    "    for benchmark in benchmarks:\n",
    "        try:\n",
    "            models = scrape_benchmark_data(benchmark)\n",
    "            if models:\n",
    "                all_models_count += len(models)\n",
    "            \n",
    "            # Be nice to the server\n",
    "            time.sleep(5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark['name']}: {e}\")\n",
    "    \n",
    "    print(f\"Collected a total of {all_models_count} models across all benchmarks\")\n",
    "    \n",
    "    # Combine all models\n",
    "    combine_all_models()\n",
    "    \n",
    "    print(\"Knowledge graphs task data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a72503-6f6e-491f-98c4-4a08f59a448e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
